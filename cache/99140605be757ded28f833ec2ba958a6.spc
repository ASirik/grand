a:4:{s:5:"child";a:1:{s:0:"";a:1:{s:3:"rss";a:1:{i:0;a:6:{s:4:"data";s:3:"


";s:7:"attribs";a:1:{s:0:"";a:1:{s:7:"version";s:3:"2.0";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:1:{s:0:"";a:1:{s:7:"channel";a:1:{i:0;a:6:{s:4:"data";s:217:"
  
  
  
  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:1:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:12:"Planet MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:27:"http://www.planetmysql.org/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 21:30:01 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"language";a:1:{i:0;a:5:{s:4:"data";s:2:"en";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:42:"Planet MySQL - http://www.planetmysql.org/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"item";a:50:{i:0;a:6:{s:4:"data";s:68:"
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:36:"Upcoming MySQL Connect Presentations";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:38:"http://ronaldbradford.com/blog/?p=4020";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:79:"http://ronaldbradford.com/blog/upcoming-mysql-connect-presentations-2012-07-11/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1449:"
The MySQL Connect 2012 conference event being held in San Francisco on Sep 29-30 has a long list of quality MySQL speakers including myself. I will be giving 2 presentations on:
CON8322 &#8211; Lessons from Managing 500+ MySQL Instances
In this presentation, learn about the issues of managing a large number of instances of MySQL, supporting 50 billion SQL statements per day. Topics covered:
• The need for monitoring and instrumentation
• How to automate installations, upgrades, and deployments
• Issues with MySQL’s Replication feature with 300 slaves per master
• Traffic minimization techniques
• Creating high availability with regions and zones
• Real-time traffic stats (aggregated every five seconds)
CON8320 &#8211; Improving Performance with Better Indexes
Learn how to use one simple advanced technique to make better indexes in MySQL and improve your queries by 500 percent or more. Even with a highly indexed schema, you can achieve significant improvements in performance by creating better indexes. This presentation introduces an approach to correct identification and verification of problem SQL statements and then describes the means of identifying index choices for optimization. Then it discusses not only how to apply indexes to improve query performance but also how to apply better indexes and provide even greater performance gains.
You can also read more information with my Interview about MySQL Connect.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 20:14:40 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:6:{i:0;a:5:{s:4:"data";s:9:"Databases";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:12:"Professional";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:16:"mysql conference";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:13:"mysql connect";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:17:"Oracle Open World";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2030:"<p><a href="http://www.oracle.com/mysqlconnect/index.html"><img src="http://ronaldbradford.com/images/site/mysqlconnect2012.gif" /></a><br />
The <a href="http://www.oracle.com/mysqlconnect/index.html">MySQL Connect 2012</a> conference event being held in San Francisco on Sep 29-30 has a long list of quality MySQL speakers including myself. I will be giving 2 presentations on:</p>
<h3>CON8322 &#8211; Lessons from Managing 500+ MySQL Instances</h3>
<p>In this presentation, learn about the issues of managing a large number of instances of MySQL, supporting 50 billion SQL statements per day. Topics covered:<br />
• The need for monitoring and instrumentation<br />
• How to automate installations, upgrades, and deployments<br />
• Issues with MySQL’s Replication feature with 300 slaves per master<br />
• Traffic minimization techniques<br />
• Creating high availability with regions and zones<br />
• Real-time traffic stats (aggregated every five seconds)</p>
<h3>CON8320 &#8211; Improving Performance with Better Indexes</h3>
<p>Learn how to use one simple advanced technique to make better indexes in MySQL and improve your queries by 500 percent or more. Even with a highly indexed schema, you can achieve significant improvements in performance by creating better indexes. This presentation introduces an approach to correct identification and verification of problem SQL statements and then describes the means of identifying index choices for optimization. Then it discusses not only how to apply indexes to improve query performance but also how to apply better indexes and provide even greater performance gains.</p>
<p>You can also read more information with my <a href="https://blogs.oracle.com/MySQL/entry/interview_with_ronald_bradford_about">Interview about MySQL Connect</a>.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33796&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33796&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"Ronald Bradford";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:1;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:35:"Is this a new golden age for MySQL?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:41:"http://opensourcedba.wordpress.com/?p=893";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:81:"http://opensourcedba.wordpress.com/2012/07/11/is-this-a-new-golden-age-for-mysql/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2672:"MySQL has taken another leap forward.  It is not a dramatic new feature or my long requested –run-faster command line option.  Recruiters are now very actively seeking MySQL Database Administrators and Developers.  One recruiter was offing a $5,000 &#8216;finders fee&#8217; for a lead on MySQL DBAs.
The MySQL Certified Professionals group on LinkedIn, for one example, has several postings each week looking to fill positions.  The Jobs board on forums.mysql.com is busy.   My local MySQL Users Groups gets several requests each month to post jobs on the website.  
I used to work for an on-line recruiting company and still occasionally lurk about various recruiting sites to see how MySQL is trending.  And MySQL is trending upwards.
Much of this is from start-ups looking to take advantage of the LAMP stack.  Some of this comes from governments and businesses strapped for budget and moving to less costly components.  I am getting questions from DBAs of other databases who have either &#8216;inherited&#8217; some MySQL instances or are being driven by business factors to MySQL who are needing to get up to speed as quickly as possible.
The recruiters, even the highly technical-oriented specialists, usually know very little about MySQL.  I usually quiz them about what requirements they provided for by their customers (such as release versions used, storage engines, replication, clustering, tuning needs) and very few have specifics.  Yes, they want the latest and greatest plus replication but version numbers they do not know.  Some of these old contacts have a vague idea about my position and are asking some tough questions like &#8216;Why are there so few MySQL DBAs out there for hire&#8217;?   (more on that in the future)
They are looking for experience.  Often as little as a year or two of experience.  Senior jobs are just that – someone with many years of working with MySQL under their belts.  Many companies can not find senior level staff and scope down their model of the perfect MySQL DBA or Developer.  One local DBA was called back twice after being told they wanted someone with more &#8216;seat time&#8217; before they came back with a job offer.
And the pay range seems to have crept up too.  A  nearby manufacturer used to view MySQL as a &#8216;toy database&#8217; now has shifted much of their data from a very costly RDMS (rhymes with Pee-Bee-Boo) to MySQL and brought the pay grade up to match the DBAs for their mainfame instances.
So now we command more &#8216;gold&#8217; in the market place.  But are there enough of &#8216;us&#8217; around to make it not-cost effective to find another commidy DBA to replace us?
         ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 19:06:04 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4672:"<p>MySQL has taken another leap forward.  It is not a dramatic new feature or my long requested <strong>–run-faster</strong> command line option.  Recruiters are now <em>very actively</em> seeking MySQL Database Administrators and Developers.  One recruiter was offing a $5,000 &#8216;finders fee&#8217; for a lead on MySQL DBAs.</p>
<p>The MySQL Certified Professionals group on LinkedIn, for one example, has several postings each week looking to fill positions.  The <a href="http://forums.mysql.com/list.php?8" target="_blank">Jobs board</a> on forums.mysql.com is busy.   My local MySQL Users Groups gets several requests each month to post jobs on the website.  </p>
<p>I used to work for an on-line recruiting company and still occasionally lurk about various recruiting sites to see how MySQL is trending.  And MySQL is trending upwards.</p>
<p>Much of this is from start-ups looking to take advantage of the LAMP stack.  Some of this comes from governments and businesses strapped for budget and moving to less costly components.  I am getting questions from DBAs of other databases who have either &#8216;inherited&#8217; some MySQL instances or are being driven by business factors to MySQL who are needing to get up to speed as quickly as possible.</p>
<p>The recruiters, even the highly technical-oriented specialists, usually know very little about MySQL.  I usually quiz them about what requirements they provided for by their customers (such as release versions used, storage engines, replication, clustering, tuning needs) and very few have specifics.  Yes, they want the latest and greatest plus replication but version numbers they do not know.  Some of these old contacts have a vague idea about my position and are asking some tough questions like &#8216;Why are there so few MySQL DBAs out there for hire&#8217;?   (more on that in the future)</p>
<p>They are looking for experience.  Often as little as a year or two of experience.  Senior jobs are just that – someone with many years of working with MySQL under their belts.  Many companies can not find senior level staff and scope down their model of the perfect MySQL DBA or Developer.  One local DBA was called back twice after being told they wanted someone with more &#8216;seat time&#8217; before they came back with a job offer.</p>
<p>And the pay range seems to have crept up too.  A  nearby manufacturer used to view MySQL as a &#8216;toy database&#8217; now has shifted much of their data from a very costly RDMS (rhymes with Pee-Bee-Boo) to MySQL and brought the pay grade up to match the DBAs for their mainfame instances.</p>
<p>So now we command more &#8216;gold&#8217; in the market place.  But are there enough of &#8216;us&#8217; around to make it not-cost effective to find another commidy DBA to replace us?</p>
<br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godelicious/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/delicious/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gofacebook/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/facebook/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gotwitter/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/twitter/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gostumble/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/stumble/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godigg/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/digg/opensourcedba.wordpress.com/893/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/goreddit/opensourcedba.wordpress.com/893/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/reddit/opensourcedba.wordpress.com/893/" /></a> <img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=opensourcedba.wordpress.com&amp;blog=15386988&amp;post=893&amp;subd=opensourcedba&amp;ref=&amp;feed=1" width="1" height="1" /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33795&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33795&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:11:"Dave Stokes";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:2;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:26:"Meet Percona Team at OSCON";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:44:"http://www.mysqlperformanceblog.com/?p=10359";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:74:"http://www.mysqlperformanceblog.com/2012/07/11/meet-percona-team-at-oscon/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:614:"Yes, We will be at OSCON next week.  I will be talking about Optimizing MySQL Configuration and host a BOF on MySQL Sharding Replication and Clustering if you&#8217;re interested in any of these technologies please come by and share your story. I would love to see both users and technology vendors working in this field. 
Jay Janssen is going to talk about Writing non-blocking code for interaction with data systems and web services in Node.js and Perl and about Running a high performance LAMP stack on a $20 Virtual server.
We&#8217;re also there at Expo Hall, please come by and say Hi our Booth number is 524";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 17:58:54 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:24:"Events and Announcements";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1190:"<p>Yes, We will be at <a href="http://www.oscon.com/oscon2012/">OSCON</a> next week.  I will be talking about <a href="http://www.oscon.com/oscon2012/public/schedule/detail/24445">Optimizing MySQL Configuration</a> and host a BOF on <a href="http://www.oscon.com/oscon2012/public/schedule/detail/26507">MySQL Sharding Replication and Clustering</a> if you&#8217;re interested in any of these technologies please come by and share your story. I would love to see both users and technology vendors working in this field. </p>
<p>Jay Janssen is going to talk about <a href="http://www.oscon.com/oscon2012/public/schedule/detail/23404">Writing non-blocking code for interaction with data systems and web services in Node.js and Perl</a> and about <a href="http://www.oscon.com/oscon2012/public/schedule/detail/23408">Running a high performance LAMP stack on a $20 Virtual server</a>.</p>
<p>We&#8217;re also there at Expo Hall, please come by and say Hi our Booth number is 524</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33794&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33794&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:22:"MySQL Performance Blog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:3;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:89:"Slides From MySQL, Geek Management and Good Ideas for DBA Talks in Trinidad and Guatemala";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:28:"667 at http://www.sheeri.com";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:72:"http://www.sheeri.com/content/slides-mysql-geek-management-and-good-idea";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1068:"In the past two days I was fortunate enough to speak to two different groups of people at the Ministry of Science, Technology and Tertiary Education in the Parliament building in Port of Spain, Trinidad &#8211; yes, I spoke at Parliament! The PDF slides for my talk are available: The Art of Cat Herding: How to Manage Geeks and Ideas for DBA&#8217;s &#8211; not &#8220;best practices&#8221;, but ideas you may or may not want to implement.
I was in Trinidad as part of the Latin America Oracle Technology Network tour of Latin America (North leg). I also spent time in Cali, Colombia and Quito, Ecuador (including visiting the Equator!). Today I am in Guatemala, and I will give talks on more MySQL-specific subjects: MySQL Security and Get Rid of Cron Scripts Using MySQL Events. Tomorrow I travel to Honduras, and on Sunday is Costa Rica, then I go home, which I haven&#8217;t seen since the 24th of June. I will have spent 43 hours on a plane in one month, and I&#8217;m excited to finally go home next week&#8230;but I still have a few more countries on the tour!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 15:45:03 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1575:"<p>In the past two days I was fortunate enough to speak to two different groups of people at the Ministry of Science, Technology and Tertiary Education in the Parliament building in Port of Spain, Trinidad &#8211; yes, I spoke at Parliament! The PDF slides for my talk are available: <a href="http://technocation.org/files/doc/2012_manage.pdf">The Art of Cat Herding: How to Manage Geeks</a> and <a href="http://technocation.org/files/doc/2012IdeasDBA.pdf">Ideas for DBA&#8217;s</a> &#8211; not &#8220;best practices&#8221;, but ideas you may or may not want to implement.</p>
<p>I was in Trinidad as part of the Latin America Oracle Technology Network tour of Latin America (North leg). I also spent time in Cali, Colombia and Quito, Ecuador (including visiting the Equator!). Today I am in Guatemala, and I will give talks on more MySQL-specific subjects: <a href="http://technocation.org/files/doc/2012_05_MySQLSecurityPres.pdf">MySQL Security</a> and <a href="http://technocation.org/files/doc/2012_07_MySQL_Events.pdf">Get Rid of Cron Scripts Using MySQL Events</a>. Tomorrow I travel to Honduras, and on Sunday is Costa Rica, then I go home, which I haven&#8217;t seen since the 24th of June. I will have spent 43 hours on a plane in one month, and I&#8217;m excited to finally go home next week&#8230;but I still have a few more countries on the tour!</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33793&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33793&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:16:"Sheeri K. Cabral";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:4;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:30:"SQLyog MySQL GUI 10.2 Released";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.webyog.com/blog/?p=3870";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:69:"http://www.webyog.com/blog/2012/07/11/sqlyog-mysql-gui-10-2-released/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:682:"Changes (as compared to 10.12) include:
Features:
* Saved connections can now be exported and imported. The option is available in the ‘tools’ menu.
* Added an option to open exported CSV and Excel XML files directly with the program associated with the file extension on the system.
Bug fixes:
* Exporting Schema Designer canvas as image on Wine generated empty boxes for tables with no information about columns. It is actually a bug in Wine (incomplete Windows API implementation for drawing routines) .
* The height of GRID-cells will now adjust to fit font setting for the GRID .
Downloads: http://webyog.com/en/downloads.php
Purchase: http://webyog.com/en/buy.php


Tweet
";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 14:27:25 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:8:"Releases";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"SQLyog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1413:"<p><strong>Changes (as compared to 10.12) include:</strong></p>
<p><strong>Features:</strong><br />
* Saved connections can now be exported and imported. The option is available in the ‘tools’ menu.<br />
* Added an option to open exported CSV and Excel XML files directly with the program associated with the file extension on the system.</p>
<p><strong>Bug fixes:</strong><br />
* Exporting Schema Designer canvas as image on Wine generated empty boxes for tables with no information about columns. It is actually a bug in Wine (incomplete Windows API implementation for drawing routines) .<br />
* The height of GRID-cells will now adjust to fit font setting for the GRID .</p>
<p><strong>Downloads:</strong> <a href="http://webyog.com/en/downloads.php">http://webyog.com/en/downloads.php</a><br />
<strong>Purchase:</strong> <a href="http://webyog.com/en/buy.php">http://webyog.com/en/buy.php</a></p>

<!-- This is the start of the WP Twitter Button code -->
<div><a href="http://twitter.com/share" data-url="http://www.webyog.com/blog/2012/07/11/sqlyog-mysql-gui-10-2-released/" data-count="horizontal" data-via="webyog">Tweet</a></div>
<!-- This is the end of the WP Twitter Button code --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33792&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33792&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:6:"Webyog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:5;a:6:{s:4:"data";s:98:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:52:"Understanding SHOW VARIABLES: DISABLED and NO values";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:33:"http://openquery.com/blog/?p=1668";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:70:"http://openquery.com/blog/understanding-show-variables-disabled-values";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1525:"When you use SHOW VARIABLES LIKE &#8220;have_%&#8221; to see whether a particular feature is enabled, you will note the value of NO for some, and DISABLED for others. These values are not intrinsically clear for the casual onlooker, and often cause confusion. Typically, this happens with SSL and InnoDB. So, here is a quick clarification!

NO means that the feature was not enabled (or was actively disabled) in the build. This means the code and any required libraries are not present in the binary.
DISABLED means that the feature is built in and capable of working in the binary, but is disabled due to relevant configuration settings.
YES means the feature is available and configured.

SSL tends to show up as DISABLED, until you configure the appropriate settings to use it (SHOW VARIABLES LIKE &#8220;ssl_%&#8221;). From then on it will show up as YES.
Depending on your MySQL version and distro build, InnoDB can be disabled via the &#8220;skip-innodb&#8221; option. Obviously that&#8217;s not recommended as InnoDB should generally be your primary engine of choice!
However, InnoDB can also show up as DISABLED if the plugin fails to load due to configuration or other errors on startup. When this happens, review the error log (often redirected to syslog/messages) to identify the problem.
If InnoDB is configured as the default storage engine, failed initialisation of the plugin should now result in mysqld not starting, rather than starting with InnoDB disabled, as obviously InnoDB is required in that case.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 05:24:32 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:12:{i:0;a:5:{s:4:"data";s:13:"Uncategorized";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:8:"DISABLED";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"InnoDB";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:7:"mariadb";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:6:"mysqld";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:2:"NO";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:6:"plugin";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:8;a:5:{s:4:"data";s:4:"SHOW";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:9;a:5:{s:4:"data";s:14:"SHOW VARIABLES";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:10;a:5:{s:4:"data";s:3:"SSL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:11;a:5:{s:4:"data";s:6:"syslog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1954:"<p>When you use<strong> SHOW VARIABLES LIKE &#8220;have_%&#8221;</strong> to see whether a particular feature is enabled, you will note the value of NO for some, and DISABLED for others. These values are not intrinsically clear for the casual onlooker, and often cause confusion. Typically, this happens with SSL and InnoDB. So, here is a quick clarification!</p>
<ul>
<li><strong>NO</strong> means that the feature was not enabled (or was actively disabled) in the build. This means the code and any required libraries are <em>not</em> present in the binary.</li>
<li><strong>DISABLED</strong> means that the feature is built in and capable of working in the binary, but is disabled due to relevant configuration settings.</li>
<li><strong>YES</strong> means the feature is available and configured.</li>
</ul>
<p><strong>SSL</strong> tends to show up as DISABLED, until you configure the appropriate settings to use it (<strong>SHOW VARIABLES LIKE &#8220;ssl_%&#8221;</strong>). From then on it will show up as <strong>YES</strong>.</p>
<p>Depending on your MySQL version and distro build, <strong>InnoDB</strong> can be disabled via the &#8220;skip-innodb&#8221; option. Obviously that&#8217;s not recommended as InnoDB should generally be your primary engine of choice!</p>
<p>However, InnoDB can also show up as DISABLED if the plugin fails to load due to configuration or other errors on startup. When this happens, review the error log (often redirected to syslog/messages) to identify the problem.</p>
<p>If InnoDB is configured as the default storage engine, failed initialisation of the plugin should now result in mysqld not starting, rather than starting with InnoDB disabled, as obviously InnoDB is required in that case.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33790&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33790&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Open Query";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:6;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:41:"MySQL joins: ON vs. USING vs. Theta-style";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:36:"http://code.openark.org/blog/?p=5050";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:73:"http://code.openark.org/blog/mysql/mysql-joins-on-vs-using-vs-theta-style";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:5811:"What is the difference between the following three syntaxes?

SELECT * FROM film JOIN film_actor ON (film.film_id = film_actor.film_id)
SELECT * FROM film JOIN film_actor USING (film_id)
SELECT * FROM film, film_actor WHERE film.film_id = film_actor.film_id

The difference is mostly syntactic sugar, but with a couple interesting notes.
To put names, the first two are called "ANSI-style" while the third is called "Theta-style".
Theta style
On the FROM clause, tables are listed as if with Cartesian products, and the WHERE clause specifies how the join should take place.
This is considered to be the "old" style. It is somewhat confusing to read. Consider the following query:

SELECT * FROM film, film_actor WHERE film.film_id = film_actor.film_id AND actor_id = 17 AND film.length &gt; 120

The above lists films over 120 minutes in length, in which actor #17 plays. Never mind the results; what about the query? Being just one part of the WHERE clause, a one out of three elements in the AND expression, the join equation gets lost. It is difficult to find and isolate the terms which make for table joins as opposed to terms which filter out rows. In the above example it is still relatively easy to point out. How about a query with 5 tables and a 20 terms WHERE clause?
ANSI style: ON
With JOIN ... ON, one separates the join terms from the filtering terms. Rewriting the previous example:

SELECT * FROM film JOIN film_actor ON (film.film_id = film_actor.film_id) WHERE actor_id = 17 AND film.length &gt; 120

It is quite clear now what belongs to what.
Note: the parenthesis are not strictly required in the ON clause. I personally like to use them: it makes for an even greater distinction between query parts. SQL syntax is such a mess!
ANSI style: USING
Is the special case where we join tables on columns of the same name, we can make a shortcut and use USING:

SELECT * FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120

This time the parenthesis are required (I'm not sure why the difference on that part).
This is mainly a nicety, less words to type, and a resulting prettified query. But also note a couple differences:
USING vs. ON
The following is valid:

SELECT film.title, film_id FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120;

But the following is not:

SELECT film.title, film_id FROM film JOIN film_actor ON (film.film_id = film_actor.film_id) WHERE actor_id = 17 AND film.length &gt; 120;
ERROR 1052 (23000): Column 'film_id' in field list is ambiguous

Since USING "knows" the film_id column is shared between both tables, it doesn't mind if we ask it without specifying an exact table. It would be the same value anyway!
ON is not as smart and requires further clarifications: which table exactly do you want?
And the above is actually the result of this interesting phenomena: when using USING, the column only appears once in the result set:

SELECT * FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120 LIMIT 1\G
*************************** 1. row ***************************
             film_id: 96
               title: BREAKING HOME
         description: A Beautiful Display of a Secret Agent And a Monkey who must Battle a Sumo Wrestler in An Abandoned Mine Shaft
        release_year: 2006
         language_id: 1
original_language_id: NULL
     rental_duration: 4
         rental_rate: 2.99
              length: 169
    replacement_cost: 21.99
              rating: PG-13
    special_features: Trailers,Commentaries
         last_update: 2006-02-15 05:03:42
            actor_id: 17
         last_update: 2006-02-15 05:05:03

But joining on ON, we get this column twice:

SELECT * FROM film JOIN film_actor ON film.film_id = film_actor.film_id WHERE actor_id = 17 AND film.length &gt; 120 LIMIT 1\G
*************************** 1. row ***************************
             film_id: 96
               title: BREAKING HOME
         description: A Beautiful Display of a Secret Agent And a Monkey who must Battle a Sumo Wrestler in An Abandoned Mine Shaft
        release_year: 2006
         language_id: 1
original_language_id: NULL
     rental_duration: 4
         rental_rate: 2.99
              length: 169
    replacement_cost: 21.99
              rating: PG-13
    special_features: Trailers,Commentaries
         last_update: 2006-02-15 05:03:42
            actor_id: 17
             film_id: 96
         last_update: 2006-02-15 05:05:03

Behind the scenes
The news is that MySQL treats all in the exact same way. With the kind help of EXPLAIN EXTENDED, we see that:

EXPLAIN EXTENDED SELECT film.title, film_id FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120\G
*************************** 1. row ***************************
...
2 rows in set, 1 warning (0.00 sec)

root@mysql-5.1.51&gt; SHOW WARNINGS\G
*************************** 1. row ***************************
  Level: Note
   Code: 1003
Message: select `sakila`.`film`.`title` AS `title`,`sakila`.`film`.`film_id` AS `film_id` 
         from `sakila`.`film` join `sakila`.`film_actor` 
         where (
                 (`sakila`.`film`.`film_id` = `sakila`.`film_actor`.`film_id`) 
                 and (`sakila`.`film_actor`.`actor_id` = 17) 
                 and (`sakila`.`film`.`length` &gt; 120)
               )

All queries are translated internally to theta-style.
This post only discusses inner joins. With outer joins the situation is somewhat different. Read this post for more insight.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 11 Jul 2012 04:54:20 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:3:"SQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"Syntax";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:7419:"<p>What is the difference between the following three syntaxes?</p>
<blockquote>
<pre>SELECT * FROM film JOIN film_actor ON (film.film_id = film_actor.film_id)
SELECT * FROM film JOIN film_actor USING (film_id)
SELECT * FROM film, film_actor WHERE film.film_id = film_actor.film_id</pre>
</blockquote>
<p>The difference is mostly syntactic sugar, but with a couple interesting notes.</p>
<p>To put names, the first two are called <strong>"ANSI-style"</strong> while the third is called <strong>"Theta-style"</strong>.</p>
<h4>Theta style</h4>
<p>On the <strong>FROM</strong> clause, tables are listed as if with Cartesian products, and the <strong>WHERE</strong> clause specifies how the join should take place.</p>
<p>This is considered to be the "old" style. It is somewhat confusing to read. Consider the following query:<span></span></p>
<blockquote>
<pre><strong>SELECT</strong> * <strong>FROM</strong> film, film_actor <strong>WHERE</strong> film.film_id = film_actor.film_id <strong>AND</strong> actor_id = 17 <strong>AND</strong> film.length &gt; 120</pre>
</blockquote>
<p>The above lists films over <strong>120</strong> minutes in length, in which actor <strong>#17</strong> plays. Never mind the results; what about the query? Being just one part of the <strong>WHERE</strong> clause, a one out of three elements in the <strong>AND</strong> expression, the join equation gets lost. It is difficult to find and isolate the terms which make for table joins as opposed to terms which filter out rows. In the above example it is still relatively easy to point out. How about a query with <strong>5</strong> tables and a <strong>20</strong> terms <strong>WHERE</strong> clause?</p>
<h4>ANSI style: ON</h4>
<p>With <strong>JOIN</strong> ... <strong>ON</strong>, one separates the join terms from the filtering terms. Rewriting the previous example:</p>
<blockquote>
<pre><strong>SELECT</strong> * <strong>FROM</strong> film <strong>JOIN</strong> film_actor <strong>ON</strong> (film.film_id = film_actor.film_id) <strong>WHERE</strong> actor_id = 17 <strong>AND</strong> film.length &gt; 120</pre>
</blockquote>
<p>It is quite clear now what belongs to what.</p>
<p>Note: the parenthesis are not strictly required in the <strong>ON</strong> clause. I personally like to use them: it makes for an even greater distinction between query parts. SQL syntax is such a mess!</p>
<h4>ANSI style: USING</h4>
<p>Is the special case where we join tables on columns of the same name, we can make a shortcut and use <strong>USING</strong>:</p>
<blockquote>
<pre><strong>SELECT</strong> * <strong>FROM</strong> film <strong>JOIN</strong> film_actor <strong>USING</strong> (film_id) <strong>WHERE</strong> actor_id = 17 <strong>AND</strong> film.length &gt; 120</pre>
</blockquote>
<p>This time the parenthesis are required (I'm not sure why the difference on that part).</p>
<p>This is mainly a nicety, less words to type, and a resulting prettified query. But also note a couple differences:</p>
<h4>USING vs. ON</h4>
<p>The following is valid:</p>
<blockquote>
<pre>SELECT film.title, film_id FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120;</pre>
</blockquote>
<p>But the following is not:</p>
<blockquote>
<pre>SELECT film.title, film_id FROM film JOIN film_actor ON (film.film_id = film_actor.film_id) WHERE actor_id = 17 AND film.length &gt; 120;
<strong>ERROR 1052 (23000): Column 'film_id' in field list is ambiguous</strong></pre>
</blockquote>
<p>Since <strong>USING</strong> "knows" the film_id column is shared between both tables, it doesn't mind if we ask it without specifying an exact table. It would be the same value anyway!</p>
<p><strong>ON</strong> is not as smart and requires further clarifications: which table exactly do you want?</p>
<p>And the above is actually the result of this interesting phenomena: when using <strong>USING</strong>, the column only appears <em>once</em> in the result set:</p>
<blockquote>
<pre>SELECT * FROM film JOIN film_actor USING (film_id) WHERE actor_id = 17 AND film.length &gt; 120 LIMIT 1\G
*************************** 1. row ***************************
             <strong>film_id</strong>: 96
               title: BREAKING HOME
         description: A Beautiful Display of a Secret Agent And a Monkey who must Battle a Sumo Wrestler in An Abandoned Mine Shaft
        release_year: 2006
         language_id: 1
original_language_id: NULL
     rental_duration: 4
         rental_rate: 2.99
              length: 169
    replacement_cost: 21.99
              rating: PG-13
    special_features: Trailers,Commentaries
         last_update: 2006-02-15 05:03:42
            actor_id: 17
         last_update: 2006-02-15 05:05:03</pre>
</blockquote>
<p>But joining on <strong>ON</strong>, we get this column <em>twice</em>:</p>
<blockquote>
<pre>SELECT * FROM film JOIN film_actor ON film.film_id = film_actor.film_id WHERE actor_id = 17 AND film.length &gt; 120 LIMIT 1\G
*************************** 1. row ***************************
             <strong>film_id</strong>: 96
               title: BREAKING HOME
         description: A Beautiful Display of a Secret Agent And a Monkey who must Battle a Sumo Wrestler in An Abandoned Mine Shaft
        release_year: 2006
         language_id: 1
original_language_id: NULL
     rental_duration: 4
         rental_rate: 2.99
              length: 169
    replacement_cost: 21.99
              rating: PG-13
    special_features: Trailers,Commentaries
         last_update: 2006-02-15 05:03:42
            actor_id: 17
             <strong>film_id</strong>: 96
         last_update: 2006-02-15 05:05:03</pre>
</blockquote>
<h4>Behind the scenes</h4>
<p>The news is that MySQL treats all in the exact same way. With the kind help of <strong>EXPLAIN EXTENDED</strong>, we see that:</p>
<blockquote>
<pre>EXPLAIN EXTENDED SELECT film.title, film_id FROM film JOIN film_actor <strong>USING</strong> (film_id) WHERE actor_id = 17 AND film.length &gt; 120\G
*************************** 1. row ***************************
...
2 rows in set, 1 warning (0.00 sec)

root@mysql-5.1.51&gt; SHOW WARNINGS\G
*************************** 1. row ***************************
  Level: Note
   Code: 1003
Message: select `sakila`.`film`.`title` AS `title`,`sakila`.`film`.`film_id` AS `film_id` 
         from `sakila`.`film` join `sakila`.`film_actor` 
         where (
                 <strong>(`sakila`.`film`.`film_id` = `sakila`.`film_actor`.`film_id`)</strong> 
                 and (`sakila`.`film_actor`.`actor_id` = 17) 
                 and (`sakila`.`film`.`length` &gt; 120)
               )</pre>
</blockquote>
<p>All queries are translated internally to <em>theta-style</em>.</p>
<p>This post only discusses inner joins. With outer joins the situation is somewhat different. Read <a href="http://www.mysqldiary.com/mysql-left-join/">this post</a> for more insight.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33789&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33789&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Shlomi Noach";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:7;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:47:"Why I Wrote the Book – Oracle and Open Source";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:29:"http://www.iheavy.com/?p=4774";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:53:"http://feedproxy.google.com/~r/iheavy/~3/QclF0DCn6Dw/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4063:"Read the original article at Why I Wrote the Book &#8211; Oracle and Open SourceBack in the late 90&#8242;s New York City was deep in the dot-com boom.  Silicon Alley was being born, and a thousand internet startups were sprouting.  Everyone was hiring, it was an exciting time to work in technology!Trend Spotting Circa 2000As an independent consultant, I had the opportunity to work at quite a few startups.  The technology stack was identical at almost all of them.  Sun Microsystems hardware, Apache webservers, and Oracle on the backend.   The database was always the sticking point, and developers struggled to get their queries right.It was an interesting role to hold.  Most career DBAs worked at large fortune 500 firms, the old stodgy kind where nothing ever changes.  Few of the Oracle old guard, the kind you&#8217;d meet at User Groups or conferences, had much exposure to Linux, and they certainly didn&#8217;t trust it.Meanwhile in the startup scene in NYC I was seeing the cutting edge uses of the technology, with more and more shops switching to Linux and commodity hardware.  There was even talk of *gasp* Oracle porting to Linux.  There was a real rumor mill around all of this.Oracle and Open Source Published &#8211; 2001Seeing this shift towards commodity hardware, and the tremendous demand for Oracle married with open source technologies, I pitched O&#8217;Reilly and Associates with a book idea.  Let&#8217;s talk about what&#8217;s happening in the trenches.  How and when does Oracle &#8211; the most commercial of relational databases, work with Open Source technologies?  What is in the mix?  What are real firms using it for?  What tools and technologies can help firms grow faster?These were the questions my co-author and I sought to answer, and to judge from the response I think we did a very good job.  As that push continued, Oracle eventually ported it&#8217;s enterprise database to Linux.  This was a seismic shift that meant existing Oracle customers would spend a lot less on hardware, and thus have more to spend on Oracle licenses.  Win-win except for Sun.  The trend continued with Oracle pushing Apache into the mix as well.Fast Forward a DecadeNow a decade later, Oracle has bought it&#8217;s former partner Sun, and in so doing owns MySQL too.What new trends are happening?  We hear an incessant drum of hype around cloud computing.  In many ways the trend parallels what happened a decade ago.  See our related piece a history lesson for cloud detractors.  How so?Commoditization: push towards new platforms, driven by cost.But this is slowed by an equally large stumbling block.Performance: new cloud servers can&#8217;t compete with their big iron cousins.  Not yet at least.Interested in Amazon EC2?  We wrote an Intro to EC2 Cloud Deployments article which digs in deeper.What&#8217;s Next for DatacentersCommiditization will continue, driving costs downward.  This will provide more gravity to cloud migrations for firms big and small.Performance will improve.  Cloud services like Amazon EC2 will get bigger &#038; better, as will the all important network &#038; disk subsystems.Big enterprises are already dipping their feet in the water with VPC technology, tying their existing datacenter to a cloud.  They can grow elastically while still having feet firmly planted on the ground.As large enterprises begin to get experience behind the wheel, it&#8217;ll chip away at the stranglehold of Oracle and the huge taxation type licensing that firms struggle with today.   Where salesforce.com had a huge impact, workday.com will be even bigger.The cloud will finally disrupt the last old guard industry &#8211; enterprise software.Read this far?  Then grab our newsletter!Related posts:Oracle to MySQL &#8211; prepare to bushwhack through the open source jungleOpen Source Enables the CloudOracle &#038; Open Source Projects &#8211; The InterviewsOpen Source – What is it and why is it important?Open Insights 03 – The Business of Open SourceFor more articles like these go to iHeavy, Inc +1-212-533-6828";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 10 Jul 2012 23:13:08 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:3:"All";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:15:"Cloud Computing";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:11:"datacenters";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:6168:"<p>Read the original article at <a href="http://www.iheavy.com/2012/07/10/why-i-wrote-the-book-oracle-and-open-source/">Why I Wrote the Book &#8211; Oracle and Open Source</a></p><p><a href="http://www.iheavy.com/2012/07/10/why-i-wrote-the-book-oracle-and-open-source/oos-book/" rel="attachment wp-att-4775"><img src="http://www.iheavy.com/wp-content/uploads/2012/07/oos-book.gif" alt="" title="oos book" width="180" height="236" class="alignleft size-full wp-image-4775" /></a></p><p>Back in the late 90&#8242;s New York City was deep in the dot-com boom.  Silicon Alley was being born, and a thousand internet startups were sprouting.  Everyone was hiring, it was an exciting time to work in technology!</p><h2>Trend Spotting Circa 2000</h2><p>As an independent consultant, I had the opportunity to work at quite a few startups.  The technology stack was identical at almost all of them.  Sun Microsystems hardware, Apache webservers, and Oracle on the backend.   The database was always the sticking point, and developers struggled to get their queries right.</p><p>It was an interesting role to hold.  Most career DBAs worked at large fortune 500 firms, the old stodgy kind where nothing ever changes.  Few of the Oracle old guard, the kind you&#8217;d meet at User Groups or conferences, had much exposure to Linux, and they certainly didn&#8217;t trust it.</p><p>Meanwhile in the startup scene in NYC I was seeing the cutting edge uses of the technology, with more and more shops switching to Linux and commodity hardware.  There was even talk of *gasp* Oracle porting to Linux.  There was a real rumor mill around all of this.</p><h2>Oracle and Open Source Published &#8211; 2001</h2><p>Seeing this shift towards commodity hardware, and the tremendous demand for Oracle married with open source technologies, I pitched O&#8217;Reilly and Associates with a book idea.  Let&#8217;s talk about what&#8217;s happening in the trenches.  How and when does Oracle &#8211; the most commercial of relational databases, work with Open Source technologies?  What is in the mix?  What are real firms using it for?  What tools and technologies can help firms grow faster?</p><p>These were the questions my co-author and I sought to answer, and to judge from the response I think we did a very good job.  As that push continued, Oracle eventually ported it&#8217;s enterprise database to Linux.  This was a seismic shift that meant existing Oracle customers would spend a lot less on hardware, and thus have more to spend on Oracle licenses.  Win-win except for Sun.  The trend continued with Oracle pushing Apache into the mix as well.</p><h2>Fast Forward a Decade</h2><p>Now a decade later, Oracle has bought it&#8217;s former partner Sun, and in so doing owns MySQL too.</p><p>What new trends are happening?  We hear an incessant drum of hype around cloud computing.  In many ways the trend parallels what happened a decade ago.  See our related piece <a href="http://www.iheavy.com/2012/01/29/history-lesson-for-cloud-detractors/">a history lesson for cloud detractors</a>.  How so?</p><div><strong>Commoditization</strong>: push towards new platforms, driven by cost.</div><p>But this is slowed by an equally large stumbling block.</p><div><strong>Performance</strong>: new cloud servers can&#8217;t compete with their big iron cousins.  Not yet at least.</div><p>Interested in Amazon EC2?  We wrote an <a href="http://www.iheavy.com/2010/12/14/introduction-to-ec2-cloud-deployments/">Intro to EC2 Cloud Deployments</a> article which digs in deeper.</p><h2>What&#8217;s Next for Datacenters</h2><p>Commiditization will continue, driving costs downward.  This will provide more gravity to cloud migrations for firms big and small.</p><p>Performance will improve.  Cloud services like Amazon EC2 will get bigger &#038; better, as will the all important network &#038; disk subsystems.</p><p>Big enterprises are already dipping their feet in the water with VPC technology, tying their existing datacenter to a cloud.  They can grow elastically while still having feet firmly planted on the ground.</p><p>As large enterprises begin to get experience behind the wheel, it&#8217;ll chip away at the stranglehold of Oracle and the huge taxation type licensing that firms struggle with today.   Where salesforce.com had a huge impact, workday.com will be even bigger.</p><div>The cloud will finally disrupt the last old guard industry &#8211; enterprise software.</div><p>Read this far?  Then <a href="http://www.iheavy.com/signup-scalable-startups-newsletter/">grab our newsletter!</a></p><p>Related posts:</p><ol><li><a href="http://www.iheavy.com/2012/02/29/oracle-to-mysql-prepare-to-bushwhack-through-the-open-source-jungle/" rel="bookmark" title="Oracle to MySQL – prepare to bushwhack through the open source jungle">Oracle to MySQL &#8211; prepare to bushwhack through the open source jungle</a></li><li><a href="http://www.iheavy.com/2011/08/10/open-source-enables-the-cloud/" rel="bookmark" title="Open Source Enables the Cloud">Open Source Enables the Cloud</a></li><li><a href="http://www.iheavy.com/2008/01/01/oracle-and-open-source-projects-the-interviews/" rel="bookmark" title="Oracle &amp; Open Source Projects – The Interviews">Oracle &#038; Open Source Projects &#8211; The Interviews</a></li><li><a href="http://www.iheavy.com/2011/06/04/open-source-what-is-it-and-why-is-it-important/" rel="bookmark" title="Open Source – What is it and why is it important?">Open Source – What is it and why is it important?</a></li><li><a href="http://www.iheavy.com/2005/01/01/open-insights-03-the-business-of-open-source/" rel="bookmark" title="Open Insights 03 – The Business of Open Source">Open Insights 03 – The Business of Open Source</a></li></ol><p>For more articles like these go to <a href="http://www.iheavy.com">iHeavy, Inc +1-212-533-6828</a></p><img src="http://feeds.feedburner.com/~r/iheavy/~4/QclF0DCn6Dw" height="1" width="1" /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33785&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33785&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:9:"Sean Hull";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:8;a:6:{s:4:"data";s:88:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:34:"So now Hadoop's days are numbered?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:69:"tag:blogger.com,1999:blog-6415786925319620734.post-953914713481895351";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:86:"http://database-scalability.blogspot.com/2012/07/so-now-hadoops-days-are-numbered.html";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2671:"Earlier this week we all read GigaOM's article with this title:"Why the days are numbered for Hadoop as we know it"I know GigaOM like to provoke scandals sometimes, we all remember some other unforgettable piece, but there is something behind it...Hadoop today (after SOA not so long ago) is one of the worst case of an abused buzzword ever known to men. It's everything, everywhere, can cure illnesses and do "big-data" at the same time! Wow!&nbsp;Actually Hadoop is a software framework that supports data-intensive distributed applications, derived from Google's MapReduce and Google File System (GFS) papers.My take from the article is this:&nbsp;Hadoop is a foundation, low-level platform. I used the word "platform" just because of a lack of a better word. Wait there is a great word that captures it all!&nbsp;This word is Assembler.&nbsp;When computers begun 70 years ago or so, Assembly is the mother of all programming languages, Assembler made it work in real world computers, silicone and copper. In the world of Big Data, map-reduce, massive distribution and parallelism is the mother of all living things (Assembly). And Hadoop enables it to actually run in the real world (Assembler)...&nbsp;Like Assembler, Hadoop core is far from being really usable. &nbsp;Doing something real, good, working, repeatable with it requires skills that only a few people can really master (Like good Assembler programmers, back in 1960's).While I consider myself lucky to have the chance to actually punch cards with brilliant(?) Assembler code, many of today's brightest minds in Silicone Valleys around the world never wrote one opcode. They're all using PHP, Ruby, Java and node.js, which are great "wrappers" around good old Assembly to bring programming, innovation, disruptiveness - to the masses, make the whole world a better place. It's how it should be.Hadoop will die only if data and big data dies. Nonsense. Data is by far the most important asset organizations have. Facebook as well as Bank Of America will be worth a fraction of their value in minutes if they loose the same fraction of their data. Both won't be able to compete if they can't be intelligent and analyze their data that multiplies every (low number) days/weeks/months. The data makes a business intelligent and Hadoop helps exactly there.&nbsp;Hadoop is the Assembler of all analytical big data processing, ETL and queries. The potential around it and its ecosystem is literally unlimited, tons of&nbsp;innovation and&nbsp;disruptiveness are poured by startups and communities all over, like Splunk, HBase, Cloudera, Hive, Hadapt, and many many more. And we're just in the "FORTRAN" phase...";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 10 Jul 2012 20:28:01 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:10:{i:0;a:5:{s:4:"data";s:9:"Scale out";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:11:"Scalability";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:10:"map reduce";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:14:"Data warehouse";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:8:"Database";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:8:"big data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:20:"Database scalability";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:9:"analytics";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:8;a:5:{s:4:"data";s:11:"Parallelism";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:9;a:5:{s:4:"data";s:6:"Hadoop";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:3986:"Earlier this week we all read GigaOM's <a href="http://gigaom.com/cloud/why-the-days-are-numbered-for-hadoop-as-we-know-it/">article</a> with this title:<br /><blockquote>"Why the days are numbered for Hadoop as we know it"</blockquote>I know GigaOM like to provoke scandals sometimes, we all remember some other unforgettable <a href="http://gigaom.com/cloud/facebook-trapped-in-mysql-fate-worse-than-death/">piece</a>, but there is something behind it...<br /><br />Hadoop today (after SOA not so long ago) is one of the worst case of an abused buzzword ever known to men. It's everything, everywhere, can cure illnesses and do "big-data" at the same time! Wow!&nbsp;<span>Actually Hadoop is a software framework that supports data-intensive distributed applications, derived from Google's MapReduce and Google File System (GFS) papers.</span><br /><br />My take from the article is this:&nbsp;<span>Hadoop is a foundation, low-level platform. I used the word "platform" just because of a lack of a better word. Wait there is a great word that captures it all!&nbsp;</span><br /><span><br /></span><br /><span>This word is <b><u>Assembler</u></b>.&nbsp;</span><br /><span><br /></span><br /><span>When computers begun 70 years ago or so, Assembly is the mother of all programming languages, Assembler made it work in real world computers, silicone and copper. In the world of Big Data, map-reduce, massive distribution and parallelism is the mother of all living things (Assembly). And Hadoop enables it to actually run in the real world (Assembler)...&nbsp;</span><br /><span><br /></span><br /><span>Like Assembler, Hadoop core is far from being really usable. &nbsp;Doing something real, good, working, repeatable with it requires skills that only a few people can really master (Like good Assembler programmers, back in 1960's).</span><br /><span><br /></span><br /><div><a href="http://2.bp.blogspot.com/-TE6D1JsrwZY/T_yNAaM1GMI/AAAAAAAAFzE/7MUY3Eg4Cho/s1600/punchCard.gif" imageanchor="1"><img border="0" height="181" src="http://2.bp.blogspot.com/-TE6D1JsrwZY/T_yNAaM1GMI/AAAAAAAAFzE/7MUY3Eg4Cho/s400/punchCard.gif" width="400" /></a></div><span><br /></span><br /><span>While I consider myself lucky to have the chance to actually punch cards with brilliant(?) Assembler code, </span><span>many of today's brightest minds in Silicone Valleys around the world never wrote one opcode. They're all using PHP, Ruby, Java and node.js, which are great "wrappers" around good old Assembly to bring programming, innovation, </span><span>disruptiveness - to the masses, make the whole world a better place. It's how it should be.</span><br /><span><br /></span><br /><span>Hadoop will die only if data and big data dies. Nonsense. Data is by far the most important asset organizations have. Facebook as well as Bank Of America will be worth a fraction of their value in minutes if they loose the same fraction of their data. Both won't be able to compete if they can't be intelligent and analyze their data that multiplies every (low number) days/weeks/months. The d</span><span>ata makes a business intelligent and </span><span>Hadoop helps exactly there.&nbsp;</span><br /><span><br /></span><br /><span>Hadoop is the Assembler of all analytical big data processing, ETL and queries. The potential around it and its ecosystem is literally unlimited, tons of&nbsp;</span><span>innovation and&nbsp;</span><span>disruptiveness are poured by s</span><span>tartups and communities all over, like Splunk, HBase, Cloudera, Hive, Hadapt, and many many more. And we're just in the "FORTRAN" phase...</span><div><img width="1" height="1" src="https://blogger.googleusercontent.com/tracker/6415786925319620734-953914713481895351?l=database-scalability.blogspot.com" alt="" /></div><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33783&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33783&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Doron Levari";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:9;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:54:"Recent Presentations at Charlotte South East LinuxFest";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:38:"http://ronaldbradford.com/blog/?p=3993";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:97:"http://ronaldbradford.com/blog/recent-presentations-at-charlotte-south-east-linuxfest-2012-07-10/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1273:"At the recent South East LinuxFest in June 2012 I gave two MySQL presentations.
The first was on Explaining the MySQL Explain. This presentation details the MySQL Query Execution Plan (QEP) of an SQL statement and how to understand and interpret the information from the EXPLAIN command.  Also discussed are additional commands and tools that exist to add supplementary information. These are essential skills that will be used daily in production operations.  Download Presentation (PDF)
More detailed information about EXPLAIN and associated commands is available in book Effective MySQL: Optimizing SQL Statements.

The second was on MySQL Disasters, and how to avoid yours. Organizations are always making improvements for scalability, however disaster preparedness is the poor cousin. This presentation will show you how to easily avoid the most common MySQL disaster situations.
Backup and recovery is critical for business continuity, many websites run the risk of data loss or corruption because existing procedures (if any) are generally flawed.
Download Presentation (PDF
More detailed information about the right backup and recovery strategy  and associated tools is available in book Effective MySQL: Backup and Recovery.
References
South East Linux Fest Agenda";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 10 Jul 2012 15:22:24 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:9:"Databases";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:12:"Professional";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2621:"<p>At the recent <a href="http://www.southeastlinuxfest.org/">South East LinuxFest</a> in June 2012 I gave two MySQL presentations.</p>
<p>The first was on <a href="http://ronaldbradford.com/mysql-presentations/ExplainingTheMySQLEXPLAIN-SELF2012.pdf">Explaining the MySQL Explain</a>. This presentation details the MySQL Query Execution Plan (QEP) of an SQL statement and how to understand and interpret the information from the <a href="http://dev.mysql.com/doc/refman/5.5/en/explain.html">EXPLAIN</a> command.  Also discussed are additional commands and tools that exist to add supplementary information. These are essential skills that will be used daily in production operations.  <a href="http://ronaldbradford.com/mysql-presentations/ExplainingTheMySQLEXPLAIN-SELF2012.pdf">Download Presentation (PDF)</a></p>
<p><img style="float:right; margin:10px" src="http://media.ronaldbradford.com/images/effective-mysql-optimizing-sql-statements.png" alt="Effective MySQL: Optimizing SQL Statements" />More detailed information about EXPLAIN and associated commands is available in book <a href="http://effectivemysql.com/book/optimizing-sql-statements/">Effective MySQL: Optimizing SQL Statements</a>.</p>
<p><img style="float:right; margin:10px; clear:both" src="http://media.ronaldbradford.com/images/effective-mysql-backup-recovery.png" alt="Effective MySQL:Backup and Recovery" /><br />
The second was on <a href="http://ronaldbradford.com/mysql-presentations/AvoidingAMySQLDisaster-SELF2012.pdf">MySQL Disasters, and how to avoid yours</a>. Organizations are always making improvements for scalability, however disaster preparedness is the poor cousin. This presentation will show you how to easily avoid the most common MySQL disaster situations.<br />
Backup and recovery is critical for business continuity, many websites run the risk of data loss or corruption because existing procedures (if any) are generally flawed.<br />
<a href="http://ronaldbradford.com/mysql-presentations/AvoidingAMySQLDisaster-SELF2012.pdf">Download Presentation (PDF</a></p>
<p>More detailed information about the right backup and recovery strategy  and associated tools is available in book <a href="http://effectivemysql.com/book/backup-recovery/">Effective MySQL: Backup and Recovery</a>.</p>
<h3>References</h3>
<p><a href="http://www.southeastlinuxfest.org/SELF-2012-Schedule2.pdf">South East Linux Fest Agenda</a></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33782&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33782&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"Ronald Bradford";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:10;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:18:"EOL of MySQL Forge";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:55:"https://blogs.oracle.com/MySQL/entry/eol_of_mysql_forge";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:55:"https://blogs.oracle.com/MySQL/entry/eol_of_mysql_forge";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1064:"Forge was intended to be a
                                        community wiki resource for
                                        sharing information with each
                                        other. &nbsp; However, over the last
                                        few years, we have seen Forge
                                        used less and&nbsp;less by MySQL
                                        Community, and more by spammers.
                                        What happened?   
  MySQL Worklogs and MySQL
                                        Internals documentation will be
                                        moved to&nbsp;dev.mysql.com
                                        and with new anti spam measures
                                        in place. 
    
  The MySQL Wiki, which was the
                                        primary focus of forge.mysql.com
                                        has been migrated to https://wikis.oracle.com/display/mysql 
    
  MySQL Forge will EOL on August 1st 2012.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 23:03:32 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:9:"Community";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"forge";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1574:"<div>Forge was intended to be a
                                        community wiki resource for
                                        sharing information with each
                                        other. &nbsp; However, over the last
                                        few years, we have seen Forge
                                        used less and&nbsp;less by MySQL
                                        Community, and more by spammers.
                                        What happened?<br /> <br /> </div> 
  <div>MySQL Worklogs and MySQL
                                        Internals documentation will be
                                        moved to&nbsp;<a href="http://dev.mysql.com/" moz-do-not-send="true">dev.mysql.com</a>
                                        and with new anti spam measures
                                        in place.</div> 
  <div><br /> </div> 
  <div>The MySQL Wiki, which was the
                                        primary focus of <a href="http://forge.mysql.com/" moz-do-not-send="true">forge.mysql.com</a>
                                        has been migrated to <a href="https://wikis.oracle.com/display/mysql/Home" moz-do-not-send="true">https://wikis.oracle.com/display/mysql</a></div> 
  <div><br /> </div> 
  <div>MySQL Forge will EOL on August 1st 2012.</div><br /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33780&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33780&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Keith Larson";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:11;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:36:"Effective MySQL: Backup and Recovery";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:41:"http://opensourcedba.wordpress.com/?p=888";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:82:"http://opensourcedba.wordpress.com/2012/07/09/effective-mysql-backup-and-recovery/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:387:"Effective MySQL: Backup and Recovery by Ronald Bradford Effective MySQL: Backup and Recovery by Ronald Bradford is hot off the press!  This is the second book in the series and I hope to have a review shortly.  Meanwhile keep an eye out for it and the first book in the series &#8212; Effective MySQL Optimizing SQL Statements. Ronald&#8217;s style is concise and fluff free.  
         ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 21:43:15 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2914:"<p><div><a href="http://opensourcedba.files.wordpress.com/2012/07/effectivemysql001.jpg"><img src="http://opensourcedba.files.wordpress.com/2012/07/effectivemysql001.jpg?w=99&amp;h=150" alt="" title="effectivemysql001" width="99" height="150" class="size-thumbnail wp-image-889" /></a><p>Effective MySQL: Backup and Recovery by Ronald Bradford</p></div> <a href="http://www.amazon.com/Effective-MySQL-Backup-Recovery-Oracle/dp/0071788573/ref=sr_1_3?ie=UTF8&amp;qid=1341869837&amp;sr=8-3&amp;keywords=effective+mysql" target="_blank">Effective MySQL: Backup and Recovery</a> by Ronald Bradford is hot off the press!  This is the second book in the series and I hope to have a review shortly.  Meanwhile keep an eye out for it and the first book in the series &#8212; <a href="http://www.amazon.com/Effective-MySQL-Optimizing-Statements-Oracle/dp/0071782796/ref=sr_1_1?ie=UTF8&amp;qid=1341869837&amp;sr=8-1&amp;keywords=effective+mysql" target="_blank">Effective MySQL Optimizing SQL Statements</a>. Ronald&#8217;s style is concise and fluff free.  </p>
<br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godelicious/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/delicious/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gofacebook/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/facebook/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gotwitter/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/twitter/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gostumble/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/stumble/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godigg/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/digg/opensourcedba.wordpress.com/888/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/goreddit/opensourcedba.wordpress.com/888/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/reddit/opensourcedba.wordpress.com/888/" /></a> <img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=opensourcedba.wordpress.com&amp;blog=15386988&amp;post=888&amp;subd=opensourcedba&amp;ref=&amp;feed=1" width="1" height="1" /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33779&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33779&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:11:"Dave Stokes";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:12;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:9:"OSCON2012";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:70:"tag:blogger.com,1999:blog-3812360659149323517.post-6084756863816119173";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:51:"http://sqlhjalp.blogspot.com/2012/07/oscon2012.html";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:875:"OSCON2012 is just a few days away !  I will be in Portland for OSCON as well as the Community Leadership summit.&nbsp; I look forward to meeting with everyone.  OSCON has another great lineup of database sessions below are just a few: 1:30pmMonday, 07/16/2012    MySQL Cluster and NoSQL  John David Duncan (Oracle Corp.), Craig L Russell (Oracle Corporation)  11:30amWednesday, 07/18/2012  Storm: distributed and fault-tolerant realtime computation DataNathan Marz (Twitter)  2:30pmWednesday, 07/18/2012  Sensor Network Data Collection and Storage DataCharles Bell (Oracle)  1:40pmThursday, 07/19/2012  InnoDB: Status, Architecture, and Latest Enhancements DataCalvin Sun (MySQL/Oracle)  2:30pmThursday, 07/19/2012  Advanced MySQL Replication Architectures DataLuís Soares (Oracle)  10:00amFriday, 07/20/2012  Optimizing MySQL Configuration DataPeter Zaitsev (Percona Inc)  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 20:07:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4315:"<br /><div><a href="http://www.oscon.com/oscon2012">OSCON2012</a> is just a few days away !  </div><div><br /></div><div>I will be in Portland for <a href="http://www.oscon.com/oscon2012">OSCON</a> as well as the <a href="http://www.communityleadershipsummit.com/">Community Leadership summi</a>t.&nbsp; I look forward to meeting with everyone.  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012">OSCON</a> has another great<a href="http://www.oscon.com/oscon2012/public/schedule/topic/807"> lineup of database sessions</a> below are just a few: </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-16-13:30">1:30pm</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-16">Monday, 07/16/2012</a>    </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/24077" name="session24077">MySQL Cluster and NoSQL</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/835">John David Duncan</a> (Oracle Corp.), <a href="http://www.oscon.com/oscon2012/public/schedule/speaker/3474">Craig L Russell</a> (Oracle Corporation)  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-18-11:30">11:30am</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-18">Wednesday, 07/18/2012</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/24054" name="session24054">Storm: distributed and fault-tolerant realtime computation</a> <a href="http://www.oscon.com/oscon2012/public/schedule/topic/Data">Data</a></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/101323">Nathan Marz</a> (Twitter)  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-18-14:30">2:30pm</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-18">Wednesday, 07/18/2012</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/23966" name="session23966">Sensor Network Data Collection and Storage</a> <a href="http://www.oscon.com/oscon2012/public/schedule/topic/Data">Data</a></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/109226">Charles Bell</a> (Oracle)  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-19-13:40">1:40pm</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-19">Thursday, 07/19/2012</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/24321" name="session24321">InnoDB: Status, Architecture, and Latest Enhancements </a><a href="http://www.oscon.com/oscon2012/public/schedule/topic/Data">Data</a></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/12396">Calvin Sun</a> (MySQL/Oracle)  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-19-14:30">2:30pm</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-19">Thursday, 07/19/2012</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/23994" name="session23994">Advanced MySQL Replication Architectures </a><a href="http://www.oscon.com/oscon2012/public/schedule/topic/Data">Data</a></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/132198">Luís Soares</a> (Oracle)  </div><div><br /></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/full#s2012-07-20-10:00">10:00am</a><a href="http://www.oscon.com/oscon2012/public/schedule/grid/2012-07-20">Friday, 07/20/2012</a>  </div><div><a href="http://www.oscon.com/oscon2012/public/schedule/detail/24445" name="session24445">Optimizing MySQL Configuration</a> <a href="http://www.oscon.com/oscon2012/public/schedule/topic/Data">Data</a></div><div><a href="http://www.oscon.com/oscon2012/public/schedule/speaker/246">Peter Zaitsev</a> (Percona Inc)  </div><div><br /></div><div><img width="1" height="1" src="https://blogger.googleusercontent.com/tracker/3812360659149323517-6084756863816119173?l=sqlhjalp.blogspot.com" alt="" /></div><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33778&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33778&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Keith Larson";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:13;a:6:{s:4:"data";s:83:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:66:"MySQL on S3: performance with storage located across the continent";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:32:"http://www.oblaksoft.com/?p=2718";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:79:"http://www.oblaksoft.com/mysql-on-s3-performance-with-storage-across-continent/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4449:"Can OLTP database workloads use Amazon S3 as primary storage? Now they can, thanks to the Cloud Storage Engine (ClouSE), but the question is: how fast? 

ClouSE on “across the street” vs. “across the continent” cloud storage
ClouSE vs InnoDB
ClouSE benefits


To answer the question about performance, we decided to run db_STRESS benchmark on a MySQL database in Amazon EC2.  We compared 3 configurations:

“Across the street storage”: ClouSE with data stored in S3 in the same region (US Standard)
“Across the continent storage”: ClouSE with data stored in S3 in another region (US West-2)
“Local storage”: InnoDB (the default MySQL storage engine)  with data on an EBS volume

Each configuration was tested with 2, 8, and 64 concurrent users.  Each run was 9 minutes long with 1 minute sleep in between runs.  The transaction mix was biased towards writes, which put the I/O subsystem to test. For data collection and reporting we used Zabbix 2.0.

ClouSE on “across the street” vs. “across the continent” cloud storage
The following graph shows the number of MySQL operations that the database can process per second.  The db_STRESS benchmark sets autocommit on, so each operation runs in its own transaction.  For each 2 select queries db_STRESS executes 1 insert + 1 delete + 1 update, so there are 3 modifying transactions for each 2 read-only transactions.


Graph 1: “across the street” ops on MySQL w/ClouSE.
The data on graph 1 corresponds to MySQL with ClouSE that stored data in Amazon S3 in the US Standard region, located in N. Virginia.  The Amazon EC2 instance ran in the US Standard region as well, so the storage was “across the street”.
Now, here is graph 2 that depicts the processing rate for MySQL with ClouSE that stored data in Amazon S3 in the US West-2 region, located in Oregon.  The Amazon EC2 instance ran in the US Standard region, so the storage was located across the continent.


Graph 2: “across the continent” ops on MySQL w/ClouSE.
The data shows that moving storage away didn’t slow the database server down.  ClouSE was indeed able to hide network latency when working with remote storage.  Designed and optimized for cloud storage from the ground up, ClouSE was able to process the same amount of data with cloud storage across the street as with cloud storage across the continent.
Why is it interesting?  With ClouSE, cloud storage becomes a true utility that can be used from anywhere, not just inside the cloud.   ClouSE makes cloud storage a viable drop-in replacement for good old SAN.  Moreover, it does so in a fully secure manner: ClouSE encrypts the data and customers are in charge of encryption key management, so they fully control who has access to their data, if anyone.

ClouSE vs InnoDB
We ran the same benchmark with InnoDB as well.  InnoDB is the default storage engine for MySQL 5.5 that uses local disk.


Graph 3: local storage ops on MySQL w/InnoDB.
On graph 3, it easy to see that ClouSE with storage across the continent works just as well as InnoDB with local disk.

ClouSE benefits
The benchmark showed that the I/O performance on local vs cloud storage is a close match. This test demonstrates how ClouSE makes the cloud storage that is on the other side of the continent as fast as a local disk for database workloads.
You can now use cloud storage with your existing MySQL applications and tools &#8211; cloud vs. local storage becomes a simple deployment choice. Note that the data can be stored in the cloud regardless of where the application runs: on premise, private cloud or EC2.
Without changing a line of code MySQL databases can enjoy all the benefits of cloud storage – scaling cost with usage, high storage availability and reliability, quick and easy disaster recovery. Using cloud storage has never been this easy – the app stays operational during migration, the adoption can be both gradual and partial, and you can just as easily go back to the local storage.
&nbsp;
The full performance report with detailed configuration, statistics, and side-by-size performance comparisons is available for download.
Ready to check out the new Cloud Storage Engine for MySQL? ClouSE 1.0b.1.2 Beta is available for FREE download from http://www.oblaksoft.com/downloads/.
Can we assist you in migrating your database to cloud? We’d love to.
See also:
WordPress on S3: run it anywhere.
Making the Cloud Work for You.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 17:30:38 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:9:{i:0;a:5:{s:4:"data";s:13:"Founders BLOG";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"cloud";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"ClouSE";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:8:"database";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:6:"innodb";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:11:"performance";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:2:"S3";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:8;a:5:{s:4:"data";s:7:"storage";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:6452:"<p><em>Can OLTP database workloads use Amazon S3 as primary storage? Now</em> <em>they can, thanks to the Cloud Storage Engine (</em><a href="http://www.oblaksoft.com/documentation/"><em>ClouSE</em></a><em>), but the question is: how fast? </em></p>
<ul>
<li><a href="http://www.oblaksoft.com#_S3">ClouSE on “across the street” vs. “across the continent” cloud storage</a></li>
<li><a href="http://www.oblaksoft.com#_InnoDB">ClouSE vs InnoDB</a></li>
<li><a href="http://www.oblaksoft.com#_ClouSE_Benefits">ClouSE benefits</a></li>
</ul>
<p><span></span><br />
To answer the question about performance, we decided to run <a href="http://dimitrik.free.fr/db_STRESS.html">db_STRESS</a> benchmark on a MySQL database in Amazon EC2.  We compared 3 configurations:</p>
<ul>
<li>“Across the street storage”: ClouSE with data stored in S3 in the same region (US Standard)</li>
<li>“Across the continent storage”: ClouSE with data stored in S3 in another region (US West-2)</li>
<li>“Local storage”: InnoDB (the default MySQL storage engine)  with data on an EBS volume</li>
</ul>
<p>Each configuration was tested with 2, 8, and 64 concurrent users.  Each run was 9 minutes long with 1 minute sleep in between runs.  The transaction mix was biased towards writes, which put the I/O subsystem to test. For data collection and reporting we used <a href="http://www.zabbix.com">Zabbix</a> 2.0.</p>
<p><a name="_S3"></a></p>
<h2>ClouSE on “across the street” vs. “across the continent” cloud storage</h2>
<p>The following graph shows the number of MySQL operations that the database can process per second.  The db_STRESS benchmark sets autocommit on, so each operation runs in its own transaction.  For each 2 select queries db_STRESS executes 1 insert + 1 delete + 1 update, so there are 3 modifying transactions for each 2 read-only transactions.</p>
<p><img src="http://www.oblaksoft.com/wp-content/uploads/2012/07/perf_1_same_region_s3.png" alt="" title="performance ClouSE on same region S3 " width="616" height="304" class="alignleft size-full wp-image-2721" /></p>
<div></div>
<p><strong>Graph 1: “across the street” ops on MySQL w/ClouSE.</strong></p>
<p>The data on graph 1 corresponds to MySQL with ClouSE that stored data in Amazon S3 in the US Standard region, located in N. Virginia.  The Amazon EC2 instance ran in the US Standard region as well, so the storage was “across the street”.</p>
<p>Now, here is graph 2 that depicts the processing rate for MySQL with ClouSE that stored data in Amazon S3 in the US West-2 region, located in Oregon.  The Amazon EC2 instance ran in the US Standard region, so the storage was located across the continent.</p>
<p><img src="http://www.oblaksoft.com/wp-content/uploads/2012/07/perf_2_diff_region_s3.png" alt="" title="performance ClouSE on across the continent S3" width="616" height="304" class="alignnone size-full wp-image-2722" /></p>
<div></div>
<p><strong>Graph 2: “across the continent” ops on MySQL w/ClouSE.</strong></p>
<p>The data shows that moving storage away didn’t slow the database server down.  ClouSE was indeed able to hide network latency when working with remote storage.  Designed and optimized for cloud storage from the ground up, ClouSE was able to process the same amount of data with cloud storage across the street as with cloud storage across the continent.</p>
<p>Why is it interesting?  With ClouSE, cloud storage becomes a true utility that can be used from anywhere, not just inside the cloud.   ClouSE makes cloud storage a viable drop-in replacement for good old SAN.  Moreover, it does so in a fully secure manner: ClouSE encrypts the data and customers are in charge of encryption key management, so they fully control who has access to their data, if anyone.</p>
<p><a name="_InnoDB"></a></p>
<h2>ClouSE vs InnoDB</h2>
<p>We ran the same benchmark with InnoDB as well.  InnoDB is the default storage engine for MySQL 5.5 that uses local disk.</p>
<p><img src="http://www.oblaksoft.com/wp-content/uploads/2012/07/perf_3_innodb.png" alt="" title="performance Innodb" width="616" height="304" class="alignnone size-full wp-image-2723" /></p>
<div></div>
<p><strong>Graph 3: local storage ops on MySQL w/InnoDB.</strong></p>
<p>On graph 3, it easy to see that ClouSE with storage across the continent works just as well as InnoDB with local disk.</p>
<p><a name="_ClouSE_Benefits"></a></p>
<h2>ClouSE benefits</h2>
<p>The benchmark showed that the I/O performance on local vs cloud storage is a close match. This test demonstrates how ClouSE makes the cloud storage that is on the other side of the continent as fast as a local disk for database workloads.</p>
<p>You can now use cloud storage with your existing MySQL applications and tools &#8211; cloud vs. local storage becomes a simple deployment choice. Note that the data can be stored in the cloud regardless of where the application runs: on premise, private cloud or EC2.</p>
<p>Without changing a line of code MySQL databases can enjoy all the benefits of cloud storage – scaling cost with usage, high storage availability and reliability, quick and easy disaster recovery. Using cloud storage has never been this easy – the app stays operational during migration, the adoption can be both gradual and partial, and you can just as easily go back to the local storage.</p>
<p>&nbsp;</p>
<p>The full performance report with detailed configuration, statistics, and side-by-size performance comparisons is available for <a href="http://www.oblaksoft.com/downloads/get/?target=/clouse-perf-062812.pdf">download</a>.</p>
<p>Ready to check out the new Cloud Storage Engine for MySQL? ClouSE 1.0b.1.2 Beta is available for FREE download from <a href="http://www.oblaksoft.com/downloads/">http://www.oblaksoft.com/downloads/</a>.</p>
<p><em>Can we assist you in migrating your database to cloud? </em><a href="http://www.oblaksoft.com/about-us/email-us/"><em>We’d love to</em></a><em>.</em></p>
<p><strong>See also:</strong></p>
<p>WordPress on S3: <a href="http://www.oblaksoft.com/wordpress-on-s3-run-it-anywhere/">run it anywhere</a>.</p>
<p><a href="http://www.oblaksoft.com/docs/making_cloud_work.pdf">Making the Cloud Work for You</a>.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33777&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33777&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:14:"Artem Livshits";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:14;a:6:{s:4:"data";s:58:"
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:30:"Helper Functions for ps_helper";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.markleith.co.uk/?p=1057";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:151:"http://www.markleith.co.uk/2012/07/09/helper-functions-for-ps_helper/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=helper-functions-for-ps_helper";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:8353:"I love our community.
Not long after posting my update on ps_helper, I had a couple of comments around the formatting of values within the output. Daniël van Eeden gave the suggestion that I could add a couple of Stored Functions, for formatting byte and time based values.
Of course, this was a great idea &#8211; not least for myself, because I no longer have to worry about how to format certain columns in the output.
I&#8217;ve added the following:
format_bytes()
format_time()
format_path()
format_statement()
extract_schema_from_file_name()
extract_table_from_file_name()
I&#8217;ve updated all of the views within ps_helper to use these new functions as well (I may have missed some).
Along the way, I caught a couple of bugs (a missing SUM on the COUNT columns of the file IO summaries), and added a couple of updates.
The first was to update the output for the schema_table_statistics view, to include more of the data available within Performance Schema, and also to start using some of the data from the INFORMATION_SCHEMA.INNODB_BUFFER_PAGE table that is now available within MySQL 5.6.
Now the output looks like this:

mysql&gt; select * from schema_table_statistics limit 1\G
*************************** 1. row ***************************
                 table_schema: mem
                   table_name: mysqlserver
                 rows_fetched: 27087
                fetch_latency: 442.72 ms
                rows_inserted: 2
               insert_latency: 185.04 µs 
                 rows_updated: 5096
               update_latency: 1.39 s
                 rows_deleted: 0
               delete_latency: 0 ps
             io_read_requests: 2565
                io_read_bytes: 1121627
              io_read_latency: 10.07 ms
            io_write_requests: 1691
               io_write_bytes: 128383
             io_write_latency: 14.17 ms
             io_misc_requests: 2698
              io_misc_latency: 433.66 ms
          innodb_buffer_pages: 19
   innodb_buffer_pages_hashed: 19
      innodb_buffer_pages_old: 19
innodb_buffer_bytes_allocated: 311296
     innodb_buffer_bytes_data: 1924
    innodb_buffer_rows_cached: 2

I&#8217;ve also added another view within the 5.6 set, called statements_with_runtimes_in_95th_percentile. 
This outputs all statement digests that have an average run time, in microseconds, that are above the 95th percentile of all statements. It&#8217;s output is similar to the statement_analysis view:

mysql&gt; select * from statements_with_runtimes_in_95th_percentile where query not like &#039;show%&#039;;
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+
| query                                                             | full_scan | exec_count | err_count | warn_count | total_latency | max_latency | avg_latency | rows_sent | rows_sent_avg | rows_scanned | digest                           |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+
| SELECT plugin_name FROM inform ... tus = ? ORDER BY plugin_name   | *         |        169 |         0 |          0 | 2.37 s        | 64.45 ms    | 14.03 ms    |      4394 |            26 |        10816 | 23234b56a0b1f1e350bf51bef3050747 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 170.08 ms     | 5.68 ms     | 1.44 ms     |     13582 |           115 |        13582 | 34694223091aee1380c565076b7dfece |
| SELECT CAST ( SUM_NUMBER_OF_BY ... WHERE EVENT_NAME = ? LIMIT ?   | *         |        566 |         0 |          0 | 779.56 ms     | 2.93 ms     | 1.38 ms     |       342 |             1 |        17286 | 58d34495d29ad818e68c859e778b0dcb |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 153.35 ms     | 3.06 ms     | 1.30 ms     |     13228 |           112 |        13228 | b816579565d5a2882cb8bd496193dc00 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 143.31 ms     | 4.57 ms     | 1.21 ms     |     13646 |           116 |        13646 | 27ff8681eb2c8cf999233e7507b439fe |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 143.04 ms     | 7.22 ms     | 1.21 ms     |     13584 |           115 |        13584 | 10b863f20e83dcd9c7782dac249acbb0 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 137.46 ms     | 16.73 ms    | 1.16 ms     |     13922 |           118 |        13922 | 351ebc26af6babb67570843bcc97f6b0 |
| UPDATE `mem30__inventory` . `R ... mestamp` = ? WHERE `hid` = ?   |           |        114 |         0 |          0 | 127.64 ms     | 30.33 ms    | 1.12 ms     |         0 |             0 |          114 | f4ecf2aebe212e7ed250a0602d86c389 |
| UPDATE `mem30__inventory` . `I ... ` = ? , `hasOldBlocksTime` ... |           |         56 |         0 |          0 | 61.05 ms      | 16.41 ms    | 1.09 ms     |         0 |             0 |           56 | cdc78c70d83c505c5708847ba810d035 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 121.76 ms     | 1.95 ms     | 1.03 ms     |     13936 |           118 |        13936 | 20f97c53c2a59f5eadc06b2fa90fbe75 |
| UPDATE `mem30__inventory` . `M ... mpileOs` = ? WHERE `hid` = ?   |           |        114 |         0 |          0 | 114.16 ms     | 22.34 ms    | 1.00 ms     |         0 |             0 |          114 | c5d4a65f3f308f4869807e730739af6d |
| CALL `dc_string_insert` (...)                                     |           |         80 |         0 |          0 | 79.89 ms      | 2.62 ms     | 998.50 ┬╡s  |         0 |             0 |          240 | 93eb9cab8ced45cf3b98400e8803f8af |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 116.19 ms     | 1.32 ms     | 984.60 ┬╡s  |     13484 |           114 |        13484 | bd23afed9a41367591e2b71dac76f334 |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+

And to give an example using some of the bytes formatting as well, here&#8217;s an example for the latest_file_io view:

mysql&gt; select * from latest_file_io limit 10;
+----------------------+----------------------------------------+------------+-----------+-----------+
| thread               | file                                   | latency    | operation | requested |
+----------------------+----------------------------------------+------------+-----------+-----------+
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 9.26 µs    | write     | 124 bytes |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 4.00 µs    | write     | 2 bytes   |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 56.34 µs   | close     | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYD             | 53.93 µs   | close     | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 104.05 ms  | delete    | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYD             | 661.18 µs  | delete    | NULL      |
| msandbox@localhost:1 | @@datadir/Cerberus.log                 | 35.99 ms   | write     | 57 bytes  |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 7.40 µs    | stat      | NULL      |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 9.81 µs    | open      | NULL      |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 16.06 µs   | read      | 3.17 KiB  |
+----------------------+----------------------------------------+------------+-----------+-----------+

Keep the feedback coming!  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 16:27:45 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:4:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:18:"performance_schema";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:9:"ps_helper";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:17:"statement digests";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:9854:"<p>I love our community.</p>
<p>Not long after posting my update on <a href="http://www.markleith.co.uk/ps_helper/">ps_helper</a>, I had a couple of comments around the formatting of values within the output. Daniël van Eeden gave the suggestion that I could add a couple of <a href="http://dev.mysql.com/doc/refman/5.0/en/create-procedure.html">Stored Functions</a>, for formatting byte and time based values.</p>
<p>Of course, this was a great idea &#8211; not least for myself, because I no longer have to worry about how to format certain columns in the output.</p>
<p>I&#8217;ve added the following:</p>
<p><a href="http://www.markleith.co.uk/ps_helper/#fn_format_bytes">format_bytes()</a><br />
<a href="http://www.markleith.co.uk/ps_helper/#fn_format_time">format_time()</a><br />
<a href="http://www.markleith.co.uk/ps_helper/#fn_format_path">format_path()</a><br />
<a href="http://www.markleith.co.uk/ps_helper/#fn_format_statement">format_statement()</a><br />
<a href="http://www.markleith.co.uk/ps_helper/#fn_extract_schema_from_file_name">extract_schema_from_file_name()</a><br />
<a href="http://www.markleith.co.uk/ps_helper/#fn_extract_table_from_file_name">extract_table_from_file_name()</a></p>
<p>I&#8217;ve updated all of the views within ps_helper to use these new functions as well (I may have missed some).</p>
<p>Along the way, I caught a couple of bugs (a missing SUM on the COUNT columns of the file IO summaries), and added a couple of updates.</p>
<p>The first was to update the output for the <a href="http://www.markleith.co.uk/ps_helper/#schema_table_statistics">schema_table_statistics view</a>, to include more of the data available within Performance Schema, and also to start using some of the data from the <a href="http://dev.mysql.com/doc/refman/5.6/en/innodb-buffer-page-table.html">INFORMATION_SCHEMA.INNODB_BUFFER_PAGE</a> table that is now available within MySQL 5.6.</p>
<p>Now the output looks like this:</p>
<pre>
mysql&gt; select * from schema_table_statistics limit 1\G
*************************** 1. row ***************************
                 table_schema: mem
                   table_name: mysqlserver
                 rows_fetched: 27087
                fetch_latency: 442.72 ms
                rows_inserted: 2
               insert_latency: 185.04 µs 
                 rows_updated: 5096
               update_latency: 1.39 s
                 rows_deleted: 0
               delete_latency: 0 ps
             io_read_requests: 2565
                io_read_bytes: 1121627
              io_read_latency: 10.07 ms
            io_write_requests: 1691
               io_write_bytes: 128383
             io_write_latency: 14.17 ms
             io_misc_requests: 2698
              io_misc_latency: 433.66 ms
          innodb_buffer_pages: 19
   innodb_buffer_pages_hashed: 19
      innodb_buffer_pages_old: 19
innodb_buffer_bytes_allocated: 311296
     innodb_buffer_bytes_data: 1924
    innodb_buffer_rows_cached: 2
</pre>
<p>I&#8217;ve also added another view within the 5.6 set, called <a href="http://www.markleith.co.uk/ps_helper/#statements_with_runtimes_in_95th_percentile">statements_with_runtimes_in_95th_percentile</a>. </p>
<p>This outputs all statement digests that have an average run time, in microseconds, that are above the 95th percentile of all statements. It&#8217;s output is similar to the <a href="http://www.markleith.co.uk/ps_helper/#statement_analysis">statement_analysis</a> view:</p>
<pre>
mysql&gt; select * from statements_with_runtimes_in_95th_percentile where query not like &#039;show%&#039;;
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+
| query                                                             | full_scan | exec_count | err_count | warn_count | total_latency | max_latency | avg_latency | rows_sent | rows_sent_avg | rows_scanned | digest                           |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+
| SELECT plugin_name FROM inform ... tus = ? ORDER BY plugin_name   | *         |        169 |         0 |          0 | 2.37 s        | 64.45 ms    | 14.03 ms    |      4394 |            26 |        10816 | 23234b56a0b1f1e350bf51bef3050747 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 170.08 ms     | 5.68 ms     | 1.44 ms     |     13582 |           115 |        13582 | 34694223091aee1380c565076b7dfece |
| SELECT CAST ( SUM_NUMBER_OF_BY ... WHERE EVENT_NAME = ? LIMIT ?   | *         |        566 |         0 |          0 | 779.56 ms     | 2.93 ms     | 1.38 ms     |       342 |             1 |        17286 | 58d34495d29ad818e68c859e778b0dcb |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 153.35 ms     | 3.06 ms     | 1.30 ms     |     13228 |           112 |        13228 | b816579565d5a2882cb8bd496193dc00 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 143.31 ms     | 4.57 ms     | 1.21 ms     |     13646 |           116 |        13646 | 27ff8681eb2c8cf999233e7507b439fe |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 143.04 ms     | 7.22 ms     | 1.21 ms     |     13584 |           115 |        13584 | 10b863f20e83dcd9c7782dac249acbb0 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 137.46 ms     | 16.73 ms    | 1.16 ms     |     13922 |           118 |        13922 | 351ebc26af6babb67570843bcc97f6b0 |
| UPDATE `mem30__inventory` . `R ... mestamp` = ? WHERE `hid` = ?   |           |        114 |         0 |          0 | 127.64 ms     | 30.33 ms    | 1.12 ms     |         0 |             0 |          114 | f4ecf2aebe212e7ed250a0602d86c389 |
| UPDATE `mem30__inventory` . `I ... ` = ? , `hasOldBlocksTime` ... |           |         56 |         0 |          0 | 61.05 ms      | 16.41 ms    | 1.09 ms     |         0 |             0 |           56 | cdc78c70d83c505c5708847ba810d035 |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 121.76 ms     | 1.95 ms     | 1.03 ms     |     13936 |           118 |        13936 | 20f97c53c2a59f5eadc06b2fa90fbe75 |
| UPDATE `mem30__inventory` . `M ... mpileOs` = ? WHERE `hid` = ?   |           |        114 |         0 |          0 | 114.16 ms     | 22.34 ms    | 1.00 ms     |         0 |             0 |          114 | c5d4a65f3f308f4869807e730739af6d |
| CALL `dc_string_insert` (...)                                     |           |         80 |         0 |          0 | 79.89 ms      | 2.62 ms     | 998.50 ┬╡s  |         0 |             0 |          240 | 93eb9cab8ced45cf3b98400e8803f8af |
| SELECT `this_` . `target` AS ` ... D `this_` . `timestamp` &lt;= ?   |           |        118 |         0 |          0 | 116.19 ms     | 1.32 ms     | 984.60 ┬╡s  |     13484 |           114 |        13484 | bd23afed9a41367591e2b71dac76f334 |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+---------------+-------------+-------------+-----------+---------------+--------------+----------------------------------+
</pre>
<p>And to give an example using some of the bytes formatting as well, here&#8217;s an example for the <a href="http://www.markleith.co.uk/ps_helper/#latest_file_io">latest_file_io </a>view:</p>
<pre>
mysql&gt; select * from latest_file_io limit 10;
+----------------------+----------------------------------------+------------+-----------+-----------+
| thread               | file                                   | latency    | operation | requested |
+----------------------+----------------------------------------+------------+-----------+-----------+
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 9.26 µs    | write     | 124 bytes |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 4.00 µs    | write     | 2 bytes   |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 56.34 µs   | close     | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYD             | 53.93 µs   | close     | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYI             | 104.05 ms  | delete    | NULL      |
| msandbox@localhost:1 | @@tmpdir/#sqlcf28_1_4e.MYD             | 661.18 µs  | delete    | NULL      |
| msandbox@localhost:1 | @@datadir/Cerberus.log                 | 35.99 ms   | write     | 57 bytes  |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 7.40 µs    | stat      | NULL      |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 9.81 µs    | open      | NULL      |
| msandbox@localhost:1 | @@datadir/ps_helper/latest_file_io.frm | 16.06 µs   | read      | 3.17 KiB  |
+----------------------+----------------------------------------+------------+-----------+-----------+
</pre>
<p>Keep the feedback coming! <img src="http://www.markleith.co.uk/wp-includes/images/smilies/icon_smile.gif" alt=":)" class="wp-smiley" /> </p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33775&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33775&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Mark Leith";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:15;a:6:{s:4:"data";s:63:"
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:70:"MySQL Cluster : Delivering Breakthrough Performance (upcoming webinar)";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:32:"http://www.clusterdb.com/?p=2386";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:224:"http://www.clusterdb.com/mysql-cluster/mysql-cluster-delivering-breakthrough-performance-upcoming-webinar/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mysql-cluster-delivering-breakthrough-performance-upcoming-webinar";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:818:"MySQL Cluster partitioning keyI&#8217;ll be presenting a webinar covering MySQL Cluster performance on Thursday, July 26. As always, the webinar will be free but you&#8217;ll need to register here &#8211; you&#8217;ll then also receive a link to the charts and a recording of the session after the event.
Here&#8217;s the agenda (hoping that I can squeeze it all in!):

Introduction to MySQL Cluster
Where does MySQL Cluster fit?
Benchmarks:

ANALYZE TABLE 
Access patterns 
AQL (fast JOINs) 
Distribution aware
Batching 
Schema optimisations 
Connection pooling 
Multi-threaded data nodes 
High performance NoSQL APIs 
Hardware choices 
More tips


The measure/optimise loop
Techniques to boost performance
Scaling out 
Other useful resources 


The session starts at 9:00 am UK time / 10:00 am Central European time.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 16:13:15 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:5:{i:0;a:5:{s:4:"data";s:13:"MySQL Cluster";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:17:"MySQL Cluster 7.2";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:17:"MySQL Cluster CGE";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:11:"Performance";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1761:"<p><div><a href="http://www.clusterdb.com/wp-content/uploads/2012/07/MySQL-Cluster-partitioning-key.png"><img src="http://www.clusterdb.com/wp-content/uploads/2012/07/MySQL-Cluster-partitioning-key-250x300.png" alt="MySQL Cluster partitioning key" title="MySQL Cluster partitioning key" width="250" height="300" class="size-medium wp-image-2385" /></a><p>MySQL Cluster partitioning key</p></div>I&#8217;ll be presenting a webinar covering MySQL Cluster performance on Thursday, July 26. As always, the webinar will be free but you&#8217;ll need to <a href="http://www.mysql.com/news-and-events/web-seminars/display-719.html" title="MySQL Cluster Essentials: Delivering Breakthrough Performance" target="_blank">register here</a> &#8211; you&#8217;ll then also receive a link to the charts and a recording of the session after the event.</p>
<p>Here&#8217;s the agenda (hoping that I can squeeze it all in!):</p>
<ul>
<li>Introduction to MySQL Cluster</li>
<li>Where does MySQL Cluster fit?</li>
<li>Benchmarks:
<ul>
<li>ANALYZE TABLE </li>
<li>Access patterns </li>
<li>AQL (fast JOINs) </li>
<li>Distribution aware</li>
<li>Batching </li>
<li>Schema optimisations </li>
<li>Connection pooling </li>
<li>Multi-threaded data nodes </li>
<li>High performance NoSQL APIs </li>
<li>Hardware choices </li>
<li>More tips</li>
</ul>
</li>
<li>The measure/optimise loop</li>
<li>Techniques to boost performance</li>
<li>Scaling out </li>
<li>Other useful resources </li>
</ul>
<p>
The session starts at 9:00 am UK time / 10:00 am Central European time.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33774&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33774&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Andrew Morgan";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:16;a:6:{s:4:"data";s:93:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:46:"My Talks at MySQL Connect and Percona Live NYC";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:30:"http://www.tokutek.com/?p=4363";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:78:"http://www.tokutek.com/2012/07/my-talks-at-mysql-connect-and-percona-live-nyc/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1288:"
Solving the Challenges of Big Databases with MySQL
When you’re using MySQL for big data (more than ten times as large as main memory), these challenges often arise: loading data fast; maintaining indexes under insertions deletions, and updates; adding and removing columns online; adding indexes online; preventing slave lag; and compressing data effectively.
This session shows why some of these challenges are difficult to solve with storage engines based on B-trees, how Fractal Tree® data structures work, and why they can help solve these problems. Tokutek sells a transaction-safe Fractal Tree storage engine for MySQL, but the presentation is primarily about the underlying technology. It includes a discussion of both the theoretical and practical aspects of Fractal Tree indexes.
I have the privilege of being able to give this talk at both conferences, so please stop by my presentation at whichever conference you plan to attend (or perhaps both, like me!).

MySQL Connect will run From September 29th to 30th. Details on my talk can be found here. A sampling of the MySQL talks compiled by @stoker can be found here.
For Percona Live NYC I will be speaking on October 2nd at 1:30 pm in Flatiron. Details can be found here. A sampling of the speakers can be found here.
";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 15:18:26 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:11:{i:0;a:5:{s:4:"data";s:8:"TokuView";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:8:"big data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"InnoDB";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:13:"MySQL Connect";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:6:"NewSQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:6:"Oracle";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:12:"Percona Live";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:8;a:5:{s:4:"data";s:14:"storage engine";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:9;a:5:{s:4:"data";s:6:"TokuDB";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:10;a:5:{s:4:"data";s:7:"Tokutek";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2113:"<p><strong><br />
Solving the Challenges of Big Databases with MySQL</strong></p>
<p>When you’re using MySQL for big data (more than ten times as large as main memory), these challenges often arise: loading data fast; maintaining indexes under insertions deletions, and updates; adding and removing columns online; adding indexes online; preventing slave lag; and compressing data effectively.</p>
<p>This session shows why some of these challenges are difficult to solve with storage engines based on B-trees, how Fractal Tree<sup>®</sup> data structures work, and why they can help solve these problems. Tokutek sells a transaction-safe Fractal Tree storage engine for MySQL, but the presentation is primarily about the underlying technology. It includes a discussion of both the theoretical and practical aspects of Fractal Tree indexes.</p>
<p>I have the privilege of being able to give this talk at both conferences, so please stop by my presentation at whichever conference you plan to attend (or perhaps both, like me!).</p>
<ul>
<li>MySQL Connect will run From September 29th to 30th. Details on my talk can be found <a href="https://oracleus.activeevents.com/connect/sessionDetail.ww?SESSION_ID=9310" target="_blank">here</a>. A sampling of the MySQL talks compiled by <a href="https://twitter.com/#!/stoker" target="_blank">@stoker</a> can be found <a href="http://opensourcedba.wordpress.com/2012/07/02/mysql-connect-sessions/" target="_blank">here</a>.</li>
<li>For Percona Live NYC I will be speaking on October 2nd at 1:30 pm in Flatiron. Details can be found <a href="http://www.percona.com/live/nyc-2012/sessions/solving-challenges-big-databases-mysql" target="_blank">here</a>. A sampling of the speakers can be found <a href="http://www.percona.com/about-us/pressreleases/speakers-unveiled-for-percona-live-new-york-2012-mysql-conference/" target="_blank">here</a>.</li>
</ul><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33773&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33773&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Tokuview Blog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:17;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:46:"Top Reasons to Take the MySQL Cluster Training";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:60:"https://blogs.oracle.com/MySQL/entry/top_reasons_to_take_the";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:60:"https://blogs.oracle.com/MySQL/entry/top_reasons_to_take_the";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:3468:"Here are the top reasons to take the authorized MySQL Cluster training course:
  
    Take training which was developed by MySQL Cluster product team and delivered by the MySQL Cluster experts at Oracle 
    Learn how to develop, deploy, manage and scale your MySQL Cluster applications more efficiently 
    Keep your mission-critical applications and essential services up and running 24x7 
    Deliver the highest performance and scalability using MySQL Cluster best practices 
  
  In this 3 day course, experienced database users learn the important details of clustering necessary to get started with MySQL Cluster, to properly configure and manage the cluster nodes to ensure high availability, to install the different nodes and provide a better understanding of the internals of the cluster.
  To see the schedule for this course,&nbsp;go to&nbsp;the Oracle University Portal (click on MySQL). Should you not see an event for a location/date that suits you, register your interest in additional events.
  Here is a small sample of the events already on the schedule for the MySQL Cluster course:
  
    
      
        
          
            &nbsp;Location
          
          
            &nbsp;Date
          
          
            &nbsp;Delivery Language
          
        
        
          
            &nbsp;Prague, Czech Republic
          
          
            &nbsp;17 September 2012
          
          
            &nbsp;Czech
          
        
        
          
            &nbsp;Warsaw, Poland
          
          
            &nbsp;1 August 2012
          
          
            &nbsp;Polish
          
        
        
          
            &nbsp;London, United Kingdom
          
          
            &nbsp;18 July 2012
          
          
            &nbsp;English
          
        
        
          
            &nbsp;Lisbon, Portugal
          
          
            &nbsp;3 December 2012
          
          
            &nbsp;European Portugese
          
        
        
          
            &nbsp;Nice, France
          
          
            &nbsp;8 October 2012
          
          
            &nbsp;French
          
        
        
          
            &nbsp;Barcelona, Spain
          
          
            &nbsp;25 September 2012
          
          
            &nbsp;Spanish
          
        
        
          
            &nbsp;Madrid, Spain
          
          
            &nbsp;20 August 2012
          
          
            &nbsp;Spanish
          
        
        
          
            &nbsp;Denver, United States
          
          
            &nbsp;17 October 2012
          
          
            &nbsp;English
          
        
        
          
            &nbsp;Chicago, United States
          
          
            &nbsp;22 August 2012
          
          
            &nbsp;English
          
        
        
          
            &nbsp;Petaling Jaya, Malaysia
          
          
            &nbsp;10 October 2012
          
          
            &nbsp;English
          
        
        
          
            &nbsp;Singapore
          
          
            &nbsp;21 August 2012
          
          
            &nbsp;English
          
        
        
          
            &nbsp;Mexico City, Mexico
          
          
            &nbsp;23 July 2012
          
          
            &nbsp;Spanish
          
        
      
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Mon, 09 Jul 2012 12:59:01 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:14:"MySQL Training";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:9:"training;";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:5206:"<p>Here are the top reasons to take the authorized MySQL Cluster training course:</p>
  <ul>
    <li>Take training which was developed by MySQL Cluster product team and delivered by the MySQL Cluster experts at Oracle </li>
    <li>Learn how to develop, deploy, manage and scale your MySQL Cluster applications more efficiently </li>
    <li>Keep your mission-critical applications and essential services up and running 24x7 </li>
    <li>Deliver the highest performance and scalability using MySQL Cluster best practices </li>
  </ul>
  <p>In this 3 day course, experienced database users learn the important details of clustering necessary to get started with MySQL Cluster, to properly configure and manage the cluster nodes to ensure high availability, to install the different nodes and provide a better understanding of the internals of the cluster.</p>
  <p>To see the schedule for this course,&nbsp;go to&nbsp;the <a href="http://www.oracle.com/education">Oracle University Portal</a> (click on MySQL). Should you not see an event for a location/date that suits you, register your interest in additional events.</p>
  <p>Here is a small sample of the events already on the schedule for the MySQL Cluster course:</p>
  <p>
    <table border="1" cellspacing="1" cellpadding="1">
      <tbody>
        <tr>
          <td>
            <p align="center">&nbsp;Location</p>
          </td>
          <td>
            <p align="center">&nbsp;Date</p>
          </td>
          <td>
            <p align="center">&nbsp;Delivery Language</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Prague, Czech Republic</p>
          </td>
          <td>
            <p align="center">&nbsp;17 September 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;Czech</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Warsaw, Poland</p>
          </td>
          <td>
            <p align="center">&nbsp;1 August 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;Polish</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;London, United Kingdom</p>
          </td>
          <td>
            <p align="center">&nbsp;18 July 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;English</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Lisbon, Portugal</p>
          </td>
          <td>
            <p align="center">&nbsp;3 December 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;European Portugese</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Nice, France</p>
          </td>
          <td>
            <p align="center">&nbsp;8 October 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;French</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Barcelona, Spain</p>
          </td>
          <td>
            <p align="center">&nbsp;25 September 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;Spanish</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Madrid, Spain</p>
          </td>
          <td>
            <p align="center">&nbsp;20 August 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;Spanish</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Denver, United States</p>
          </td>
          <td>
            <p align="center">&nbsp;17 October 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;English</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Chicago, United States</p>
          </td>
          <td>
            <p align="center">&nbsp;22 August 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;English</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Petaling Jaya, Malaysia</p>
          </td>
          <td>
            <p align="center">&nbsp;10 October 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;English</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Singapore</p>
          </td>
          <td>
            <p align="center">&nbsp;21 August 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;English</p>
          </td>
        </tr>
        <tr>
          <td>
            <p align="center">&nbsp;Mexico City, Mexico</p>
          </td>
          <td>
            <p align="center">&nbsp;23 July 2012</p>
          </td>
          <td>
            <p align="center">&nbsp;Spanish</p>
          </td>
        </tr>
      </tbody>
    </table>
  </p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33771&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33771&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"MySQL Community";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:18;a:6:{s:4:"data";s:63:"
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:48:"Data fragmentation problem in MySQL &amp; MyISAM";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:32:"http://www.dbasquare.com/?p=2463";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:79:"http://www.dbasquare.com/2012/07/09/data-fragmentation-problem-in-mysql-myisam/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:10080:"The other day I worked on a case of what turned out to be a problem with poor data locality or a data fragmentation problem if you will. I tought that it would make a good article as it was a great demonstration of how badly it can affect MySQL performance. And while the post is mostly around MyISAM tables, the problem is not really specific to any particular storage engine, it can affect a database that runs on InnoDB in a very similar way.

The problem
MyISAM lacks support for clustering keys or even anything remotely similar. Its data file format allows new information to be written anywhere inside a table. Anywhere can be either at the end of a file where it can be simply appended or an empty space somewhere in the middle left after previously deleted row(s). This implies no particular order in which rows are stored unless there are absolutely no deletes. 
Unlike MyISAM, InnoDB not only supports clustering keys, but even implicitly assumes one for every table. This way primary keys also become clustering keys. Such index determines the physical order of rows in a table. This can be leveraged to benefit frequent queries which read many rows having something in common. The examples of such queries could be asking for all data that belongs to a certain user or all messages saved in a single folder. 
The benefits I mentioned appear when data size is big enough to shift the workload type from CPU-bound to disk-bound, or in other words when these queries often do not find the information they need in memory and need to read it from disk much of the time. Good data locality means that all the relevant information can be accessed with very few sequential reads rather than with many small I/Os to random places on disk. It can make a dramatic difference to database performance. A detailed example was given in this article.
The problem of data locality or fragmentation applies to MyISAM tables the same way it does to InnoDB tables. And while you can easily design a good InnoDB table that largely avoids this problem by making a good use of clustering indexes or use things like extended slow log for analysis to determine whether or not some tables are somehow affected by the problem, that does not work with MyISAM. But there is hope. It is possible to verify and manage data fragmentation with MyISAM.
Examining data
What happens when you do a full table scan on an InnoDB table? You get the result that is sorted by the primary key. There is no way to do any kind of sequential read to return rows in the actual order in which they appear inside a data file. 
MyISAM is much simpler in that matter. Its data files do not have any elaborate structure that would require some special way of accessing rows. If you do a full table scan on a MyISAM table, MySQL just goes through the file reading them sequentially which is almost no different than reading a flat file:

mysql> INSERT INTO a (id) VALUES (2), (1);

mysql> SELECT * FROM a;
+----+------+
| id | foo  |
+----+------+
|  2 | NULL |
|  1 | NULL |
+----+------+

mysql> DELETE FROM a WHERE id = 2;

mysql> INSERT INTO a (id) VALUES (3), (4);

mysql> SELECT * FROM a;
+----+------+
| id | foo  |
+----+------+
|  3 | NULL |
|  1 | NULL |
|  4 | NULL |
+----+------+

This behaviour can actually be used to examine the physical organization of rows on disk. All that is needed for the analysis of data fragmentation is a complete dump of values from the column(s) that identify the sets of rows we want in as small number of fragments as possible. If for example we had a table that listed files users own, we&#8217;d be interested in examining the column that carries user identifiers, because we&#8217;d like all files that belong to a single user to be in one place on disk. 
It is of course important that the dump is taken through a full table scan, so any indexes that contain the relevant column(s) should be explicitly disabled during the operation:

mysql> SELECT user_id FROM files IGNORE INDEX (ix_on__user_id_and_uploaded,
           uk_on__user_id_and_file_id) INTO OUTFILE '/tmp/files-user_id.txt';

You can also make sure the query does not use any index by running EXPLAIN on it first. If it says type: ALL, you can use it to dump data. 
The result is a relatively small file containing the list of user_id values exactly in the order as they appear in rows on disk:

# head -6 /tmp/files-user_id.txt
12404
12404
12404
12398
12404
12401

What this tells us is that three files that belong to user 12404 are first, followed by a file that belongs to user 12398, then one that belongs to 12404 again, and so on. Using this information we can easily calculate how fragmented the data is:

% sort files-user_id.txt | uniq | wc -l
1480
% uniq files-user_id.txt | wc -l
40269366

In the example files that belong to 1480 different users are stored in 40269366 contiguous blocks (that&#8217;s over 40M!). By a contiguous block I mean one or more files that belong to the same user stored next to each other (for example 12404-12404-12404, 12398, 12404 makes three such blocks). And this result seems bad result for data locality as ideally the two numbers should be reasonably close.
Of course that is only a very simple and by no means realistic calculation, because it does not account for row lengths or disk block sizes, but it&#8217;s a number that gives some idea about how data looks like inside of a .MYD file. With a little bit more elaborate script that actually does something about the two factors I just mentioned, the result could be slightly different:

% ./frag.py --file=files-user_id.txt --row-length=168
1480 sets are stored in 17634553 fragments.

It is probably a little bit more accurate information, but this is less about accuracy and more about getting an idea whether there could be a problem with poor data locality or not. Going further we can start dumping detailed information about each user:

% ./frag.py --file=files-user_id.txt --row-length=168 --verbose
UserID    Count     Fragments
..
176175    60945     367
81045     1         1
80083     1170866   515153
68042     12        1
..
1480 sets are stored in 17634553 fragments.

We can see that user 176175 has 60945 rows in only 367 fragments, while 80083 has rows scattered across over half million (!) different places. Accessing each fragment likely requires one I/O, so the latter might need as many as 515153!
Addressing the problem
InnoDB with its clustering index does all the work and internally maintains the proper sort order of rows. It does not actually prevent data fragmentation from happening entirely, but it can help keeping similar rows in relatively small number fragments and in larger groups. Since MyISAM doesn&#8217;t do that, managing data locality will always have to be part of database maintenance work. 
One of the tools shipped with MySQL is myisamcheck. It is primarily designed to allow fixing broken MyISAM tables. However it comes with one option that can be used to improve data locality:

% myisamchk --help
..
  -R, --sort-records=#
                      Sort records according to an index.  This makes your
                      data much more localized and may speed up things
                      (It may be VERY slow to do a sort the first time!).
..

In my example I&#8217;d have to use it on an index that begins with user_id column as that would place all the rows with the same user_id value next to each other. Unfortunately myisamcheck cannot be used on a file that can be used by MySQL. That&#8217;s why this method can only be used on a standby slave that can be stopped for a while or during a maintenance window.
Right after defragmentation the statistics should start looking much better:

% ./frag.py --file=files-user_id-defragmented.txt --row-length=168 --verbose
1480 sets are stored in 1480 fragments.

1480 in 1480 fragments is just perfect.
The gain
In disk-bound workloads the gain from improving data locality can be enormous. Here is a real life example of one query that was executed two times &#8211; before the data file optimization and after &#8211; in MySQL instance running on top of a 3.6GHz CPU and a 30-disk RAID50 volume. Each time it was run on empty caches to see the worst case scenario as it had to read everything from disk:
Before defragmentation: 1103702 rows in set (52 min 45.29 sec)
After defragmentation: 1103702 rows in set (0.58 sec)
That is several orders of magnitude slower when data was fragmented. How many I/O operations were needed? With approximately 200 IOPS the first query needed 53 minutes, so it gives approximately 636000 I/O operations, almost every single one was 4KB long (a single block read). The estimation was for 515153 I/O operations, which wasn&#8217;t too far away from the real number. The second query needed only a couple hundred long reads.
The same experiment was also repeated on a system with four Fusion-io cards in RAID 10 configuration:
Before defragmentation: 1103702 rows in set (1 min 22.28 sec)
After defragmentation: 1103702 rows in set (4.13 sec)
Using Fusion-io didn&#8217;t really help to mitigate the data fragmentation problem. While the slow query ran 50 times faster compared to a system with a storage based on regular hard drives, almost one and a half minute was still a very poor result. 
Conclusions
Data fragmentation can become a very serious problem affecting MySQL performance once the working set grows beyond a certain size that can no longer be cached in memory. Keeping the fragmentation under control can guarantee that even disk-bound queries can execute quickly, but it also reduces the pressure on the storage. Unfortunately MyISAM has no built-in features that would help with managing data locality or either limit or prevent fragmentation in any way. It has to be taken care of as part of a regular database maintenance work. That is why it may usually be better to consider a migration to InnoDB if data locality can become an important factor.
Extra thought: even the most capable flash-based storage may not really solve some basic problems!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sun, 08 Jul 2012 22:50:27 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:5:{i:0;a:5:{s:4:"data";s:14:"Managing MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:13:"Fragmentation";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:3:"I/O";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:6:"MyISAM";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:11:"Performance";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:11071:"<p>The other day I worked on a case of what turned out to be a problem with poor data locality or a data fragmentation problem if you will. I tought that it would make a good article as it was a great demonstration of how badly it can affect MySQL performance. And while the post is mostly around MyISAM tables, the problem is not really specific to any particular storage engine, it can affect a database that runs on InnoDB in a very similar way.</p>
<p><span></span></p>
<h5>The problem</h5>
<p>MyISAM lacks support for clustering keys or even anything remotely similar. Its data file format allows new information to be written anywhere inside a table. <em>Anywhere</em> can be either at the end of a file where it can be simply appended or an empty space somewhere in the middle left after previously deleted row(s). This implies no particular order in which rows are stored unless there are absolutely no deletes. </p>
<p>Unlike MyISAM, InnoDB not only supports clustering keys, but even implicitly assumes one for every table. This way primary keys also become clustering keys. Such index determines the physical order of rows in a table. This can be leveraged to benefit frequent queries which read many rows having something in common. The examples of such queries could be asking for all data that belongs to a certain user or all messages saved in a single folder. </p>
<p>The benefits I mentioned appear when data size is big enough to shift the workload type from CPU-bound to disk-bound, or in other words when these queries often do not find the information they need in memory and need to read it from disk much of the time. Good data locality means that all the relevant information can be accessed with very few sequential reads rather than with many small I/Os to random places on disk. It can make a dramatic difference to database performance. A detailed example was given in <a href="http://www.dbasquare.com/2012/04/04/how-important-a-primary-key-can-be-for-mysql-performance/" title="How important a primary key can be for MySQL performance?" target="_blank">this article</a>.</p>
<p>The problem of data locality or fragmentation applies to MyISAM tables the same way it does to InnoDB tables. And while you can easily design a good InnoDB table that largely avoids this problem by making a good use of clustering indexes or use things like extended slow log for analysis to determine whether or not some tables are somehow affected by the problem, that does not work with MyISAM. But there is hope. It is possible to verify and manage data fragmentation with MyISAM.</p>
<h6>Examining data</h6>
<p>What happens when you do a full table scan on an InnoDB table? You get the result that is sorted by the primary key. There is no way to do any kind of sequential read to return rows in the actual order in which they appear inside a data file. </p>
<p>MyISAM is much simpler in that matter. Its data files do not have any elaborate structure that would require some special way of accessing rows. If you do a full table scan on a MyISAM table, MySQL just goes through the file reading them sequentially which is almost no different than reading a flat file:</p>
<pre>
mysql> INSERT INTO a (id) VALUES (2), (1);

mysql> SELECT * FROM a;
+----+------+
| id | foo  |
+----+------+
|  2 | NULL |
|  1 | NULL |
+----+------+

mysql> DELETE FROM a WHERE id = 2;

mysql> INSERT INTO a (id) VALUES (3), (4);

mysql> SELECT * FROM a;
+----+------+
| id | foo  |
+----+------+
|  3 | NULL |
|  1 | NULL |
|  4 | NULL |
+----+------+
</pre>
<p>This behaviour can actually be used to examine the physical organization of rows on disk. All that is needed for the analysis of data fragmentation is a complete dump of values from the column(s) that identify the sets of rows we want in as small number of fragments as possible. If for example we had a table that listed files users own, we&#8217;d be interested in examining the column that carries user identifiers, because we&#8217;d like all files that belong to a single user to be in one place on disk. </p>
<p>It is of course important that the dump is taken through a full table scan, so any indexes that contain the relevant column(s) should be explicitly disabled during the operation:</p>
<pre>
mysql> SELECT user_id FROM files IGNORE INDEX (ix_on__user_id_and_uploaded,
           uk_on__user_id_and_file_id) INTO OUTFILE '/tmp/files-user_id.txt';
</pre>
<p>You can also make sure the query does not use any index by running <em>EXPLAIN</em> on it first. If it says <em>type: ALL</em>, you can use it to dump data. </p>
<p>The result is a relatively small file containing the list of <em>user_id</em> values exactly in the order as they appear in rows on disk:</p>
<pre>
# head -6 /tmp/files-user_id.txt
12404
12404
12404
12398
12404
12401
</pre>
<p>What this tells us is that three files that belong to user <em>12404</em> are first, followed by a file that belongs to user <em>12398</em>, then one that belongs to <em>12404</em> again, and so on. Using this information we can easily calculate how fragmented the data is:</p>
<pre>
% sort files-user_id.txt | uniq | wc -l
1480
% uniq files-user_id.txt | wc -l
40269366
</pre>
<p>In the example files that belong to 1480 different users are stored in 40269366 contiguous blocks (that&#8217;s over 40M!). By a <em>contiguous block</em> I mean one or more files that belong to the same user stored next to each other (for example <em>12404-12404-12404</em>, <em>12398</em>, <em>12404</em> makes three such blocks). And this result seems bad result for data locality as ideally the two numbers should be reasonably close.</p>
<p>Of course that is only a very simple and by no means realistic calculation, because it does not account for row lengths or disk block sizes, but it&#8217;s a number that gives some idea about how data looks like inside of a <em>.MYD</em> file. With a little bit more elaborate script that actually does something about the two factors I just mentioned, the result could be slightly different:</p>
<pre>
% ./frag.py --file=files-user_id.txt --row-length=168
1480 sets are stored in 17634553 fragments.
</pre>
<p>It is probably a little bit more accurate information, but this is less about accuracy and more about getting an idea whether there could be a problem with poor data locality or not. Going further we can start dumping detailed information about each user:</p>
<pre>
% ./frag.py --file=files-user_id.txt --row-length=168 --verbose
UserID    Count     Fragments
..
176175    60945     367
81045     1         1
80083     1170866   515153
68042     12        1
..
1480 sets are stored in 17634553 fragments.
</pre>
<p>We can see that user <em>176175</em> has 60945 rows in only 367 fragments, while <em>80083</em> has rows scattered across over half million (!) different places. Accessing each fragment likely requires one I/O, so the latter might need as many as 515153!</p>
<h5>Addressing the problem</h5>
<p>InnoDB with its clustering index does all the work and internally maintains the proper sort order of rows. It does not actually prevent data fragmentation from happening entirely, but it can help keeping similar rows in relatively small number fragments and in larger groups. Since MyISAM doesn&#8217;t do that, managing data locality will always have to be part of database maintenance work. </p>
<p>One of the tools shipped with MySQL is <em>myisamcheck</em>. It is primarily designed to allow fixing broken MyISAM tables. However it comes with one option that can be used to improve data locality:</p>
<pre>
% myisamchk --help
..
  -R, --sort-records=#
                      Sort records according to an index.  This makes your
                      data much more localized and may speed up things
                      (It may be VERY slow to do a sort the first time!).
..
</pre>
<p>In my example I&#8217;d have to use it on an index that begins with <em>user_id</em> column as that would place all the rows with the same <em>user_id</em> value next to each other. Unfortunately <em>myisamcheck</em> cannot be used on a file that can be used by MySQL. That&#8217;s why this method can only be used on a standby slave that can be stopped for a while or during a maintenance window.</p>
<p>Right after defragmentation the statistics should start looking much better:</p>
<pre>
% ./frag.py --file=files-user_id-defragmented.txt --row-length=168 --verbose
1480 sets are stored in 1480 fragments.
</pre>
<p>1480 in 1480 fragments is just perfect.</p>
<h5>The gain</h5>
<p>In disk-bound workloads the gain from improving data locality can be enormous. Here is a real life example of one query that was executed two times &#8211; before the data file optimization and after &#8211; in MySQL instance running on top of a 3.6GHz CPU and a 30-disk RAID50 volume. Each time it was run on empty caches to see the worst case scenario as it had to read everything from disk:</p>
<p><em>Before defragmentation:</em> 1103702 rows in set (52 min 45.29 sec)<br />
<em>After defragmentation:</em> 1103702 rows in set (0.58 sec)</p>
<p>That is several orders of magnitude slower when data was fragmented. How many I/O operations were needed? With approximately 200 IOPS the first query needed 53 minutes, so it gives approximately 636000 I/O operations, almost every single one was 4KB long (a single block read). The estimation was for 515153 I/O operations, which wasn&#8217;t too far away from the real number. The second query needed only a couple hundred long reads.</p>
<p>The same experiment was also repeated on a system with four Fusion-io cards in RAID 10 configuration:</p>
<p><em>Before defragmentation:</em> 1103702 rows in set (1 min 22.28 sec)<br />
<em>After defragmentation:</em> 1103702 rows in set (4.13 sec)</em></p>
<p>Using Fusion-io didn&#8217;t really help to mitigate the data fragmentation problem. While the slow query ran 50 times faster compared to a system with a storage based on regular hard drives, almost one and a half minute was still a very poor result. </p>
<h5>Conclusions</h5>
<p>Data fragmentation can become a very serious problem affecting MySQL performance once the working set grows beyond a certain size that can no longer be cached in memory. Keeping the fragmentation under control can guarantee that even disk-bound queries can execute quickly, but it also reduces the pressure on the storage. Unfortunately MyISAM has no built-in features that would help with managing data locality or either limit or prevent fragmentation in any way. It has to be taken care of as part of a regular database maintenance work. That is why it may usually be better to consider a migration to InnoDB if data locality can become an important factor.</p>
<p>Extra thought: <strong>even the most capable flash-based storage may not really solve some basic problems!</strong></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33770&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33770&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"dba square";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:19;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:72:"Heads up! No more query cache for partitioned tables as of MySQL 5.5.23.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:29:"1045 at http://www.skysql.com";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:89:"http://www.skysql.com/blogs/kolbe/heads-no-more-query-cache-partitioned-tables-mysql-5523";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1543:" A customer opened an issue recently to ask why the query cache wasn't working after he upgraded to MySQL 5.5.25. The reason really ended up surprising me.
As of MySQL 5.5.23, the Query Cache is disabled for partitioned tables!
This is a "fix" for bug #53775.
At first I thought perhaps the fix for the bug had resulted in the query cache being inadvertently disabled for partitioned tables, but the comments that go along with the commit make it pretty clear that disabling the query cache was the intended "fix". You can review the commit message and the code changed at revision 2661.803.1 in the MySQL Server 5.5 repository.
I agree with Mikael Ronstrom, who wrote on bug #53775, "Disabling the query cache for partitioned tables is a bit too harsh of a solution."
However, the change in behavior is not nearly as harsh as the complete failure to update the documentation to reflect this pretty serious change in functionality. Not even the entry in the MySQL 5.5.23 release notes makes any mention of the effect of the change, only a small statement about the initial bug report. There's no change to How the Query Cache Operates, which explains conditions under which queries are not cached, nor to Restrictions and Limitations on Partitioning, which explains some perhaps unexpected side-effects of using Partitioning (I think the complete unavailability of query caching would qualify).
So if you're seeing lower query cache utilization and you use partitioned tables, this change could very well be the explanation you're looking for.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sat, 07 Jul 2012 23:51:03 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:12:"partitioning";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:11:"query cache";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2192:"<p> A customer opened an issue recently to ask why the query cache wasn't working after he upgraded to MySQL 5.5.25. The reason really ended up surprising me.</p>
<p><b>As of MySQL 5.5.23, the Query Cache is <i>disabled</i> for partitioned tables!</b></p>
<p>This is a "fix" for <a href="http://bugs.mysql.com/bug.php?id=53775">bug #53775</a>.</p>
<p>At first I thought perhaps the fix for the bug had resulted in the query cache being inadvertently disabled for partitioned tables, but the comments that go along with the commit make it pretty clear that disabling the query cache was the intended "fix". You can review the commit message and the code changed at <a href="http://bazaar.launchpad.net/~mysql/mysql-server/5.5/revision/2661.803.1">revision 2661.803.1</a> in the MySQL Server 5.5 repository.</p>
<p>I agree with Mikael Ronstrom, who wrote on bug #53775, "Disabling the query cache for partitioned tables is a bit too harsh of a solution."</p>
<p>However, the change in behavior is not nearly as harsh as the complete failure to update the documentation to reflect this pretty serious change in functionality. Not even the entry in the <a href="http://dev.mysql.com/doc/refman/5.5/en/news-5-5-23.html">MySQL 5.5.23 release notes</a> makes any mention of the effect of the change, only a small statement about the initial bug report. There's no change to <a href="https://dev.mysql.com/doc/refman/5.5/en/query-cache-operation.html">How the Query Cache Operates</a>, which explains conditions under which queries are not cached, nor to <a href="http://dev.mysql.com/doc/refman/5.5/en/partitioning-limitations.html">Restrictions and Limitations on Partitioning</a>, which explains some perhaps unexpected side-effects of using Partitioning (I think the complete unavailability of query caching would qualify).</p>
<p>So if you're seeing lower query cache utilization and you use partitioned tables, this change could very well be the explanation you're looking for.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33769&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33769&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:6:"SkySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:20;a:6:{s:4:"data";s:58:"
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:73:"Mysql: creating a link to your glossary while fetching text for a webpage";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:36:"http://moinne.com/blog/ronald/?p=244";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:108:"http://moinne.com/blog/ronald/mysql/mysql-creating-a-link-to-your-glossary-while-fetching-text-for-a-webpage";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:136:"This MySql function will generate a link to your glossary when you are fetching text for a web page.
All code is provided in a zip file.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sat, 07 Jul 2012 16:00:58 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:4:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:13:"download code";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:9:"functions";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:8:"glossary";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:350:"This MySql function will generate a link to your glossary when you are fetching text for a web page.
All code is provided in a zip file.<br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33768&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33768&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"Ronald Speelman";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:21;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:52:"From Months to Seconds with Subquery Materialization";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:70:"tag:blogger.com,1999:blog-1508669603650457962.post-5687086978185523569";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:78:"http://oysteing.blogspot.com/2012/07/from-months-to-seconds-with-subquery.html";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:5705:"In an earlier blog post, I showed how optimizer improvements in MySQL 5.6 gave better performance for several of the queries in the DBT-3 benchmark. However, for one of the queries, Query 18, I was not able to give exact numbers for the improvement since the query took very long in MySQL 5.5.  I decided to try to find out exactly how long the query would take, but when the query had run for one month, I gave up.  How can a query take so long?  Especially, when I had set up InnoDB with a buffer pool that should be large enough to hold the entire database. Let's have a look at the query: select c_name, c_custkey, o_orderkey, o_orderdate, o_totalprice, sum(l_quantity)from customer, orders, lineitemwhere o_orderkey in (                select l_orderkey                from lineitem                group by l_orderkey                having sum(l_quantity) > 313  )  and c_custkey = o_custkey  and o_orderkey = l_orderkeygroup by c_name, c_custkey, o_orderkey, o_orderdate, o_totalpriceorder by o_totalprice desc, o_orderdateLIMIT 100; This query will find the orders from customers that have placed big orders.  The reason that this takes so long in MySQL 5.5, is that the sub-query in the WHERE clause will be executed for each row produced by the three table join of the (outer) query. This can be seen from the EXPLAIN output for this query: +----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+| id | select_type        | table    | type  | possible_keys                              | key                   | key_len | ref                     | rows    | Extra                           |+----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+|  1 | PRIMARY            | customer | ALL   | PRIMARY                                    | NULL                  | NULL    | NULL                    |  150000 | Using temporary; Using filesort | |  1 | PRIMARY            | orders   | ref   | PRIMARY,i_o_custkey                        | i_o_custkey           | 5       | dbt3.customer.c_custkey |       7 | Using where                     | |  1 | PRIMARY            | lineitem | ref   | PRIMARY,i_l_orderkey,i_l_orderkey_quantity | i_l_orderkey_quantity | 4       | dbt3.orders.o_orderkey  |       2 | Using index                     | |  2 | DEPENDENT SUBQUERY | lineitem | index | NULL                                       | PRIMARY               | 8       | NULL                    | 6001215 | NULL                            | +----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+The select_type for the subquery is DEPENDENT SUBQUERY.  This implies that the index scan of the lineitem table will be performed once for every row produced by the three preceeding tables.  In other words, it will be executed more than 2 million times (150000*7*2).  I have measured that one execution of the sub-query takes about 3.5 seconds when all the data is in memory.  Hence, it will take more than 80 days to execute it 2 million times. In MySQL 5.6, Subquery Materialization may be used to avoid the repeated execution of subqueries.  This implies that the subquery is executed once and the result stored (materialized) in a temporary table.  Then, for each row produced by the outer query, a hash-based look-up will be made into the temporary table to check whether there is a match.  The EXPLAIN output for Query 18, looks like this in MySQL 5.6: +----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+| id | select_type | table    | type  | possible_keys                              | key                   | key_len | ref                     | rows    | Extra                           |+----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+|  1 | PRIMARY     | customer | ALL   | PRIMARY                                    | NULL                  | NULL    | NULL                    |  150000 | Using temporary; Using filesort | |  1 | PRIMARY     | orders   | ref   | PRIMARY,i_o_custkey                        | i_o_custkey           | 5       | dbt3.customer.c_custkey |       7 | Using where                     | |  1 | PRIMARY     | lineitem | ref   | PRIMARY,i_l_orderkey,i_l_orderkey_quantity | i_l_orderkey_quantity | 4       | dbt3.orders.o_orderkey  |       2 | Using index                     | |  2 | SUBQUERY    | lineitem | index | NULL                                       | PRIMARY               | 8       | NULL                    | 6001215 | NULL                            | +----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+The only difference is that select_type of the subquery now is SUBQUERY.  In other words, it is no longer dependent on the preceding tables, and can be executed only once.  In my run of DBT-3 with MySQL 5.6, Query 18 takes only 6.8 seconds.  Hence, we have gone from a couple of months to just a few seconds! My colleague Guilhem has earlier written about another DBT-3 query that is improved with Subquery Materialization. ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sat, 07 Jul 2012 12:11:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:6658:"<p>In an earlier <a href="http://oysteing.blogspot.com/2012/04/improved-dbt-3-results-with-mysql-565.html">blog post</a>, I showed how optimizer improvements in MySQL 5.6 gave better performance for several of the queries in the DBT-3 benchmark. </p><p>However, for one of the queries, Query 18, I was not able to give exact numbers for the improvement since the query took very long in MySQL 5.5.  I decided to try to find out exactly how long the query would take, but when the query had run for one month, I gave up.  How can a query take so long?  Especially, when I had set up InnoDB with a buffer pool that should be large enough to hold the entire database. Let's have a look at the query: <br/><div><pre><span>select c_name, c_custkey, o_orderkey, o_orderdate, o_totalprice, sum(l_quantity)<br />from customer, orders, lineitem<br />where o_orderkey in (<br />                select l_orderkey<br />                from lineitem<br />                group by l_orderkey<br />                having sum(l_quantity) > 313<br />  )<br />  and c_custkey = o_custkey<br />  and o_orderkey = l_orderkey<br />group by c_name, c_custkey, o_orderkey, o_orderdate, o_totalprice<br />order by o_totalprice desc, o_orderdate<br />LIMIT 100;<br /></span></pre></div><br/> This query will find the orders from customers that have placed big orders.  The reason that this takes so long in MySQL 5.5, is that the sub-query in the WHERE clause will be executed for each row produced by the three table join of the (outer) query. This can be seen from the EXPLAIN output for this query: <div><pre><span>+----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br />| id | select_type        | table    | type  | possible_keys                              | key                   | key_len | ref                     | rows    | Extra                           |<br />+----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br />|  1 | PRIMARY            | customer | ALL   | PRIMARY                                    | NULL                  | NULL    | NULL                    |  150000 | Using temporary; Using filesort | <br />|  1 | PRIMARY            | orders   | ref   | PRIMARY,i_o_custkey                        | i_o_custkey           | 5       | dbt3.customer.c_custkey |       7 | Using where                     | <br />|  1 | PRIMARY            | lineitem | ref   | PRIMARY,i_l_orderkey,i_l_orderkey_quantity | i_l_orderkey_quantity | 4       | dbt3.orders.o_orderkey  |       2 | Using index                     | <br />|  2 | DEPENDENT SUBQUERY | lineitem | index | NULL                                       | PRIMARY               | 8       | NULL                    | 6001215 | NULL                            | <br />+----+--------------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br /></span></pre></div><br/><p>The <tt>select_type</tt> for the subquery is <tt>DEPENDENT SUBQUERY</tt>.  This implies that the index scan of the <tt>lineitem</tt> table will be performed once for every row produced by the three preceeding tables.  In other words, it will be executed more than 2 million times (<tt>150000*7*2</tt>).  I have measured that one execution of the sub-query takes about 3.5 seconds when all the data is in memory.  Hence, it will take more than 80 days to execute it 2 million times. </p><p>In MySQL 5.6, Subquery Materialization may be used to avoid the repeated execution of subqueries.  This implies that the subquery is executed once and the result stored (materialized) in a temporary table.  Then, for each row produced by the outer query, a hash-based look-up will be made into the temporary table to check whether there is a match.  The EXPLAIN output for Query 18, looks like this in MySQL 5.6: </p><div><pre><span>+----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br />| id | select_type | table    | type  | possible_keys                              | key                   | key_len | ref                     | rows    | Extra                           |<br />+----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br />|  1 | PRIMARY     | customer | ALL   | PRIMARY                                    | NULL                  | NULL    | NULL                    |  150000 | Using temporary; Using filesort | <br />|  1 | PRIMARY     | orders   | ref   | PRIMARY,i_o_custkey                        | i_o_custkey           | 5       | dbt3.customer.c_custkey |       7 | Using where                     | <br />|  1 | PRIMARY     | lineitem | ref   | PRIMARY,i_l_orderkey,i_l_orderkey_quantity | i_l_orderkey_quantity | 4       | dbt3.orders.o_orderkey  |       2 | Using index                     | <br />|  2 | SUBQUERY    | lineitem | index | NULL                                       | PRIMARY               | 8       | NULL                    | 6001215 | NULL                            | <br />+----+-------------+----------+-------+--------------------------------------------+-----------------------+---------+-------------------------+---------+---------------------------------+<br /></span></pre></div><br/>The only difference is that <tt>select_type</tt> of the subquery now is <tt>SUBQUERY</tt>.  In other words, it is no longer dependent on the preceding tables, and can be executed only once.  In my run of DBT-3 with MySQL 5.6, Query 18 takes only 6.8 seconds.  Hence, we have gone from a couple of months to just a few seconds! <p>My colleague Guilhem has earlier written about <a href="http://guilhembichot.blogspot.com/2012/04/faster-subqueries-with-materialization.html">another DBT-3 query that is improved with Subquery Materialization</a>. </p><div><img width="1" height="1" src="https://blogger.googleusercontent.com/tracker/1508669603650457962-5687086978185523569?l=oysteing.blogspot.com" alt="" /></div><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33765&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33765&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:17:"Øystein Grøvlen";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:22;a:6:{s:4:"data";s:63:"
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:27:"MySql Lorum Ipsum generator";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:36:"http://moinne.com/blog/ronald/?p=238";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:63:"http://moinne.com/blog/ronald/mysql/mysql-lorum-ipsum-generator";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:103:"A short MySql function to generate a Lorum Ipsum text.
You can download the code in the zip file below.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sat, 07 Jul 2012 11:23:25 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:5:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:13:"download code";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:9:"functions";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:6:"random";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:9:"test data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:317:"A short MySql function to generate a Lorum Ipsum text.
You can download the code in the zip file below.<br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33764&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33764&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"Ronald Speelman";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:23;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:37:"OurSQL Episode 98: MySQL Puppeteering";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:31:"1170 at http://technocation.org";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:71:"http://technocation.org/content/oursql-episode-98%3A-mysql-puppeteering";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:380:"This week we present using Puppet to manage MySQL. Ear Candy is netcat and At the Movies is a video about the Master High Availability toolkit by Yoshinori Matsunobu.
Feedback
Daniël van Eeden gave us lots of great feedback this week:
Episode 93, which included iotop and ionice in ear candy
CFQ scheduler for Linux
noop scheduler for Linux
deadline scheduler for Linux
read more";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Sat, 07 Jul 2012 11:11:38 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:8:"Podcasts";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:8:"Learning";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:13:"Server Tuning";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:997:"<p>This week we present using Puppet to manage MySQL. Ear Candy is netcat and At the Movies is a video about the Master High Availability toolkit by Yoshinori Matsunobu.</p>
<p><strong>Feedback</strong><br />
<a href="http://databaseblog.myname.nl/">Daniël van Eeden</a> gave us lots of great feedback this week:<br />
<a href="http://bit.ly/oursql093">Episode 93</a>, which included iotop and ionice in ear candy<br />
<a href="http://en.wikipedia.org/wiki/CFQ">CFQ scheduler for Linux</a><br />
<a href="http://en.wikipedia.org/wiki/Noop_scheduler">noop scheduler for Linux</a><br />
<a href="http://en.wikipedia.org/wiki/Deadline_scheduler">deadline scheduler for Linux</a></p>
<p><a href="http://technocation.org/content/oursql-episode-98%3A-mysql-puppeteering">read more</a></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33763&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33763&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Technocation";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:24;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:32:"Why I don't use sysdate in MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:81:"http://flite.github.com/mechanics/blog/2012/07/06/why-i-dont-use-sysdate-in-mysql";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:82:"http://flite.github.com/mechanics/blog/2012/07/06/why-i-dont-use-sysdate-in-mysql/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4056:"When I first started using MySQL, I was happy to see that MySQL had the familiar SYSDATE() function that I'd been using for years in Oracle. I wrote a bunch of code using SYSDATE() in MySQL, and then one day I noticed that my slave database sometimes had higher timestamp values than the master database for the same rows. Upon closer investigation, it turned out that SYSDATE() was the problem. As stated in the MySQL Reference Manual:

In addition, the SET TIMESTAMP statement affects the value returned by NOW() but not by SYSDATE(). This means that timestamp settings in the binary log have no effect on invocations of SYSDATE().

Essentially, this means that if you use SYSDATE() in a DML statement that is replicated, the slave gets the timestamp of when the slave executed the DML, not when the master executed the DML. Given that MySQL timestamps have 1 second granularity these values are often the same (if replication never falls behind), but eventually the slave will fall 1 second or more behind and this will introduce data discrepancies. Once I learned that I went back and updated all of my DML to use NOW() instead of SYSDATE() so my slave would have identical data to the master.




And another thing

Using SYSDATE() in MySQL can also lead to very slow query performance in certain scenarios.

Say I have a post table with ~40 million rows and I want to know how many of them were created in the past 8 hours. I've got an index on the created timestamp, so I think this query will be fast:

```
mysql> select count(*)

-&gt; from post 
-&gt; where created_on &gt;= sysdate() - interval 8 hour;


+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (51.41 sec)
```

Despite the index, that query took 50 seconds to run!

Again, the answer is to use NOW() instead of SYSDATE():

```
mysql> select count(*)

-&gt; from post 
-&gt; where created_on &gt;= now() - interval 8 hour;


+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (0.00 sec)
```

That's more like it!

But why was the first query so slow? The explain shows how the queries are different:
```
mysql> explain select count(*)  from pabeta.app_instance  where created_on >= sysdate() - interval 8 hour;
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
| id | select_type | table | type  | possible_keys | key             | key_len | ref  | rows     | Extra                    |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
|  1 | SIMPLE      | post  | index | NULL          | CREATED_ON_INDX | 4       | NULL | 41766790 | Using where; Using index |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
1 row in set (0.00 sec)

mysql> explain select count(*)  from pabeta.app_instance  where created_on >= now() - interval 8 hour;
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
| id | select_type | table | type  | possible_keys   | key             | key_len | ref  | rows | Extra                    |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
|  1 | SIMPLE      | post  | range | CREATED_ON_INDX | CREATED_ON_INDX | 4       | NULL | 2418 | Using where; Using index |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
1 row in set (0.00 sec)
```

I think I'll take the range scan over 2000 rows instead of an index scan of the entire 40 Million index entries, thank you very much.

And the MySQL Reference Manual explains why they are different:

The nondeterministic nature of SYSDATE() also means that indexes cannot be used for evaluating expressions that refer to it.

Given these issues I always use NOW() instead of SYSDATE() in all of my MySQL queries.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Fri, 06 Jul 2012 22:15:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4740:"<p>When I first started using MySQL, I was happy to see that MySQL had the familiar SYSDATE() function that I'd been using for years in Oracle. I wrote a bunch of code using SYSDATE() in MySQL, and then one day I noticed that my slave database sometimes had higher timestamp values than the master database for the same rows. Upon closer investigation, it turned out that SYSDATE() was the problem. As stated in the <a href="http://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_sysdate">MySQL Reference Manual</a>:</p>

<blockquote><p>In addition, the SET TIMESTAMP statement affects the value returned by NOW() but not by SYSDATE(). This means that timestamp settings in the binary log have no effect on invocations of SYSDATE().</p></blockquote>

<p>Essentially, this means that if you use SYSDATE() in a DML statement that is replicated, the slave gets the timestamp of when <em>the slave</em> executed the DML, not when the master executed the DML. Given that MySQL timestamps have 1 second granularity these values are often the same (if replication never falls behind), but eventually the slave will fall 1 second or more behind and this will introduce data discrepancies. Once I learned that I went back and updated all of my DML to use NOW() instead of SYSDATE() so my slave would have identical data to the master.</p>

<!-- more -->


<h2>And another thing</h2>

<p>Using SYSDATE() in MySQL can also lead to very slow query performance in certain scenarios.</p>

<p>Say I have a post table with ~40 million rows and I want to know how many of them were created in the past 8 hours. I've got an index on the created timestamp, so I think this query will be fast:</p>

<p>```
mysql> select count(*)</p>

<pre><code>-&gt; from post 
-&gt; where created_on &gt;= sysdate() - interval 8 hour;
</code></pre>

<p>+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (51.41 sec)
```</p>

<p>Despite the index, that query took 50 seconds to run!</p>

<p>Again, the answer is to use NOW() instead of SYSDATE():</p>

<p>```
mysql> select count(*)</p>

<pre><code>-&gt; from post 
-&gt; where created_on &gt;= now() - interval 8 hour;
</code></pre>

<p>+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (0.00 sec)
```</p>

<p>That's more like it!</p>

<p>But why was the first query so slow? The explain shows <em>how</em> the queries are different:
```
mysql> explain select count(*)  from pabeta.app_instance  where created_on >= sysdate() - interval 8 hour;
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
| id | select_type | table | type  | possible_keys | key             | key_len | ref  | rows     | Extra                    |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
|  1 | SIMPLE      | post  | index | NULL          | CREATED_ON_INDX | 4       | NULL | 41766790 | Using where; Using index |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
1 row in set (0.00 sec)</p>

<p>mysql> explain select count(*)  from pabeta.app_instance  where created_on >= now() - interval 8 hour;
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
| id | select_type | table | type  | possible_keys   | key             | key_len | ref  | rows | Extra                    |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
|  1 | SIMPLE      | post  | range | CREATED_ON_INDX | CREATED_ON_INDX | 4       | NULL | 2418 | Using where; Using index |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
1 row in set (0.00 sec)
```</p>

<p>I think I'll take the range scan over 2000 rows instead of an index scan of the entire 40 Million index entries, thank you very much.</p>

<p>And the <a href="http://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_sysdate">MySQL Reference Manual</a> explains <em>why</em> they are different:</p>

<blockquote><p>The nondeterministic nature of SYSDATE() also means that indexes cannot be used for evaluating expressions that refer to it.</p></blockquote>

<p>Given these issues I always use NOW() instead of SYSDATE() in all of my MySQL queries.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33776&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33776&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Ike Walker";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:25;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:32:"Why I don't use sysdate in MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:74:"http://mechanics.flite.com/blog/2012/07/06/why-i-dont-use-sysdate-in-mysql";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:75:"http://mechanics.flite.com/blog/2012/07/06/why-i-dont-use-sysdate-in-mysql/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4056:"When I first started using MySQL, I was happy to see that MySQL had the familiar SYSDATE() function that I'd been using for years in Oracle. I wrote a bunch of code using SYSDATE() in MySQL, and then one day I noticed that my slave database sometimes had higher timestamp values than the master database for the same rows. Upon closer investigation, it turned out that SYSDATE() was the problem. As stated in the MySQL Reference Manual:

In addition, the SET TIMESTAMP statement affects the value returned by NOW() but not by SYSDATE(). This means that timestamp settings in the binary log have no effect on invocations of SYSDATE().

Essentially, this means that if you use SYSDATE() in a DML statement that is replicated, the slave gets the timestamp of when the slave executed the DML, not when the master executed the DML. Given that MySQL timestamps have 1 second granularity these values are often the same (if replication never falls behind), but eventually the slave will fall 1 second or more behind and this will introduce data discrepancies. Once I learned that I went back and updated all of my DML to use NOW() instead of SYSDATE() so my slave would have identical data to the master.




And another thing

Using SYSDATE() in MySQL can also lead to very slow query performance in certain scenarios.

Say I have a post table with ~40 million rows and I want to know how many of them were created in the past 8 hours. I've got an index on the created timestamp, so I think this query will be fast:

```
mysql> select count(*)

-&gt; from post 
-&gt; where created_on &gt;= sysdate() - interval 8 hour;


+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (51.41 sec)
```

Despite the index, that query took 50 seconds to run!

Again, the answer is to use NOW() instead of SYSDATE():

```
mysql> select count(*)

-&gt; from post 
-&gt; where created_on &gt;= now() - interval 8 hour;


+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (0.00 sec)
```

That's more like it!

But why was the first query so slow? The explain shows how the queries are different:
```
mysql> explain select count(*)  from pabeta.app_instance  where created_on >= sysdate() - interval 8 hour;
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
| id | select_type | table | type  | possible_keys | key             | key_len | ref  | rows     | Extra                    |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
|  1 | SIMPLE      | post  | index | NULL          | CREATED_ON_INDX | 4       | NULL | 41766790 | Using where; Using index |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
1 row in set (0.00 sec)

mysql> explain select count(*)  from pabeta.app_instance  where created_on >= now() - interval 8 hour;
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
| id | select_type | table | type  | possible_keys   | key             | key_len | ref  | rows | Extra                    |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
|  1 | SIMPLE      | post  | range | CREATED_ON_INDX | CREATED_ON_INDX | 4       | NULL | 2418 | Using where; Using index |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
1 row in set (0.00 sec)
```

I think I'll take the range scan over 2000 rows instead of an index scan of the entire 40 Million index entries, thank you very much.

And the MySQL Reference Manual explains why they are different:

The nondeterministic nature of SYSDATE() also means that indexes cannot be used for evaluating expressions that refer to it.

Given these issues I always use NOW() instead of SYSDATE() in all of my MySQL queries.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Fri, 06 Jul 2012 22:15:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4740:"<p>When I first started using MySQL, I was happy to see that MySQL had the familiar SYSDATE() function that I'd been using for years in Oracle. I wrote a bunch of code using SYSDATE() in MySQL, and then one day I noticed that my slave database sometimes had higher timestamp values than the master database for the same rows. Upon closer investigation, it turned out that SYSDATE() was the problem. As stated in the <a href="http://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_sysdate">MySQL Reference Manual</a>:</p>

<blockquote><p>In addition, the SET TIMESTAMP statement affects the value returned by NOW() but not by SYSDATE(). This means that timestamp settings in the binary log have no effect on invocations of SYSDATE().</p></blockquote>

<p>Essentially, this means that if you use SYSDATE() in a DML statement that is replicated, the slave gets the timestamp of when <em>the slave</em> executed the DML, not when the master executed the DML. Given that MySQL timestamps have 1 second granularity these values are often the same (if replication never falls behind), but eventually the slave will fall 1 second or more behind and this will introduce data discrepancies. Once I learned that I went back and updated all of my DML to use NOW() instead of SYSDATE() so my slave would have identical data to the master.</p>

<!-- more -->


<h2>And another thing</h2>

<p>Using SYSDATE() in MySQL can also lead to very slow query performance in certain scenarios.</p>

<p>Say I have a post table with ~40 million rows and I want to know how many of them were created in the past 8 hours. I've got an index on the created timestamp, so I think this query will be fast:</p>

<p>```
mysql> select count(*)</p>

<pre><code>-&gt; from post 
-&gt; where created_on &gt;= sysdate() - interval 8 hour;
</code></pre>

<p>+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (51.41 sec)
```</p>

<p>Despite the index, that query took 50 seconds to run!</p>

<p>Again, the answer is to use NOW() instead of SYSDATE():</p>

<p>```
mysql> select count(*)</p>

<pre><code>-&gt; from post 
-&gt; where created_on &gt;= now() - interval 8 hour;
</code></pre>

<p>+----------+
| count(*) |
+----------+
|     2419 |
+----------+
1 row in set (0.00 sec)
```</p>

<p>That's more like it!</p>

<p>But why was the first query so slow? The explain shows <em>how</em> the queries are different:
```
mysql> explain select count(*)  from pabeta.app_instance  where created_on >= sysdate() - interval 8 hour;
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
| id | select_type | table | type  | possible_keys | key             | key_len | ref  | rows     | Extra                    |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
|  1 | SIMPLE      | post  | index | NULL          | CREATED_ON_INDX | 4       | NULL | 41766790 | Using where; Using index |
+----+-------------+-------+-------+---------------+-----------------+---------+------+----------+--------------------------+
1 row in set (0.00 sec)</p>

<p>mysql> explain select count(*)  from pabeta.app_instance  where created_on >= now() - interval 8 hour;
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
| id | select_type | table | type  | possible_keys   | key             | key_len | ref  | rows | Extra                    |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
|  1 | SIMPLE      | post  | range | CREATED_ON_INDX | CREATED_ON_INDX | 4       | NULL | 2418 | Using where; Using index |
+----+-------------+-------+-------+-----------------+-----------------+---------+------+------+--------------------------+
1 row in set (0.00 sec)
```</p>

<p>I think I'll take the range scan over 2000 rows instead of an index scan of the entire 40 Million index entries, thank you very much.</p>

<p>And the <a href="http://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_sysdate">MySQL Reference Manual</a> explains <em>why</em> they are different:</p>

<blockquote><p>The nondeterministic nature of SYSDATE() also means that indexes cannot be used for evaluating expressions that refer to it.</p></blockquote>

<p>Given these issues I always use NOW() instead of SYSDATE() in all of my MySQL queries.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33786&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33786&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Ike Walker";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:26;a:6:{s:4:"data";s:73:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:26:"help starting my sql ideas";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:54:"http://mysqljoin.com/joins/help-starting-my-sql-ideas/";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:54:"http://mysqljoin.com/joins/help-starting-my-sql-ideas/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1528:"
The Brief
The Client’s mystery shopping programme consists of visits to each of the client’s ten locations, grouped into five different areas. Each location receives a single visit each month, but on occasion a visit may not take place.
On the completion of each visit, a questionnaire is completed by the mystery shopper that is used to rate the service they received during the visit. The completed questionnaire is assigned a score, which is calculated based on the number of points achieved over the number of points available, normally expressed as a percentage.
Points Achieved x 100 Points Available
= % Score
PHP Development Test
￼￼The Client has requested that an end of year report is generated to summarise the data collected over the previous year. The report should be easy to understand and show as much useful information as possible.
Test Requirements
A PHP application should be created to generate the required report and example data will be provided in a CSV file in a denormalised format. Ideally an RDBMS (such as MySQL) should be used, but it is not essential, and any data manipulation should be done by the application.
The report may be presented in one of the following formats:
 A simple HTML table
 A PDF report (using a library such as TCPDF)
 An Excel report (using a library such as PHPExcel)
 Graphically (using a client side library like highcharts, a server side library like pChart, or an API like Google Charts).
All source code for the application should be provided.

";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Fri, 06 Jul 2012 22:09:18 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:7:{i:0;a:5:{s:4:"data";s:9:"Questions";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:4:"help";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:5:"ideas";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:8:"pointers";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:5:"RDBMS";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:6:"urgent";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1894:"<!-- google_ad_section_start -->
<p>The Brief<br />
The Client’s mystery shopping programme consists of visits to each of the client’s ten locations, grouped into five different areas. Each location receives a single visit each month, but on occasion a visit may not take place.<br />
On the completion of each visit, a questionnaire is completed by the mystery shopper that is used to rate the service they received during the visit. The completed questionnaire is assigned a score, which is calculated based on the number of points achieved over the number of points available, normally expressed as a percentage.<br />
Points Achieved x 100 Points Available<br />
= % Score<br />
PHP Development Test<br />
￼￼The Client has requested that an end of year report is generated to summarise the data collected over the previous year. The report should be easy to understand and show as much useful information as possible.<br />
Test Requirements<br />
A PHP application should be created to generate the required report and example data will be provided in a CSV file in a denormalised format. Ideally an RDBMS (such as MySQL) should be used, but it is not essential, and any data manipulation should be done by the application.<br />
The report may be presented in one of the following formats:<br />
 A simple HTML table<br />
 A PDF report (using a library such as TCPDF)<br />
 An Excel report (using a library such as PHPExcel)<br />
 Graphically (using a client side library like highcharts, a server side library like pChart, or an API like Google Charts).<br />
All source code for the application should be provided.</p>

<!-- google_ad_section_end --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33762&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33762&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Jan Brinkmann";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:27;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:45:"What were the conditions and what is the fix?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.webyog.com/blog/?p=3865";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:83:"http://www.webyog.com/blog/2012/07/06/what-were-the-conditions-and-what-is-the-fix/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2024:"MySQL 5.5.25a has been released with the fix for the bug http://bugs.mysql.com/bug.php?id=65745. This bug &#8211; introduced in 5.5.25 &#8211; could ultimately have the result that a single UPDATE statement made all disk space on the system unavailable. The InnoDB tablespace(s) grow(s)  even though data and indexes do not require more disk space.
This is of course an ugly bug &#8211; but &#8216;s&#8230; happens&#8216; they say. So it is not the purpose of this to ridicule or expose anyone. After all it was fixed  as soon as possible after the bug was reported. But after the fix has been released it is still unclear what exact conditions will trigger the effect.
Kolbe Kegel from SkySQL who reported the bug report, provided this CREATE TABLE statement (the simplest he could find to reproduce the issue)
CREATE TABLE t1 (
 id1 int NOT NULL,
 id2 int NOT NULL,
 a int,
 b int,
 PRIMARY KEY (id1,id2),
 KEY (id1, a)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
.. and further added that &#8220;removing any further row, column or index makes the problem disappear&#8220;. But WHY? Why will it help to drop the index `id1` or the column `b` for instance?
Also what is the fix for this for users affected? If not innodb_file_per_table is used I cannot find any other solution than to dump everything and restore to a fresh server or InnodDB instance (if innodb_file_per_table is used, I think OPTIMIZE TABLE for affected table(s) will free the &#8216;hijacked&#8217; diskspace to the system &#8211; provided that there is still enough diskspace to execute OPTIMIZE TABLE, of course).
The lack of willingness of Oracle here to expose the exact conditions for this bug to affected users (and how to repair it should they be affected) is the real problem here. Release notes http://dev.mysql.com/doc/refman/5.5/en/news-5-5-25a.html just say &#8220;A regression bug in the optimizer could cause excessive disk usage for UPDATE statements.&#8220;. This does not really help anyone to understand and to cope with the bug.


Tweet
";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Fri, 06 Jul 2012 19:17:41 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:3090:"<p>MySQL 5.5.25a has been released with the fix for the bug <a href="http://bugs.mysql.com/bug.php?id=65745">http://bugs.mysql.com/bug.php?id=65745</a>. This bug &#8211; introduced in 5.5.25 &#8211; could ultimately have the result that <em><strong>a single UPDATE statement made all disk space on the system unavailable</strong></em>. The InnoDB tablespace(s) grow(s)  even though data and indexes do not require more disk space.</p>
<p>This is of course an ugly bug &#8211; but &#8216;<em>s&#8230; happens</em>&#8216; they say. So it is not the purpose of this to ridicule or expose anyone. After all it was fixed  as soon as possible after the bug was reported. But after the fix has been released<em><strong> it is still unclear what exact conditions will trigger the effect</strong></em>.</p>
<p>Kolbe Kegel from SkySQL who reported the bug report, provided this CREATE TABLE statement (the simplest he could find to reproduce the issue)</p>
<p><em>CREATE TABLE t1 (</em><br />
<em> id1 int NOT NULL,</em><br />
<em> id2 int NOT NULL,</em><br />
<em> a int,</em><br />
<em> b int,</em><br />
<em> PRIMARY KEY (id1,id2),</em><br />
<em> KEY (id1, a)</em><br />
<em>) ENGINE=InnoDB DEFAULT CHARSET=utf8;</em></p>
<p>.. and further added that &#8220;<em>removing any further row, column or index makes the problem disappear</em>&#8220;. But <strong>WHY</strong>? <em><strong>Why will it help to drop the index `id1` or the column `b` </strong></em>for instance?</p>
<p>Also <em><strong>what is the fix for this for users affected</strong></em>? If not innodb_file_per_table is used I cannot find any other solution than to<em><strong> dump everything and restore</strong></em> to a fresh server or InnodDB instance (if innodb_file_per_table is used, I think OPTIMIZE TABLE for affected table(s) will free the &#8216;hijacked&#8217; diskspace to the system &#8211; provided that there is still enough diskspace to execute OPTIMIZE TABLE, of course).</p>
<p>The <em><strong>lack of willingness of Oracle here to expose</strong></em> the exact conditions for <em><strong>this bug</strong> <strong>to</strong></em> affected <em><strong>users</strong></em> (and how to repair it should they be affected) is the real problem here. Release notes <a href="http://dev.mysql.com/doc/refman/5.5/en/news-5-5-25a.html">http://dev.mysql.com/doc/refman/5.5/en/news-5-5-25a.html</a> just say &#8220;<em>A regression bug in the optimizer could cause excessive disk usage for UPDATE statements.</em>&#8220;. This does not really help anyone to understand and to cope with the bug.</p>

<!-- This is the start of the WP Twitter Button code -->
<div><a href="http://twitter.com/share" data-url="http://www.webyog.com/blog/2012/07/06/what-were-the-conditions-and-what-is-the-fix/" data-count="horizontal" data-via="webyog">Tweet</a></div>
<!-- This is the end of the WP Twitter Button code --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33760&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33760&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:6:"Webyog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:28;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:27:"Mozilla DB News, Fri 6 July";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:28:"666 at http://www.sheeri.com";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:56:"http://www.sheeri.com/content/mozilla-db-news-fri-6-july";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1158:"The database team has been pretty busy, since half of it (me!) has been traveling. We will get to that later, but first, I have an important news item to share with you: Bassel Khartabil, an open source developer and a Creative Commons volunteer is being detained in Syria, with no explanation why. His family, including his fiancee, have gotten no reason why he has been detained for over four months. Please read more and sign a petition to free Bassel so he can come home to his family and continue his important volunteer contributions to the open source movement.
In happier news, I mentioned travel. Where I spoke and my slides are listed here:
At the 1st Latin American Conference on MySQL, NoSQL and the Cloud I co-taught a tutorial on Monitoring MySQL (my monitoring MySQL with Nagios PDF slides) with Gerry Narvaja of Tokutek (and the OurSQL Podcast co-host).

I also gave a talk on MySQL Security at the conference.
This week I gave 2 talks in Cali, Colombia and 2 talks in Quito, Ecuador as part of the OTN Latin America Tour (North). Those talks were:

Optimizing MySQL Queries using EXPLAIN (given in both Cali and Quito) and do not forget the ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Fri, 06 Jul 2012 19:04:33 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4108:"<p>The database team has been pretty busy, since half of it (me!) has been traveling. We will get to that later, but first, I have an important news item to share with you: Bassel Khartabil, an open source developer and a Creative Commons volunteer is being detained in Syria, with no explanation why. His family, including his fiancee, have gotten no reason why he has been detained for over four months. Please <a href="https://blog.lizardwrangler.com/2012/07/05/contributor-imprisoned-in-syria/">read more and sign a petition to free Bassel</a> so he can come home to his family and continue his important volunteer contributions to the open source movement.</p>
<p>In happier news, I mentioned travel. Where I spoke and my slides are listed here:<br />
<UL><LI>At the <a href="http://mysqlnosqlcloud.com">1st Latin American Conference on MySQL, NoSQL and the Cloud</a> I co-taught a tutorial on Monitoring MySQL (<a href="http://technocation.org/files/doc/2012_06_Nagios.pdf">my monitoring MySQL with Nagios PDF slides</a>) with <a href="http://www.tokutek.com/author/gerry/">Gerry Narvaja of Tokutek</a> (and the <a href="http://www.oursql.com">OurSQL Podcast</a> co-host).</li></ul></p>
<p></p>
<p><LI>I also gave a talk on <a href="http://technocation.org/files/doc/2012_05_MySQLSecurityPres.pdf">MySQL Security</a> at the conference.</li></p>
<p><LI>This week I gave 2 talks in Cali, Colombia and 2 talks in Quito, Ecuador as part of the <a href="http://www.oracle.com/technetwork/es/community/user-groups/otn-latinoamerica-tour-2012-1634120-esa.html">OTN Latin America Tour (North)</a>. Those talks were:<br />
<UL><br />
<LI><a href="http://technocation.org/files/doc/EXPLAIN.pdf">Optimizing MySQL Queries using EXPLAIN</a> (given in both Cali and Quito) and do not forget the <A HREF="http://www.pythian.com/news/wp-content/uploads/explain-diagram.pdf>EXPLAIN cheat sheet</a> by Pythian<br />
</li><LI><a href="http://technocation.org/files/doc/2012_07_MySQL_Events.pdf">Getting Rid of Cron Scripts Using MySQL Events</a> (Cali only)<br />
</li><LI><a href="http://technocation.org/files/doc/2012_05_MySQLSecurityPres.pdf">MySQL Security</a> (Quito only)<br />
</li></ul></li></p>
<p>And in actual work, here&#8217;s what the database team has accomplished:<br />
<LI>Supported a bugzilla downtime. Submitted for your entertainment, the maintenance page:<br />
<a href="http://hardhat.mozilla.net/en-US/bugzilla.html"><img src="http://hardhat.mozilla.net/img/tignish/template/background-outages-bugzie.png" /></a><br />
</li><LI>Did a historical purge of the <a href="https://crash-stats.mozilla.com/home/dashboard">Crash-stats</a> database<br />
</li><LI>Refreshed the stage db with production data for the <a href="https://crash-stats.mozilla.com/home/dashboard">Crash-stats</a> database<br />
</li><LI>Migrated the database for our internal app that tracks vacation/sick time (called &#8220;PTO&#8221;, or &#8220;paid time off&#8221;)<br />
</li><LI>Started backing up our new webdev database cluster<br />
</li><LI>Audited how we do binary log rotation/purging/backup<br />
</li><LI>Cleared up a lot of legacy development databases that are not used, or do not need to be backed up, on our backup server. This caused the backup server&#8217;s development instance to go from over 300G of space used to under 50G, which makes backups faster and more efficient, since we are not backing up redundant databases nor older databases.<br />
</li><LI>Defragmented tables in the <a href="http://support.mozilla.org">support database</a> to get rid of deadlocks that were occurring<br />
</li><LI>Set <a href="http://support.mozilla.org">support database</a> backups to transfer to a support.mozilla.org tools server for developer use<br />
</li><LI>Created backups for the new builder database servers for the <a href="http://addons.mozilla.org">Addons</a> service.<br />
</li><br />
</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33759&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33759&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:16:"Sheeri K. Cabral";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:29;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:49:"MySQL Enterprise Monitor 2.3.11 Is Now Available!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:76:"https://blogs.oracle.com/mysqlenterprise/entry/mysql_enterprise_monitor_2_37";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:76:"https://blogs.oracle.com/mysqlenterprise/entry/mysql_enterprise_monitor_2_37";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1128:" 
    We are pleased to announce that MySQL Enterprise Monitor 2.3.11 is now available for download on the My Oracle Support (MOS) web site. It will also be available via the Oracle Software Delivery Cloud in approximately 1-2 weeks. This is a maintenance release that contains several new features and fixes a number of bugs. You can find more information on the contents of this release in the changelog:
     http://dev.mysql.com/doc/mysql-monitor/2.3/en/mem-news-2-3-11.html  You will find binaries for the new release on My Oracle Support:
     https://support.oracle.com  Choose the &quot;Patches &amp; Updates&quot; tab, and then use the &quot;Product or Family (Advanced Search)&quot; feature.
     And from the Oracle Software Delivery Cloud (in about 1-2 weeks):
     http://edelivery.oracle.com/  Choose &quot;MySQL Database&quot; as the Product Pack and you will find the Enterprise Monitor along with other MySQL products.
     If you haven't looked at 2.3 recently, please do so now and let us know what you think.
      
    Thanks and Happy Monitoring!
     - The MySQL Enterprise Tools Development Team
     
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 21:12:45 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:12:"New Releases";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1920:"<div> 
    <p>We are pleased to announce that MySQL Enterprise Monitor 2.3.11 is now available for download on the My Oracle Support (MOS) web site. It will also be available via the Oracle Software Delivery Cloud in approximately 1-2 weeks. This is a maintenance release that contains several new features and fixes a number of bugs. You can find more information on the contents of this release in the changelog:
    <br /> <br /><a href="http://dev.mysql.com/doc/mysql-monitor/2.3/en/mem-news-2-3-11.html">http://dev.mysql.com/doc/mysql-monitor/2.3/en/mem-news-2-3-11.html</a> <br /> <br />You will find binaries for the new release on My Oracle Support:
    <br /> <br /><a href="https://support.oracle.com/">https://support.oracle.com</a> <br /> <br />Choose the &quot;Patches &amp; Updates&quot; tab, and then use the &quot;Product or Family (Advanced Search)&quot; feature.
    <br /> <br />And from the Oracle Software Delivery Cloud (in about 1-2 weeks):
    <br /> <br /><a href="http://edelivery.oracle.com/">http://edelivery.oracle.com/</a> <br /> <br />Choose &quot;MySQL Database&quot; as the Product Pack and you will find the Enterprise Monitor along with other MySQL products.
    <br /> <br />If you haven't looked at 2.3 recently, please do so now and let us know what you think.
    <br /> </p> 
    <p>Thanks and Happy Monitoring!
    <br /> <br />- The MySQL Enterprise Tools Development Team
  </p> <!-- mem-advisor-bundle-version: 2.3.11.2165 --> <!-- mem-advisor-url: https://updates.oracle.com/Orion/Services/download/p14283675_23_Generic.zip?aru=15221504&patch_file=p14283675_23_Generic.zip --><!-- mem-advisor-bundle-filename: AdvisorScript-Platinum-2.3.11.2165.jar --> 
  </div><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33756&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33756&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:28:"MySQL Enterprise Tools  Team";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:30;a:6:{s:4:"data";s:78:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:27:"A CTO Must Never Do This…";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:29:"http://www.iheavy.com/?p=4745";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:53:"http://feedproxy.google.com/~r/iheavy/~3/rn8FryZqmiM/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2306:"Read the original article at A CTO Must Never Do This&#8230;A couple years back I was contacted to look at a very strange problem.The firm ran flash sales.  An email goes out at noon, the website traffic explodes for a couple of hours, then settles back down to a trickle.Of course you might imagine where this is going.  During that peak, the MySQL database was brought to its knees.  I was asked to do analysis during this peak load, and identify and fix problems.  Make it go faster, please!First day on the job I&#8217;m working with a team of outsourced DBAs.  I was also working with a sort of swat team chatting on SKYPE, while monitoring the systems closely.Then up popped one comment from a gentlemen I hadn&#8217;t worked with.  He insisted there was contention for a little known MySQL resource called the AUTO_INC lock.  Since I wanted to know more, I asked who the guy was and to my surprise he turned out to be the CTO.The CTO was tuning and troubleshooting the database!Wow, that&#8217;s a first.  I thought I&#8217;d seen it all.  A CTO is normally overseeing technology &#038; the team rather than crawling around in the trenches on the front line.This all raised some important points1. The app was having major growing pains 2. Current architecture was not scaling 3. Amazon elasticity was not helping at the database layer 4. People &#038; process were also failing, hence the CTOs hands on approachIt was shocking to see a problem deteriorate to this point, but when you consider the context its understandable.  A company like this is struggling with hypergrowth to such a degree, that each day seems like a hurricane storm.  With emergency meetings, followed by hardware &#038; application emergencies, trouble seems constant.  It can be very difficult to step back and see the larger picture.The takeaway from this experience…o Amazon EC2 can&#8217;t do it all &#8211; consider physical servers for disk intensive apps o MySQL still has some real scalability limitations o use technology for its intended purpose &#8211; MySQL isn&#8217;t great for queueing o A CTO tuning the database means problems have deteriorated too farRead all the way to the end? Grab our newsletter &#8211; scalable startups.No related posts.For more articles like these go to iHeavy, Inc +1-212-533-6828";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 20:19:14 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:8:{i:0;a:5:{s:4:"data";s:3:"All";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:7:"CTO/CIO";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:8:"Startups";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:11:"War Stories";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:3:"aws";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:3:"dba";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:15:"troubleshooting";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:6:"tuning";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:3418:"<p>Read the original article at <a href="http://www.iheavy.com/2012/07/05/a-cto-must-never-do-this-2/">A CTO Must Never Do This&#8230;</a></p><p><a href="http://www.iheavy.com/2012/07/05/a-cto-must-never-do-this/there-are-terrific-rain-storms-as-the-british-attack-the-germans/" rel="attachment wp-att-4729"><img src="http://www.iheavy.com/wp-content/uploads/2012/07/trenches.jpg" alt="" title="soldiers in the trenches" width="275" height="253" class="alignleft size-full wp-image-4729" /></a></p><p>A couple years back I was contacted to look at a very strange problem.</p><p>The firm ran flash sales.  An email goes out at noon, the website traffic explodes for a couple of hours, then settles back down to a trickle.</p><p>Of course you might imagine where this is going.  During that peak, the MySQL database was brought to its knees.  I was asked to do analysis during this peak load, and identify and fix problems.  Make it go faster, please!</p><p>First day on the job I&#8217;m working with a team of outsourced DBAs.  I was also working with a sort of swat team chatting on SKYPE, while monitoring the systems closely.</p><p>Then up popped one comment from a gentlemen I hadn&#8217;t worked with.  He insisted there was contention for a little known MySQL resource called the AUTO_INC lock.  Since I wanted to know more, I asked who the guy was and to my surprise he turned out to be the CTO.</p><div>The CTO was tuning and troubleshooting the database!</div><p>Wow, that&#8217;s a first.  I thought I&#8217;d seen it all.  A CTO is normally overseeing technology &#038; the team rather than crawling around in the trenches on the front line.</p><h2>This all raised some important points</h2><p>1. The app was having major growing pains<br
/> 2. Current architecture was not scaling<br
/> 3. Amazon elasticity was not helping at the database layer<br
/> 4. People &#038; process were also failing, hence the CTOs hands on approach</p><p>It was shocking to see a problem deteriorate to this point, but when you consider the context its understandable.  A company like this is struggling with hypergrowth to such a degree, that each day seems like a hurricane storm.  With emergency meetings, followed by hardware &#038; application emergencies, trouble seems constant.  It can be very difficult to step back and see the larger picture.</p><h2>The takeaway from this experience…</h2><p>o Amazon EC2 can&#8217;t do it all &#8211; consider physical servers for disk intensive apps<br
/> o MySQL still has some real scalability limitations<br
/> o use technology for its intended purpose &#8211; <a href="http://www.engineyard.com/blog/2011/5-subtle-ways-youre-using-mysql-as-a-queue-and-why-itll-bite-you/">MySQL isn&#8217;t great for queueing</a><br
/> o A CTO tuning the database means problems have deteriorated too far</p><p>Read all the way to the end? <a href="http://www.iheavy.com/signup-scalable-startups-newsletter/">Grab our newsletter &#8211; scalable startups.</a></p><p>No related posts.</p><p>For more articles like these go to <a href="http://www.iheavy.com">iHeavy, Inc +1-212-533-6828</a></p><img src="http://feeds.feedburner.com/~r/iheavy/~4/rn8FryZqmiM" height="1" width="1" /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33755&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33755&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:9:"Sean Hull";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:31;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:48:"Impact of memory allocators on MySQL performance";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:44:"http://www.mysqlperformanceblog.com/?p=10115";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:96:"http://www.mysqlperformanceblog.com/2012/07/05/impact-of-memory-allocators-on-mysql-performance/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4778:"MySQL server intensively uses dynamic memory allocation so a good choice of memory allocator is quite important for the proper utilization of CPU/RAM resources. Efficient memory allocator should help to improve scalability, increase throughput and keep memory footprint under the control. In this post I&#8217;m going to check impact of several memory allocators on the performance/scalability of MySQL server in the read-only workloads.
For my testing i chose following allocators: lockless, jemalloc-2.2.5, jemalloc-3.0, tcmalloc(gperftools-2.0), glibc-2.12.1(new malloc)(CentOS 6.2), glibc-2.13(old malloc), glibc-2.13(new malloc), glibc-2.15(new malloc).
Let me clarify a bit about malloc in glibc. Starting from glibc-2.10 it had two malloc implementations that one can choose with configure option &#8211;enable-experimental-malloc. (You can find details about new malloc here). Many distros switched to this new malloc in 2009. From my experince this new malloc behaved not always efficiently with MySQL so i decided to include old one to comparison as well. I used glibc-2.13 for that purpose because later &#8211;enable-experimental-malloc option was removed from glibc sources.
I built all allocators from sources(except system glibc 2.12.1) with stock CentOS gcc(version 4.4.6 20110731). All of them were built with -O3. I used LD_PRELOAD for lockless, jemalloc-2.2.5, jemalloc-3.0, tcmalloc and for glibc I prefixed mysqld with:
 /[path]/glibc-[X].root/lib/ld-[X].so --library-path /[path]/glibc-[X].root/lib:/lib64:/usr/lib64 

Testing details:

Cisco USC_C250 box
Percona Server 5.5.24
2 read only scnearios: OLTP_RO and POINT_SELECT from the latest sysbench-0.5
dataset consists of 4 sysbench tables(50M rows each) ~50G data / CPU bound case
innodb_buffer_pool_size=52G

For every malloc allocator  perform the following steps:

start Percona server either with LD_PRELOAD=[allocator_lib.so] or glibc prefix(see above)/get RSS/VSZ size of mysqld
 warmup with &#8216;select avg(id) from sbtest$i FORCE KEY (PRIMARY)&#8217; and then OLTP_RO for 600sec
 run OLTP_RO/POINT_SELECT test cases, duration 300 sec and vary number of threads: 8/64/128/256/512/1024/1536
 stop server/get RSS/VSZ size of mysqld



The best throughput/scalability we have with lockless/jemalloc-3.0/tcmalloc. jemalloc-2.2.5 slightly drops with higher number of threads. On the graph with response time(see below) there are spikes for it that may be caused by some contention in the lib.  All variations of glibc that are based on new malloc with increasing concurrency demonstrate notable drops &#8211; almost two times at high threads. In the same time glibc-2.13 built with old malloc looks good, results are very similar to lockless/jemalloc-3.0/tcmalloc.


For POINT_SELECT test with increasing concurrency we have two allocators that handle load very well &#8211; tcmalloc and only slightly behind &#8230; glibc-2.13 with old malloc. Then we have jemalloc-3.0/lockless/jemalloc-2.2.5 and last ones are glibc allocators based on new malloc. Along with the best throughput/scalability runs with tcmalloc also demonstrate best response time (30-50 ms at the high threads).

Besides throughput and latency there is one more factor that should be taken into account &#8211; memory footprint.



memory allocator
mysqld RSS size grow(kbytes)
mysqld VSZ size grow(kbytes)


lockless
6.966.736
105.780.880


jemalloc-2.2.5
214.408
3.706.880


jemalloc-3.0
216.084
5.804.032


tcmalloc
456.028
514.544


glibc-2.13-new-malloc
210.120
232.624


glibc-2.13-old-malloc
253.568
1.006.204


glibc-2.12.1-system
162.952
215.064


glibc-2.15-new-malloc
5.106.124
261.636



The only two allocators lockless and glibc-2.15-with new malloc notably incressed RSS memory footprint of mysqld server &#8211; more than on 5G. Memory usage for others allocators looks more or less acceptable. 
Taking into account all 3 factors &#8211; throughput, latency and memory usage for above POINT_SELECT/OLTP_RO type of workloads the most suitable allocators are tcmalloc, jemalloc-3.0 and glibc-2.13 with old malloc. 
Important point to take is that new glibc with new malloc implementation may be NOT suitable and may show worse results than on older platforms.
UPDATE:
To cover some questions raised in the comments I rerun OLTP_RO/POINT_SELECT tests with jemalloc-2.2.5/jemalloc-3.0/tcmalloc, varied /sys/kernel/mm/transparent_hugepage/enabled(always|never) and gathered mysqld size with &#8216;ps &#8211;sort=-rss -eopid,rss,vsz,pcpu&#8217; during the test run. Just to remind whole test run cycle looks like following:
start server, warmup, OLTP_RO test, POINT_SELECT test. So on charts below you will see how mysqld footprint  is changed during the test cycle and what is the impact of disabling of hugepages.


";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 17:39:18 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:10:"Benchmarks";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:8030:"<p>MySQL server intensively uses dynamic memory allocation so a good choice of memory allocator is quite important for the proper utilization of CPU/RAM resources. Efficient memory allocator should help to improve scalability, increase throughput and keep memory footprint under the control. In this post I&#8217;m going to check impact of several memory allocators on the performance/scalability of MySQL server in the read-only workloads.</p>
<p>For my testing i chose following allocators: <a href="http://locklessinc.com" />lockless</a>, <a href="http://www.canonware.com/jemalloc" />jemalloc-2.2.5, jemalloc-3.0</a>, <a href="http://code.google.com/p/gperftools" />tcmalloc(gperftools-2.0)</a>, glibc-2.12.1(new malloc)(CentOS 6.2), <a href="http://www.gnu.org/software/libc" />glibc-2.13(old malloc), glibc-2.13(new malloc), glibc-2.15(new malloc)</a>.<br/></p>
<p>Let me clarify a bit about malloc in glibc. Starting from glibc-2.10 it had two malloc implementations that one can choose with configure option <em>&#8211;enable-experimental-malloc</em>. (You can find details about new malloc <a href="http://udrepper.livejournal.com/20948.html">here).</a> Many distros switched to this new malloc in 2009. From my experince this new malloc behaved not always efficiently with MySQL so i decided to include old one to comparison as well. I used glibc-2.13 for that purpose because later <em>&#8211;enable-experimental-malloc</em> option was removed from glibc sources.<br/></p>
<p>I built all allocators from sources(except system glibc 2.12.1) with stock CentOS gcc(version 4.4.6 20110731). All of them were built with -O3. I used LD_PRELOAD for lockless, jemalloc-2.2.5, jemalloc-3.0, tcmalloc and for glibc I prefixed mysqld with:</p>
<pre> /[path]/glibc-[X].root/lib/ld-[X].so --library-path /[path]/glibc-[X].root/lib:/lib64:/usr/lib64 </pre>
<ul>
<li>Testing details:</p>
<ul>
<li><a href="http://www.percona.com/docs/wiki/benchmark%3Ahardware%3Acisco_ucs_c250">Cisco USC_C250 box</a>
<li>Percona Server 5.5.24
<li>2 read only scnearios: OLTP_RO and POINT_SELECT from the latest <a href="https://launchpad.net/sysbench" />sysbench-0.5</a>
<li>dataset consists of 4 sysbench tables(50M rows each) ~50G data / CPU bound case
<li>innodb_buffer_pool_size=52G
</ul>
<li>For every malloc allocator  perform the following steps:
<ul>
<li>start Percona server either with LD_PRELOAD=[allocator_lib.so] or glibc prefix(see above)/get RSS/VSZ size of mysqld
<li> warmup with &#8216;select avg(id) from sbtest$i FORCE KEY (PRIMARY)&#8217; and then OLTP_RO for 600sec
<li> run OLTP_RO/POINT_SELECT test cases, duration 300 sec and vary number of threads: 8/64/128/256/512/1024/1536
<li> stop server/get RSS/VSZ size of mysqld
</ul>
</ul>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tps_OLTP_RO.data_.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tps_OLTP_RO.data_.png" alt="" title="tps_OLTP_RO.data" width="616" height="491" class="alignnone size-full wp-image-10147" /></a></p>
<p>The best throughput/scalability we have with lockless/jemalloc-3.0/tcmalloc. jemalloc-2.2.5 slightly drops with higher number of threads. On the graph with response time(see below) there are spikes for it that may be caused by some contention in the lib.  All variations of glibc that are based on new malloc with increasing concurrency demonstrate notable drops &#8211; almost two times at high threads. In the same time glibc-2.13 built with old malloc looks good, results are very similar to lockless/jemalloc-3.0/tcmalloc.</p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/pct99_OLTP_RO.data_.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/pct99_OLTP_RO.data_.png" alt="" title="pct99_OLTP_RO.data" width="616" height="491" class="alignnone size-full wp-image-10145" /></a></p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tps_POINT_SELECT.data_.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tps_POINT_SELECT.data_.png" alt="" title="tps_POINT_SELECT" width="616" height="491" class="alignnone size-full wp-image-10148" /></a></p>
<p>For POINT_SELECT test with increasing concurrency we have two allocators that handle load very well &#8211; tcmalloc and only slightly behind &#8230; glibc-2.13 with old malloc. Then we have jemalloc-3.0/lockless/jemalloc-2.2.5 and last ones are glibc allocators based on new malloc. Along with the best throughput/scalability runs with tcmalloc also demonstrate best response time (30-50 ms at the high threads).</p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/pct99_POINT_SELECT.data_.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/pct99_POINT_SELECT.data_.png" alt="" title="pct99_POINT_SELECT" width="616" height="491" class="alignnone size-full wp-image-10146" /></a></p>
<p>Besides throughput and latency there is one more factor that should be taken into account &#8211; memory footprint.</p>

<table>
<tr>
<th>memory allocator</th>
<th>mysqld RSS size grow(kbytes)</th>
<th>mysqld VSZ size grow(kbytes)</th>
</tr>
<tr>
<td>lockless</td>
<td>6.966.736</td>
<td>105.780.880</td>
</tr>
<tr>
<td>jemalloc-2.2.5</td>
<td>214.408</td>
<td>3.706.880</td>
</tr>
<tr>
<td>jemalloc-3.0</td>
<td>216.084</td>
<td>5.804.032</td>
</tr>
<tr>
<td>tcmalloc</td>
<td>456.028</td>
<td>514.544</td>
</tr>
<tr>
<td>glibc-2.13-new-malloc</td>
<td>210.120</td>
<td>232.624</td>
</tr>
<tr>
<td>glibc-2.13-old-malloc</td>
<td>253.568</td>
<td>1.006.204</td>
</tr>
<tr>
<td>glibc-2.12.1-system</td>
<td>162.952</td>
<td>215.064</td>
</tr>
<tr>
<td>glibc-2.15-new-malloc</td>
<td>5.106.124</td>
<td>261.636</td>
</tr>
</table>
<p></p>
<p>The only two allocators lockless and glibc-2.15-with new malloc notably incressed RSS memory footprint of mysqld server &#8211; more than on 5G. Memory usage for others allocators looks more or less acceptable. </p>
<p>Taking into account all 3 factors &#8211; throughput, latency and memory usage for above POINT_SELECT/OLTP_RO type of workloads the most suitable allocators are tcmalloc, jemalloc-3.0 and glibc-2.13 with old malloc. </p>
<p>Important point to take is that new glibc with new malloc implementation may be NOT suitable and may show worse results than on older platforms.</p>
<p><strong>UPDATE:</strong></p>
<p>To cover some questions raised in the comments I rerun OLTP_RO/POINT_SELECT tests with jemalloc-2.2.5/jemalloc-3.0/tcmalloc, varied /sys/kernel/mm/transparent_hugepage/enabled(always|never) and gathered mysqld size with &#8216;ps &#8211;sort=-rss -eopid,rss,vsz,pcpu&#8217; during the test run. Just to remind whole test run cycle looks like following:<br />
start server, warmup, OLTP_RO test, POINT_SELECT test. So on charts below you will see how mysqld footprint  is changed during the test cycle and what is the impact of disabling of hugepages.</p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/jem.225.6721.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/jem.225.6721.png" alt="" title="jem.225.672" width="700" height="300" class="alignnone size-full wp-image-10352" /></a></p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/jem.3.655.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/jem.3.655.png" alt="" title="jem.3.655" width="700" height="300" class="alignnone size-full wp-image-10345" /></a></p>
<p><a href="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tcm.658.png"><img src="http://www.mysqlperformanceblog.com/wp-content/uploads/2012/07/tcm.658.png" alt="" title="tcm.658" width="700" height="300" class="alignnone size-full wp-image-10349" /></a></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33753&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33753&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:22:"MySQL Performance Blog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:32;a:6:{s:4:"data";s:58:"
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:36:"Who’s leaking prepared statements?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:39:"http://mysqlblog.fivefarmers.com/?p=229";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:77:"http://mysqlblog.fivefarmers.com/2012/07/05/whos-leaking-prepared-statements/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:8054:"In my last post, I described a specific problem with prepared statements into which PERFORMANCE_SCHEMA can give visibility.  That made me wonder whether PERFORMANCE_SCHEMA can also be used to identify other areas where prepared statements run into problems.  The most significant problem tends to be leakage of prepared statements.  This can inflate memory usage, both on the server and application side, and it&#8217;s not uncommon to find applications which fail to close prepared statements.
So the question is, what can PERFORMANCE_SCHEMA tell us about how connections close (or more importantly, fail to close) prepared statements?
At the most basic level, one can check the number of PREPARE statements executed compared to DEALLOCATE PREPARE, and you can do that using global status variables.  You&#8217;re shooting for general equality between (Com_prepare_sql + Com_stmt_prepare) and (Com_stmt_close + Com_dealloc_sql).  I&#8217;m using session status below to make it easy to follow, but obviously global status variables will be most interesting in any review of prepared statement leakage:

mysql&gt; flush status;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'com_%prepare%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| Com_prepare_sql    | 0     |
| Com_stmt_prepare   | 0     |
| Com_stmt_reprepare | 0     |
| Com_xa_prepare     | 0     |
+--------------------+-------+
4 rows in set (0.00 sec)

mysql&gt; PREPARE stmt FROM 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SHOW SESSION STATUS LIKE 'com_%prepare%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| Com_prepare_sql    | 1     |
| Com_stmt_prepare   | 1     |
| Com_stmt_reprepare | 0     |
| Com_xa_prepare     | 0     |
+--------------------+-------+
4 rows in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_stmt_close';
+----------------+-------+
| Variable_name  | Value |
+----------------+-------+
| Com_stmt_close | 0     |
+----------------+-------+
1 row in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_dealloc_sql';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| Com_dealloc_sql | 0     |
+-----------------+-------+
1 row in set (0.00 sec)

mysql&gt; DEALLOCATE PREPARE stmt;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_stmt_close';
+----------------+-------+
| Variable_name  | Value |
+----------------+-------+
| Com_stmt_close | 1     |
+----------------+-------+
1 row in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_dealloc_sql';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| Com_dealloc_sql | 1     |
+-----------------+-------+
1 row in set (0.00 sec)


That&#8217;s useful as a start to identify whether a problem exists, but it doesn&#8217;t help isolate the source very well.  PERFORMANCE_SCHEMA can take it the next step.
The PERFORMANCE_SCHEMA counters are smarter than the status variables, and we can count the number of prepared statements that have been opened (statement/sql/prepare_sql event for normal statements, statement/com/Prepare event for connections using COM_STMT_PREPARE protocol command) and the number closed (statement/sql/dealloc_sql event for normal statements, statement/com/Close stmt event for COM_STMT_CLOSE command).  Once can write a query which computes the difference between the prepares and closes:

SELECT sp.thread_id, t.processlist_user user, t.processlist_host host,
sp.count_star - sd.count_star open_com_query_ps,
cp.count_star - cc.count_star open_com_prepare_ps
FROM
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/sql/prepare_sql' ) sp
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/com/Prepare' ) cp
ON (cp.THREAD_ID = sp.THREAD_ID)
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/sql/dealloc_sql' ) sd
ON (sd.THREAD_ID = sp.THREAD_ID)
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/com/Close stmt' ) cc
ON (cc.THREAD_ID = sp.THREAD_ID)
JOIN threads t ON (t.thread_id = sp.thread_id)
ORDER BY GREATEST(open_com_query_ps, open_com_prepare_ps) DESC;

That&#8217;s nice, and it gives us output like the following:

+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 3 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.08 sec)


Of course, there are still ways in which this isn&#8217;t 100% accurate.  For example, one can do stuff like this:

+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 3 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.08 sec)

mysql&gt; PREPARE stmt FROM 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; PREPARE stmt FROM 'SELECT 2';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SELECT sp.thread_id, t.processlist_user user, t.processlist_host hos
-&gt; sp.count_star - sd.count_star open_com_query_ps,
-&gt; cp.count_star - cc.count_star open_com_prepare_ps
-&gt; FROM
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/sql/prepare_sql' ) sp
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/com/Prepare' ) cp
-&gt;  ON (cp.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/sql/dealloc_sql' ) sd
-&gt;  ON (sd.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/com/Close stmt' ) cc
-&gt;  ON (cc.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN threads t ON (t.thread_id = sp.thread_id)
-&gt;  WHERE sp.count_star - sd.count_star &lt;&gt; 0
-&gt;   OR cp.count_star - cc.count_star &lt;&gt; 0
-&gt; ORDER BY GREATEST(open_com_query_ps, open_com_prepare_ps) DESC;
+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 5 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.09 sec)

In the above, I prepared the same statement handler twice without closing it.
I know Marc has some ideas for further PERFORMANCE_SCHEMA monitoring of prepared statements, but what&#8217;s implemented today is a good step forward.  It&#8217;s easier to tie a potential prepared statement leak to specific accounts, and to isolate the leakage to protocol-based prepared statements (drivers using COM_PREPARE) or the more generic COM_QUERY interface used by the mysql command-line interface.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 17:30:02 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:4:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:9:"MySQL 5.6";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:18:"performance_schema";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:19:"prepared statements";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:8528:"<p>In my last post, I described a specific problem with prepared statements into which PERFORMANCE_SCHEMA can give visibility.  That made me wonder whether PERFORMANCE_SCHEMA can also be used to identify other areas where prepared statements run into problems.  The most significant problem tends to be leakage of prepared statements.  This can inflate memory usage, both on the server and application side, and it&#8217;s not uncommon to find applications which fail to close prepared statements.</p>
<p>So the question is, what can PERFORMANCE_SCHEMA tell us about how connections close (or more importantly, fail to close) prepared statements?</p>
<p>At the most basic level, one can check the number of PREPARE statements executed compared to DEALLOCATE PREPARE, and you can do that using global status variables.  You&#8217;re shooting for general equality between (Com_prepare_sql + Com_stmt_prepare) and (Com_stmt_close + Com_dealloc_sql).  I&#8217;m using session status below to make it easy to follow, but obviously global status variables will be most interesting in any review of prepared statement leakage:</p>
<pre>
mysql&gt; flush status;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'com_%prepare%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| Com_prepare_sql    | 0     |
| Com_stmt_prepare   | 0     |
| Com_stmt_reprepare | 0     |
| Com_xa_prepare     | 0     |
+--------------------+-------+
4 rows in set (0.00 sec)

mysql&gt; PREPARE stmt FROM 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SHOW SESSION STATUS LIKE 'com_%prepare%';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| Com_prepare_sql    | 1     |
| Com_stmt_prepare   | 1     |
| Com_stmt_reprepare | 0     |
| Com_xa_prepare     | 0     |
+--------------------+-------+
4 rows in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_stmt_close';
+----------------+-------+
| Variable_name  | Value |
+----------------+-------+
| Com_stmt_close | 0     |
+----------------+-------+
1 row in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_dealloc_sql';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| Com_dealloc_sql | 0     |
+-----------------+-------+
1 row in set (0.00 sec)

mysql&gt; DEALLOCATE PREPARE stmt;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_stmt_close';
+----------------+-------+
| Variable_name  | Value |
+----------------+-------+
| Com_stmt_close | 1     |
+----------------+-------+
1 row in set (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE 'Com_dealloc_sql';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| Com_dealloc_sql | 1     |
+-----------------+-------+
1 row in set (0.00 sec)

</pre>
<p>That&#8217;s useful as a start to identify whether a problem exists, but it doesn&#8217;t help isolate the source very well.  PERFORMANCE_SCHEMA can take it the next step.</p>
<p>The PERFORMANCE_SCHEMA counters are smarter than the status variables, and we can count the number of prepared statements that have been opened (statement/sql/prepare_sql event for normal statements, statement/com/Prepare event for connections using COM_STMT_PREPARE protocol command) and the number closed (statement/sql/dealloc_sql event for normal statements, statement/com/Close stmt event for COM_STMT_CLOSE command).  Once can write a query which computes the difference between the prepares and closes:</p>
<pre>
SELECT sp.thread_id, t.processlist_user user, t.processlist_host host,
sp.count_star - sd.count_star open_com_query_ps,
cp.count_star - cc.count_star open_com_prepare_ps
FROM
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/sql/prepare_sql' ) sp
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/com/Prepare' ) cp
ON (cp.THREAD_ID = sp.THREAD_ID)
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/sql/dealloc_sql' ) sd
ON (sd.THREAD_ID = sp.THREAD_ID)
JOIN
( SELECT COUNT_STAR,
THREAD_ID
FROM events_statements_summary_by_thread_by_event_name
WHERE event_name = 'statement/com/Close stmt' ) cc
ON (cc.THREAD_ID = sp.THREAD_ID)
JOIN threads t ON (t.thread_id = sp.thread_id)
ORDER BY GREATEST(open_com_query_ps, open_com_prepare_ps) DESC;
</pre>
<p>That&#8217;s nice, and it gives us output like the following:</p>
<pre>
+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 3 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.08 sec)

</pre>
<p>Of course, there are still ways in which this isn&#8217;t 100% accurate.  For example, one can do stuff like this:</p>
<pre>
+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 3 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.08 sec)

mysql&gt; PREPARE stmt FROM 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; PREPARE stmt FROM 'SELECT 2';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SELECT sp.thread_id, t.processlist_user user, t.processlist_host hos
-&gt; sp.count_star - sd.count_star open_com_query_ps,
-&gt; cp.count_star - cc.count_star open_com_prepare_ps
-&gt; FROM
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/sql/prepare_sql' ) sp
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/com/Prepare' ) cp
-&gt;  ON (cp.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/sql/dealloc_sql' ) sd
-&gt;  ON (sd.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN
-&gt;  ( SELECT COUNT_STAR,
-&gt;     THREAD_ID
-&gt;    FROM events_statements_summary_by_thread_by_event_name
-&gt;    WHERE event_name = 'statement/com/Close stmt' ) cc
-&gt;  ON (cc.THREAD_ID = sp.THREAD_ID)
-&gt;  JOIN threads t ON (t.thread_id = sp.thread_id)
-&gt;  WHERE sp.count_star - sd.count_star &lt;&gt; 0
-&gt;   OR cp.count_star - cc.count_star &lt;&gt; 0
-&gt; ORDER BY GREATEST(open_com_query_ps, open_com_prepare_ps) DESC;
+-----------+------+-----------+-------------------+---------------------+
| thread_id | user | host      | open_com_query_ps | open_com_prepare_ps |
+-----------+------+-----------+-------------------+---------------------+
|        22 | root | localhost |                 5 |                   0 |
+-----------+------+-----------+-------------------+---------------------+
1 row in set (0.09 sec)
</pre>
<p>In the above, I prepared the same statement handler twice without closing it.</p>
<p>I know Marc has some ideas for further PERFORMANCE_SCHEMA monitoring of prepared statements, but what&#8217;s implemented today is a good step forward.  It&#8217;s easier to tie a potential prepared statement leak to specific accounts, and to isolate the leakage to protocol-based prepared statements (drivers using COM_PREPARE) or the more generic COM_QUERY interface used by the mysql command-line interface.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33754&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33754&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:11:"Todd Farmer";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:33;a:6:{s:4:"data";s:73:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:74:"Balada Para Un Loco – A Review of the MySQL, NoSQL, and Cloud Conference";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:30:"http://www.tokutek.com/?p=4346";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:100:"http://www.tokutek.com/2012/07/balada-para-un-loco-a-review-of-the-mysql-nosql-and-cloud-conference/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:3565:"… Ya se que estás piantao, piantao, piantao…
For my lastest blog, a review of the MySQL, NoSQL and Cloud Conference, I&#8217;ll continue to use the tango metaphor. Balada para un loco (ballad for a crazy one) is a Piazzola classic and explains what I think of Santiago Lertora from Binlogic for single handedly putting together this event; he had to be piantao (slang for &#8216;crazy&#8217;) to pursue his vision to kick start the Open Source database community in South America into becoming as active as it is in the US and Europe. He was able to gather some renowned speakers such as our own Martin Farach-Colton, Sheeri Cabral from Mozilla, Max Mether and Massimo Brignoli from SkySQL, Colin Charles from Monty Program, Alejandro Kojima from the MySQL team at Oracle, Oracle ACE director Ronald Bradford, and many more. Among the firms attending, some also featuring speakers, were some of the best known Internet companies in South America such as despegar.com, OLX and MercadoLibre. The talks included non-MySQL presentations like the ones covering MongoDB and Cassandra topics.
For many foreigners who experience Argentina for the first time I always tell them that the first rule of business in Buenos Aires is: Chaos is part of the system. So don&#8217;t take it personally if deadlines aren&#8217;t met, meetings don&#8217;t start on time or plans change at the last minute. Wednesday truckers&#8217; strike and demonstration near the hotel (see Reuterscoverage), and Boca Juniors (the most popular soccer club in Argentina) first leg of their Libertadores&#8217; finals game (you can read more from the goal.com article) the same night didn&#8217;t help.
The tutorial I delivered with Sheeri Cabral and my talk, both around monitoring MySQL, had roughly the same number of people I would&#8217;ve expect at the MySQL Users Conference talks. We got great audience participation and feedback.
Martin&#8217;s talk was early Thursday morning, and given that the soccer game ended around midnight the previous night, not many people showed up on time. However, at the request of the attendees that missed it, he gave it again later in the day. Listening to him using Argentine cultural references to explain database indexing strategies was really entertaining for everybody and made the points very clear.
Right after Martin&#8217;s talk, I stayed to listen to Colin Charles where he explained the development process behind MariaDB. It&#8217;s interesting to get to know the process by which they take the original MySQL code as published by Oracle and apply the different patches to come up with their version. It&#8217;s a real challenge that they have solved in an interesting way for the benefit of MySQL users everywhere.
In summary it was a great event. The number of paying attendees and total people registered at the website were well above expectations. Most importantly, it achieved the goal of gathering a critical mass of local and international community members which hopefully will kick start an active local community as Santiago hoped for. Based on the feedback provided by attendees and speakers, I have no doubt that the 2013 edition will be even better. I look forward to being a part of it and I&#8217;m sure Martin wouldn&#8217;t mind another excuse to visit our native country.
Thank you Santiago for being piantao enough to organize this event and jump-start the regional Open Source database community in such a great way. We wish you the best.
You can read the lyrics in Spanish and English, and listen to this tango here.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 17:07:12 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:7:{i:0;a:5:{s:4:"data";s:8:"TokuView";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:10:"conference";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:6:"NewSQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:5:"NoSQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:6:"TokuDB";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:7:"Tokutek";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4500:"<p><em>… Ya se que estás piantao, piantao, piantao…</em></p>
<p>For my lastest blog, a review of the <a href="http://www.tokutek.com/2012/06/mi-buenos-aires-querido/" target="_blank">MySQL, NoSQL and Cloud Conference</a>, I&#8217;ll continue to use the tango metaphor. Balada para un loco (ballad for a crazy one) is a Piazzola classic and explains what I think of Santiago Lertora from Binlogic for single handedly putting together this event; he had to be <em>piantao</em> (slang for &#8216;crazy&#8217;) to pursue his vision to kick start the Open Source database community in South America into becoming as active as it is in the US and Europe. He was able to gather some renowned speakers such as our own Martin Farach-Colton, Sheeri Cabral from Mozilla, Max Mether and Massimo Brignoli from SkySQL, Colin Charles from Monty Program, Alejandro Kojima from the MySQL team at Oracle, Oracle ACE director Ronald Bradford, and many more. Among the firms attending, some also featuring speakers, were some of the best known Internet companies in South America such as <a href="http://www.despegar.com/" target="_blank">despegar.com</a>, <a href="http://www.olx.com.ar/" target="_blank">OLX</a> and <a href="http://www.mercadolibre.com/" target="_blank">MercadoLibre</a>. The talks included non-MySQL presentations like the ones covering MongoDB and Cassandra topics.</p>
<p>For many foreigners who experience Argentina for the first time I always tell them that the first rule of business in Buenos Aires is: <em>Chaos is part of the system</em>. So don&#8217;t take it personally if deadlines aren&#8217;t met, meetings don&#8217;t start on time or plans change at the last minute. Wednesday truckers&#8217; strike and demonstration near the hotel (see <a href="http://www.reuters.com/article/2012/06/27/us-argentina-strike-idUSBRE85Q1JC20120627" target="_blank">Reuterscoverage</a>), and <a href="http://www.bocajuniors.com.ar/home/sitio" target="_blank">Boca Juniors</a> (the most popular soccer club in Argentina) first leg of their Libertadores&#8217; finals game (you can read more from the <a href="http://www.goal.com/en-us/match/82151/boca-juniors-vs-corinthians/report" target="_blank">goal.com article</a>) the same night didn&#8217;t help.</p>
<p>The tutorial I delivered with Sheeri Cabral and my talk, both around monitoring MySQL, had roughly the same number of people I would&#8217;ve expect at the MySQL Users Conference talks. We got great audience participation and feedback.</p>
<p>Martin&#8217;s talk was early Thursday morning, and given that the soccer game ended around midnight the previous night, not many people showed up on time. However, at the request of the attendees that missed it, he gave it again later in the day. Listening to him using Argentine cultural references to explain database indexing strategies was really entertaining for everybody and made the points very clear.</p>
<p>Right after Martin&#8217;s talk, I stayed to listen to Colin Charles where he explained the development process behind MariaDB. It&#8217;s interesting to get to know the process by which they take the original MySQL code as published by Oracle and apply the different patches to come up with their version. It&#8217;s a real challenge that they have solved in an interesting way for the benefit of MySQL users everywhere.</p>
<p>In summary it was a great event. The number of paying attendees and total people registered at the website were well above expectations. Most importantly, it achieved the goal of gathering a critical mass of local and international community members which hopefully will kick start an active local community as Santiago hoped for. Based on the feedback provided by attendees and speakers, I have no doubt that the 2013 edition will be even better. I look forward to being a part of it and I&#8217;m sure Martin wouldn&#8217;t mind another excuse to visit our native country.</p>
<p>Thank you Santiago for being <em>piantao</em> enough to organize this event and jump-start the regional Open Source database community in such a great way. We wish you the best.</p>
<p>You can read the lyrics in Spanish and English, and listen to this tango <a href="http://letrasdetango.wordpress.com/2011/09/11/1079/" target="_blank">here.</a></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33752&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33752&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Tokuview Blog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:34;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:42:"MySQL Connect — Come learn from the best";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:41:"http://opensourcedba.wordpress.com/?p=877";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:85:"http://opensourcedba.wordpress.com/2012/07/05/mysql-connect-come-learn-from-the-best/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2563:"Did you ever wonder how the big companies run things behind the scenes?  Come to the MySQL Connect Conference and learn from the best.  There are over seventy sessions and the following session are from MySQL customers.  And please note the early registration discount window is closing!

Sheeri Cabral &#8212; Database Scaling at Mozilla  andGoogle-Hacking MySQL
Bradley Kuszmaul Chief Architect , MIT &#8212; Solving the Challenges of Big Databases with MySQL
Andrew Aksyonoff Sphinx Technologies Inc  &#8212; Full-Text Search with MySQL and Sphinx 
Daniel Austin Chief Architect , PayPal, Inc. &#8212; Big Data Is a Big Scam (Most of the Time) 
Eric Grancher DBA , Cern &#8212; CERN’s MySQL “as a Service” Deployment with Oracle VM: Empowering Users 
VADYM TKACHENKO Percona Inc &#8212; MySQL and Solid-State Drives: Usage and Tuning 
Henrik Ingo Senior Performance Architect , OpenLife.cc  &#8212; Evaluating MySQL High-Availability Alternatives 
Danil Zburivsky Pythian Group Inc &#8212; Debug and Fix Replication Issues Like a Pro 
Ronald Bradford Founder &amp; CEO , EffectiveMySQL  &#8212; Lessons from Managing 500+ MySQL Instances 	and
Chris Schneider &#8211; Sr. MySQL Architect, Ning.com &#8212; Improving Performance with Better Indexes  
MySQL and Hadoop
Jeremy Cole &#8211; ,   Davi Arnaut &#8211; Software Engineer, Twitter, Inc. &#8212; MySQL at Twitter: Development and Deployment
Giuseppe Maxia &#8211; QA Director, Continuent, Inc &#8212; MySQL High Availability: Power and Usability
Levi Junkert &#8211; , Facebook &#8212; MySQL Pool Scanner, an Automated Service for Host Management

Ed Presz &#8211; ,
    Andrew Yee &#8211; , Ticketmaster &#8212; Thriving in a MySQL Replicated World
Grant McAlister &#8211; Senior Principal Engineer, Amazon.com &#8212; Durability Is Key: How to Protect Your Data from Corruption
Yashwanth Nelapati &#8211; ,
    Marty Weiner &#8211; , Pinterest &#8212; Scaling Pinterest
Patrick Galbraith &#8211; , Hewlett Packard &#8211;Database as a Service: Database Resources on Demand

Singer Wang &#8211; , The Pythian Group Inc. &#8212; Balancing Safety and Performance in MySQL
Ligaya Turmelle &#8211; Web Database Administrator, Kaplan Professional &#8212; A Journey into NoSQLand: MySQL’s NoSQL Implementation
Rick James &#8211; MySQL Geek, Yahoo! Inc &#8212; Rick’s RoTs (Rules of Thumb)
Francisco Bordenave &#8211; , Pythian
and    Marco Tusa &#8211; , Pythian &#8211;Scaling MySQL with Multimaster Synchronous Replication

And there are also session from oracle&#8217;s MySQL Team too.  
         ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 16:15:31 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:14:"community team";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:18:"MySQL Connect 2012";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:4741:"<p>Did you ever wonder how the big companies run things behind the scenes?  Come to the <a href="http://www.oracle.com/mysqlconnect/index.html" target="_blank">MySQL Connect Conference</a> and learn from the best.  There are over seventy sessions and the following session are from MySQL customers.  And please note the early registration discount window is closing!</p>
<ul>
<li>Sheeri Cabral &#8212; Database Scaling at Mozilla  <em>and</em>Google-Hacking MySQL</li>
<li>Bradley Kuszmaul Chief Architect , MIT &#8212; Solving the Challenges of Big Databases with MySQL</li>
<li>Andrew Aksyonoff Sphinx Technologies Inc  &#8212; Full-Text Search with MySQL and Sphinx </li>
<li>Daniel Austin Chief Architect , PayPal, Inc. &#8212; Big Data Is a Big Scam (Most of the Time) </li>
<li>Eric Grancher DBA , Cern &#8212; CERN’s MySQL “as a Service” Deployment with Oracle VM: Empowering Users </li>
<li>VADYM TKACHENKO Percona Inc &#8212; MySQL and Solid-State Drives: Usage and Tuning </li>
<li>Henrik Ingo Senior Performance Architect , OpenLife.cc  &#8212; Evaluating MySQL High-Availability Alternatives </li>
<li>Danil Zburivsky Pythian Group Inc &#8212; Debug and Fix Replication Issues Like a Pro </li>
<li>Ronald Bradford Founder &amp; CEO , EffectiveMySQL  &#8212; Lessons from Managing 500+ MySQL Instances 	<i>and</i>
<li>Chris Schneider &#8211; Sr. MySQL Architect, Ning.com &#8212; Improving Performance with Better Indexes  </li>
<p>MySQL and Hadoop</li>
<li>Jeremy Cole &#8211; ,   Davi Arnaut &#8211; Software Engineer, Twitter, Inc. &#8212; MySQL at Twitter: Development and Deployment</li>
<li>Giuseppe Maxia &#8211; QA Director, Continuent, Inc &#8212; MySQL High Availability: Power and Usability</li>
<li>Levi Junkert &#8211; , Facebook &#8212; MySQL Pool Scanner, an Automated Service for Host Management
</li>
<li>Ed Presz &#8211; ,<br />
    Andrew Yee &#8211; , Ticketmaster &#8212; Thriving in a MySQL Replicated World</li>
<li>Grant McAlister &#8211; Senior Principal Engineer, Amazon.com &#8212; Durability Is Key: How to Protect Your Data from Corruption</li>
<li>Yashwanth Nelapati &#8211; ,<br />
    Marty Weiner &#8211; , Pinterest &#8212; Scaling Pinterest</li>
<li>Patrick Galbraith &#8211; , Hewlett Packard &#8211;Database as a Service: Database Resources on Demand
</li>
<li>Singer Wang &#8211; , The Pythian Group Inc. &#8212; Balancing Safety and Performance in MySQL</li>
<li>Ligaya Turmelle &#8211; Web Database Administrator, Kaplan Professional &#8212; A Journey into NoSQLand: MySQL’s NoSQL Implementation</li>
<li>Rick James &#8211; MySQL Geek, Yahoo! Inc &#8212; Rick’s RoTs (Rules of Thumb)</li>
<li>Francisco Bordenave &#8211; , Pythian<br />
and    Marco Tusa &#8211; , Pythian &#8211;Scaling MySQL with Multimaster Synchronous Replication</li>
</ul>
<p>And there are also session from oracle&#8217;s MySQL Team too.  </p>
<br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godelicious/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/delicious/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gofacebook/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/facebook/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gotwitter/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/twitter/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gostumble/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/stumble/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/godigg/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/digg/opensourcedba.wordpress.com/877/" /></a> <a rel="nofollow" href="http://feeds.wordpress.com/1.0/goreddit/opensourcedba.wordpress.com/877/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/reddit/opensourcedba.wordpress.com/877/" /></a> <img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=opensourcedba.wordpress.com&amp;blog=15386988&amp;post=877&amp;subd=opensourcedba&amp;ref=&amp;feed=1" width="1" height="1" /><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33751&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33751&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:11:"Dave Stokes";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:35;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:29:"MySQL Monitoring on the cloud";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.webyog.com/blog/?p=3844";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:68:"http://www.webyog.com/blog/2012/07/05/mysql-monitoring-on-the-cloud/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1177:"Hi,
Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. Amazon EC2 is known for its uptime, thereby making it the right candidate for running monitoring tools like MONyog. You cannot monitor something effectively, unless the monitoring system is available almost all the time.
We are happy to introduce pre-configured Amazon Machine Image (AMI) which runs MONyog on an Amazon Linux instance of EC2. Launching this AMI would create an EC2 instance and starts the trial version of MONyog. In short, with just a click of a button you get your favorite MySQL monitoring tool on a trusted platform that is available almost all the time. MONyog can monitor MySQL servers running on Amazon RDS, Amazon EC2 or any other remote machine.
Additionally, popular cloud database services like Amazon RDS cannot be monitored by agent-based monitoring tools, so if you are planning to migrate to cloud anytime soon, please consider evaluating MONyog.
Links:

Click here to try MONyog AMI
Technical help

We are excited about this release, and hope that you will like it. We would love to hear from you!
Cheers,
Team MONyog


Tweet
";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 12:35:54 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:6:"MONyog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2129:"<p>Hi,</p>
<p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. Amazon EC2 is known for its uptime, thereby making it the right candidate for running monitoring tools like MONyog. You cannot monitor something effectively, unless the monitoring system is available almost all the time.</p>
<p>We are happy to introduce <a href="https://aws.amazon.com/marketplace/pp/B008DWS6GC" target="_blank">pre-configured Amazon Machine Image (AMI)</a> which runs MONyog on an Amazon Linux instance of EC2. Launching this AMI would create an EC2 instance and starts the trial version of MONyog. In short, with just a click of a button you get your favorite MySQL monitoring tool on a trusted platform that is available almost all the time. MONyog can monitor MySQL servers running on Amazon RDS, Amazon EC2 or any other remote machine.</p>
<p><strong>Additionally, popular cloud database services like Amazon RDS <a href="http://www.webyog.com/blog/2011/01/25/is-your-mysql-monitoring-tool-cloud-ready/" target="_blank">cannot be monitored</a> by agent-based monitoring tools</strong>, so if you are planning to migrate to cloud anytime soon, please consider evaluating MONyog.</p>
<p>Links:</p>
<ul>
<li><a href="https://aws.amazon.com/marketplace/pp/B008DWS6GC" target="_blank">Click here to try MONyog AMI</a></li>
<li><a href="http://www.webyog.com/doc/MONyog/MONyog.htm#MONyog_AMI.htm" target="_blank">Technical help</a></li>
</ul>
<p>We are excited about this release, and hope that you will like it. We would love to hear from you!</p>
<p>Cheers,<br />
Team MONyog</p>

<!-- This is the start of the WP Twitter Button code -->
<div><a href="http://twitter.com/share" data-url="http://www.webyog.com/blog/2012/07/05/mysql-monitoring-on-the-cloud/" data-count="horizontal" data-via="webyog">Tweet</a></div>
<!-- This is the end of the WP Twitter Button code --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33750&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33750&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:6:"Webyog";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:36;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:54:"I’ll see you at MySQL Connect! I will … won’t I?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.markleith.co.uk/?p=1045";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:175:"http://www.markleith.co.uk/2012/07/05/ill-see-you-at-mysql-connect-i-will-wont-i/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=ill-see-you-at-mysql-connect-i-will-wont-i";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:974:"I&#8217;ve had a talk accepted for the MySQL Connect conference, in San Francisco at the end of September. I&#8217;ll be talking about Profiling with Performance Schema.
Here&#8217;s the abstract:
The Performance Schema feature was introduced in MySQL 5.5 and has been greatly enhanced within the coming 5.6 release. It exposes a new wealth of instrumentation, to DBAs and developers alike, that enable you to find answers to many of the questions that have been impossible, or more difficult than necessary, to answer in the past. Come to this session to learn how to set up and use Performance Schema to perform everyday profiling and performance monitoring tasks, such as finding problem queries; researching blocked hosts; profiling I/O usage; analyzing resource usage by schema, table, or user; or tracing a session to see exactly where it spends its time.
There is a lot of great content planned for MySQL Connect, it should be a great event! 
I hope to see you there!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 10:09:40 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1589:"<p>I&#8217;ve had a talk accepted for the <a href="http://www.oracle.com/mysqlconnect/index.html">MySQL Connect</a> conference, in San Francisco at the end of September. I&#8217;ll be talking about <strong>Profiling with Performance Schema</strong>.</p>
<p>Here&#8217;s the abstract:</p>
<p><em>The Performance Schema feature was introduced in MySQL 5.5 and has been greatly enhanced within the coming 5.6 release. It exposes a new wealth of instrumentation, to DBAs and developers alike, that enable you to find answers to many of the questions that have been impossible, or more difficult than necessary, to answer in the past. Come to this session to learn how to set up and use Performance Schema to perform everyday profiling and performance monitoring tasks, such as finding problem queries; researching blocked hosts; profiling I/O usage; analyzing resource usage by schema, table, or user; or tracing a session to see exactly where it spends its time.</em></p>
<p>There is a <a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=&amp;p=&amp;i(10942)=15982&amp;i(11425)=&amp;i(10053)=&amp;i(10048)=&amp;i(11404)=&amp;i(10089)=&amp;i(11488)=">lot of great content</a> planned for MySQL Connect, it should be a great event! </p>
<p>I hope to see you there!</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33749&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33749&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Mark Leith";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:37;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:13:"Comments Gone";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:59:"tag:blogger.com,1999:blog-13517334.post-4621423262044297356";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:55:"http://gilfster.blogspot.com/2012/07/comments-gone.html";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:681:"Well seeing as I'm going to open this blog up again I figured I better come in and have a little clean up. Checking the comments it was obvious that most of the 1000+ comments were some form of spam so I decided that I'd clear then all out, including the good ones, sorry about that but I didn't have the time to read each one and decide what was worth keeping.  It seems that the blog still gets a number of visitors even though it's not been updated in some considerable time. It's linked from a number of significant places and still rates reasonably well in Google.  In the next few days I'll be writing something about my new Raspberry Pi computer, including setting up MySQL.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 09:37:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1069:"Well seeing as I'm going to open this blog up again I figured I better come in and have a little clean up. Checking the comments it was obvious that most of the 1000+ comments were some form of spam so I decided that I'd clear then all out, including the good ones, sorry about that but I didn't have the time to read each one and decide what was worth keeping.  <br /><br />It seems that the blog still gets a number of visitors even though it's not been updated in some considerable time. It's linked from a number of significant places and still rates reasonably well in Google.  <br /><br />In the next few days I'll be writing something about my new Raspberry Pi computer, including setting up MySQL.<div><img width="1" height="1" src="https://blogger.googleusercontent.com/tracker/13517334-4621423262044297356?l=gilfster.blogspot.com" alt="" /></div><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33748&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33748&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:14:"Andrew Gilfrin";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:38;a:6:{s:4:"data";s:73:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:49:"Generate dummy test data for MySQL using routines";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:41:"http://kedar.nitty-witty.com/blog/?p=1790";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:83:"http://kedar.nitty-witty.com/blog/generate-dummy-test-data-for-mysql-using-routines";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:192:"At times you&#8217;ll find yourself responsible for generating test data for newly created tables for testing or sampling purpose. There are tools that will generate random data for you but...";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 07:51:20 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:7:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:4:"free";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:8:"Function";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:19:"generate dummy data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:18:"generate test data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:11:"random data";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:7:"routine";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:405:"At times you&#8217;ll find yourself responsible for generating test data for newly created tables for testing or sampling purpose. There are tools that will generate random data for you but...<br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33747&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33747&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:5:"Kedar";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:39;a:6:{s:4:"data";s:38:"
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:5:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:26:"MySQL Community Server 5.5";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:45:"http://dev.mysql.com/downloads/mysql/5.5.html";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:45:"http://dev.mysql.com/downloads/mysql/5.5.html";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:75:"MySQL Community Server 5.5 (5.5.25a GA, published on Thursday, 05 Jul 2012)";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Thu, 05 Jul 2012 00:00:00 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:288:"MySQL Community Server 5.5 (5.5.25a GA, published on Thursday, 05 Jul 2012)<br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=22657&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=22657&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:40;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:60:"Warning and error information in stored procedures revisited";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:78:"https://blogs.oracle.com/svetasmirnova/entry/warning_and_error_information_in1";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:78:"https://blogs.oracle.com/svetasmirnova/entry/warning_and_error_information_in1";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1700:"Originally way to handle warnings and errors in MySQL stored routine was designed as follows: 
   
    if warning was generated during stored routine execution which has a handler for such a warning/error, MySQL remembered the handler, ignored the warning and continued execution 
    after routine is executed MySQL checked if there is a remembered handler and activated if any 
  This logic was not ideal and causes several problems, particularly: 
   
    it was not possible to choose right handler for an instruction which generated several warnings or errors, because only first one was chosen 
    handling conditions in current scope messed with conditions in different 
    there were no generated warning/errors in Diagnostic Area that is against SQL Standard. 
  First try to fix this was done in version 5.5. Patch left Diagnostic Area intact after stored routine execution, but cleared it in the beginning of each statement which can generate warnings or to work with tables. Diagnostic Area checked after stored routine execution.This patch solved issue with order of condition handlers, but lead to new issues. Most popular was that outer stored routine could see warnings which should be already handled by handler inside inner stored routine, although latest has handler. I even had to wrote a blog post about it.And now I am happy to announce this behaviour changed third time.Since version 5.6 Diagnostic Area cleared after instruction leaves its handler.This lead to that only one handler will see condition it is supposed to proceed and in proper order. All past problems are solved.I am happy that my old blog post describing weird behaviour in version 5.5 is not true any more.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 12:25:53 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:2:"sp";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2220:"<p>Originally way to handle warnings and errors in MySQL stored routine was designed as follows:<br /> <br/>
  <ul> <br/>
    <li>if warning was generated during stored routine execution which has a handler for such a warning/error, MySQL remembered the handler, ignored the warning and continued execution</li> <br/>
    <li>after routine is executed MySQL checked if there is a remembered handler and activated if any<br /></li> <br/>
  </ul>This logic was not ideal and causes several problems, particularly:<br /> <br/>
  <ul> <br/>
    <li>it was not possible to choose right handler for an instruction which generated several warnings or errors, because only first one was chosen</li> <br/>
    <li>handling conditions in current scope messed with conditions in different</li> <br/>
    <li>there were no generated warning/errors in Diagnostic Area that is against SQL Standard.<br /></li> <br/>
  </ul>First try to fix this was done in version 5.5. Patch left Diagnostic Area intact after stored routine execution, but cleared it in the beginning of each statement which can generate warnings or to work with tables. Diagnostic Area checked after stored routine execution.<br /><br />This patch solved issue with order of condition handlers, but lead to new issues. Most popular was that outer stored routine could see warnings which should be already handled by handler inside inner stored routine, although latest has handler. I even had to wrote <a href="https://blogs.oracle.com/svetasmirnova/entry/warning_and_error_information_in" target="_blank">a blog post about it</a>.<br /><br />And now I am happy to announce this behaviour changed third time.<br /><br />Since version 5.6 Diagnostic Area cleared after instruction leaves its handler.<br /><br />This lead to that only one handler will see condition it is supposed to proceed and in proper order. All past problems are solved.<br /><br />I am happy that my old blog post describing weird behaviour in version 5.5 is not true any more.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33744&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33744&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:14:"Sveta Smirnova";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:41;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:19:"New ps_helper pages";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:34:"http://www.markleith.co.uk/?p=1035";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:129:"http://www.markleith.co.uk/2012/07/04/new-ps_helper-pages/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=new-ps_helper-pages";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:961:"I&#8217;ve been updating my (sadly very very neglected, and hacked) blog recently, and along with the new look and feel, have put together a new page for ps_helper (a schema of helper views and procedures for analyzing MySQL&#8217;s Performance Schema data):
http://www.markleith.co.uk/ps_helper/
I&#8217;ve also updated it with the things that I&#8217;ve talked about recently (including my last post on Statement Digests), and put together a couple of version specific scripts, that can be used against 5.5 or the new 5.6 versions.
Each view or procedure now has examples, and their source individually listed too, so that you can see the benefits that you may get from each.
I&#8217;ve also updated the dump_thread_stack() procedure to also list the new instruments within 5.6 (such as network IO and idle time), and given it a page of it&#8217;s own as well:
http://www.markleith.co.uk/ps_helper/ps_helper-dump_thread_stack/
Feedback (and requests) welcome!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 10:40:07 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:7:"General";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1585:"<p>I&#8217;ve been updating my (sadly very very neglected, and hacked) blog recently, and along with the new look and feel, have put together a new page for <strong>ps_helper</strong> (a schema of helper views and procedures for analyzing MySQL&#8217;s <a href="http://dev.mysql.com/doc/en/performance-schema.html">Performance Schema</a> data):</p>
<p><a href="http://www.markleith.co.uk/ps_helper/">http://www.markleith.co.uk/ps_helper/</a></p>
<p>I&#8217;ve also updated it with the things that I&#8217;ve talked about recently (including my last post on <a href="http://www.markleith.co.uk/2012/07/04/mysql-performance-schema-statement-digests/" title="MySQL Performance Schema Statement Digests">Statement Digests</a>), and put together a couple of version specific scripts, that can be used against 5.5 or the new 5.6 versions.</p>
<p>Each view or procedure now has examples, and their source individually listed too, so that you can see the benefits that you may get from each.</p>
<p>I&#8217;ve also updated the dump_thread_stack() procedure to also list the new instruments within 5.6 (such as network IO and idle time), and given it a page of it&#8217;s own as well:</p>
<p><a href="http://www.markleith.co.uk/ps_helper/ps_helper-dump_thread_stack/">http://www.markleith.co.uk/ps_helper/ps_helper-dump_thread_stack/</a></p>
<p>Feedback (and requests) welcome!</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33742&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33742&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Mark Leith";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:42;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:42:"MySQL Performance Schema Statement Digests";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:33:"http://www.markleith.co.uk/?p=813";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:175:"http://www.markleith.co.uk/2012/07/04/mysql-performance-schema-statement-digests/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=mysql-performance-schema-statement-digests";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:25717:"MySQL 5.6.5 was released recently, with many a great blog written by various developers involved in making it such an awesome release. The list is large, I&#8217;d recommend reading the change log. The MySQL 5.6 What Is New page provides a great overview of the 5.6 release so far as well.
The changelog includes this nugget, which I&#8217;ll expand upon here:

The Performance Schema now maintains statement digest information. This normalizes and groups statements with the same “signature” and permits questions to be answered about the types of statements the server is executing and how often they occur.

A statement_digest consumer in the setup_consumers table controls whether the Performance Schema maintains digest information.
The statement event tables (events_statements_current, events_statements_history, and events_statements_history_long) have DIGEST and DIGEST_TEXT columns that contain digest MD5 values and the corresponding normalized statement text strings.
A events_statements_summary_by_digest table provides aggregated statement digest information.


Many of you should be familiar with this view now:

It is the Query Analysis view from MySQL Enterprise Monitor. This was first released at the end of 2008, and was conceived (actually in 2006, presented and much discussed on a snowy day at a MEM team meeting in Amsterdam) from the need to easily monitor what statements are running within the database instance. 
Yes, there were the Slow Query Log and General Query Log, but they had their downfalls when we were evaluating this &#8211; not fine grained enough on the one side, not enough info with too much overhead on the other, and both require access at the OS level to read the log files &#8211; not always a problem, but certainly is in some cases. The only tool that did any kind of analysis and statistics aggregation on queries that MySQL had been executing, at the time, was mysqldumpslow. 
And so MySQL Proxy was born, and hooked up with MEM to collect statistics on queries that were funneled through it. It was revolutionary (for the MySQL world, at least). Hot on it&#8217;s heels we saw new GUI tools from the community trying to do the same thing. We&#8217;ve also seen extensions to the old ways of doing things as well with the extended slow query logging, and command line scripts such as mk-query-digest being born (and re-born). We then extended Query Analysis in to the Connectors as well, to share the load of statistics gathering across application servers, and not have to funnel everything through MySQL Proxy (among other benefits). 
These are all still great solutions today. Yet they all have one major downfall &#8211; they try and skirt the issue of the database instance &#8230; actually generating this data and having it available with a SQL interface. That&#8217;s what databases are supposed to be good at &#8230; right?
Now it&#8217;s 2012, and we finally have a solution for this &#8211; Performance Schema Statement Digests. So with the picture above in mind, now take a look at:

Strikingly similar, no? And it&#8217;s being returned by the database directly. It gets better.
One of the problems with trying to gather statistics on statement traffic from within the protocol stream (done by Proxy, Connectors, any tcpdump solution, etc.) is that there are certain statistics not exposed within the MySQL protocol &#8211; everything like the number of rows examined, temporary table usage, select type etc. that can be seen in a SHOW SESSION STATUS for example are practically impossible to correlate (we would have to inject that in to each connection to wrap each &#8220;real&#8221; statement).
The Slow Query Log does not have this problem &#8211; and the extensions there in the community have proved that it&#8217;s useful to be able to aggregate more at the statement level too. 
Statement Digests most certainly do not have this problem. Here&#8217;s the current structure of the new events_statements_summary_by_digest table:

mysql&gt; DESC performance_schema.events_statements_summary_by_digest;
+-----------------------------+---------------------+------+-----+---------------------+-------+
| Field                       | Type                | Null | Key | Default             | Extra |
+-----------------------------+---------------------+------+-----+---------------------+-------+
| DIGEST                      | varchar(32)         | YES  |     | NULL                |       |
| DIGEST_TEXT                 | longtext            | YES  |     | NULL                |       |
| COUNT_STAR                  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| MIN_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| AVG_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| MAX_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_LOCK_TIME               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ERRORS                  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_WARNINGS                | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_AFFECTED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_SENT               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_EXAMINED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_CREATED_TMP_DISK_TABLES | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_CREATED_TMP_TABLES      | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_FULL_JOIN        | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_FULL_RANGE_JOIN  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_RANGE            | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_RANGE_CHECK      | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_SCAN             | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_MERGE_PASSES       | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_RANGE              | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_ROWS               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_SCAN               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_NO_INDEX_USED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_NO_GOOD_INDEX_USED      | bigint(20) unsigned | NO   |     | NULL                |       |
| FIRST_SEEN                  | timestamp           | NO   |     | 0000-00-00 00:00:00 |       |
| LAST_SEEN                   | timestamp           | NO   |     | 0000-00-00 00:00:00 |       |
+-----------------------------+---------------------+------+-----+---------------------+-------+
28 rows in set (0.02 sec)

Possibilities abound!
Before diving in to some of the use cases, first it&#8217;s worth describing exactly what a digest is. 
As with Query Analysis in MEM, we track and aggregate statistics by normalizing the statements. Performance Schema does this in slightly different ways to how the MEM components do this, however they both follow roughly the same process, by working on a tokenized representation of the statement.
Performance Schema does this by tying in to the lexer within the MySQL server, which spits out the tokenized statement to be parsed, by recording it&#8217;s token output stream and doing some of it&#8217;s own normalization on top. 
A normalized statement may have any of the following done to it:

Stripping whitespace (done in the lexer layer)
Stripping comments (done in the lexer layer)
Replacing literals (integer and string inputs) with a &#8220;?&#8221; placeholder

SELECT foo, bar FROM foobar WHERE foo = 100; becomes SELECT foo, bar FROM foobar WHERE foo = ?
For a single value INSERT statement, INSERT INTO foobar VALUES (100); becomes INSERT INTO foobar VALUES (?); 

Lists of values are folded

Folding lists of IN values

SELECT foo FROM foobar WHERE bar IN (1, 2, 3) becomes SELECT foo FROM foobar WHERE bar IN (&#8230;)

Folding multi-row INSERTs with single values

INSERT INTO t1 VALUES (1), (2) becomes INSERT INTO t1 VALUES (?) /* , &#8230; */

Folding multi-row INSERTs with many values

INSERT INTO t1 VALUES (1, 2, 3), (4, 5, 6) becomes INSERT INTO t1 VALUES (&#8230;) /* , &#8230; */

Lists of values in the SELECT list

SELECT 1, 2, 3, foo, bar FROM foobar becomes SELECT ?, &#8230; , foo, bar FROM foobar



Long normalized statements are truncated, with &#8220;&#8230;&#8221; added at the end

 &#8220;long&#8221; is currently defined as 1024 bytes, set by PSI_MAX_DIGEST_STORAGE_SIZE in psi.h, unfortunately this is not configurable yet, however it should also be noted that this is on the normalized statement &#8211; so large string values etc. are not a concern here



MEM does not fold lists of values down, i.e. &#8220;INSERT INTO foobar VALUES (1,2,3,4,5)&#8221; will be normalized to &#8220;INSERT INTO foobar VALUES (?,?,?,?,?)&#8221; within MEM. Performance Schema does this to save memory, by discarding the value tokens along the way and using the above forms for normalization.
With that description out of the way, now let&#8217;s take a look at some the value that you can get from the data!
First, let&#8217;s get the obvious out the way:
A high level overview of the statements like Query Analysis, sorted by those queries with the highest latency

SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       IF(SUM_NO_GOOD_INDEX_USED &gt; 0 OR SUM_NO_INDEX_USED &gt; 0, &#039;*&#039;, &#039;&#039;) AS full_scan,
       COUNT_STAR AS exec_count,
       SUM_ERRORS AS err_count,
       SUM_WARNINGS AS warn_count,
       SEC_TO_TIME(SUM_TIMER_WAIT/1000000000000) AS exec_time_total,
       SEC_TO_TIME(MAX_TIMER_WAIT/1000000000000) AS exec_time_max,
       (AVG_TIMER_WAIT/1000000000) AS exec_time_avg_ms,
       SUM_ROWS_SENT AS rows_sent,
       ROUND(SUM_ROWS_SENT / COUNT_STAR) AS rows_sent_avg,
       SUM_ROWS_EXAMINED AS rows_scanned,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC LIMIT 5;

+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+
| query                                                             | full_scan | exec_count | err_count | warn_count | exec_time_total | exec_time_max | exec_time_avg_ms | rows_total | rows_avg | rows_scanned | digest                           |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+
| SELECT `mysqlserve0_` . `hid`  ... ` . `performanceSchema` AS ... | *         |       7747 |         0 |          0 | 00:00:02.5492   | 00:00:00.0009 |           0.3288 |       5200 |        1 |         5200 | 3176fe79ae8ce631eb328feaaafcf972 |
| SELECT `agent0_` . `hid` AS `h ... ent0_` . `id` = ? FOR UPDATE   |           |       7706 |         0 |          0 | 00:00:02.4067   | 00:00:00.0008 |           0.3122 |       7706 |        1 |         7706 | cbcf9ed706ce0974f1ef91b7c2d690eb |
| SELECT `os0_` . `hid` AS `hid1 ... ion` AS `hasVersion121_` , ... | *         |       5098 |         0 |          0 | 00:00:02.2526   | 00:00:00.0047 |           0.4416 |       5098 |        1 |         5098 | 1677026fd320b4876261b1270d28be38 |
| SELECT `replicatio0_` . `hid`  ... ess` AS `hasMast18_107_` , ... | *         |       2550 |         0 |          0 | 00:00:02.0543   | 00:00:00.0016 |           0.8055 |       2550 |        1 |         2550 | 90b8d082e28985953ff558546e00eb33 |
| SELECT `environmen0_` . `hid`  ... ck130_` , `environmen0_` . ... | *         |       2550 |         0 |          0 | 00:00:01.6082   | 00:00:00.0047 |           0.6304 |       2550 |        1 |         2550 | f0ba68f44b6d9357f1cff9670dc4ff2a |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+

My test data is a little bland (this is a MEM schema monitoring a single MySQL instance), but you get the idea. You can instantly get a picture of your top statements, ordered in this case by latency, along with some extra statistics like whether it caused a full scan, their average latency, how many rows were both scanned and returned etc.
But you could slice and dice this data any way you want &#8211; it&#8217;s just SQL.
Here&#8217;s a few more use cases:
List all normalized statements that use temporary tables ordered by number of on disk temporary tables descending first, then by the number of memory tables.

SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_CREATED_TMP_TABLES AS memory_tmp_tables,
       SUM_CREATED_TMP_DISK_TABLES AS disk_tmp_tables,
       ROUND(SUM_CREATED_TMP_TABLES / COUNT_STAR) AS avg_tmp_tables_per_query,
       ROUND((SUM_CREATED_TMP_DISK_TABLES / SUM_CREATED_TMP_TABLES) * 100) AS tmp_tables_to_disk_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_CREATED_TMP_TABLES &gt; 0
ORDER BY SUM_CREATED_TMP_DISK_TABLES DESC, SUM_CREATED_TMP_TABLES DESC LIMIT 5;

+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+
| query                                                             | exec_count | memory_tmp_tables | disk_tmp_tables | avg_tmp_tables_per_query | tmp_tables_to_disk_pct |
+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+
| SELECT DISTINCTROW `hibalarm0_ ... testeval2_` . `alarm_id` = ... |          5 |                15 |               5 |                        3 |                     33 |
| SELECT DISTINCTROW `hibalarm0_ ... testeval2_` . `alarm_id` = ... |          2 |                 6 |               2 |                        3 |                     33 |
| SELECT * FROM latest_file_io                                      |          2 |                 4 |               2 |                        2 |                     50 |
| SELECT * FROM PROCESSLIST                                         |          2 |                 2 |               2 |                        1 |                    100 |
| SELECT SQL_CALC_FOUND_ROWS `st ...  , MIN ( `min_exec_time` ) ... |          1 |                 3 |               1 |                        3 |                     33 |
+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+

List all normalized statements that have done sorts, ordered by sort_merge_passes, sort_scans and sort_rows, all descending.

SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_SORT_MERGE_PASSES AS sort_merge_passes,
       ROUND(SUM_SORT_MERGE_PASSES / COUNT_STAR) AS avg_sort_merges,
       SUM_SORT_SCAN AS sorts_using_scans,
       SUM_SORT_RANGE AS sort_using_range,
       SUM_SORT_ROWS AS rows_sorted,
       ROUND(SUM_SORT_ROWS / COUNT_STAR) AS avg_rows_sorted,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_SORT_ROWS &gt; 0
 ORDER BY SUM_SORT_MERGE_PASSES DESC, SUM_SORT_SCAN DESC, SUM_SORT_ROWS DESC LIMIT 5;

+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+
| query                                                             | exec_count | sort_merge_passes | avg_sort_merges | sorts_using_scans | sort_using_range | rows_sorted | avg_rows_sorted | digest                           |
+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+
| SELECT * FROM ps_helper . statements_with_sorting                 |          7 |                 0 |               0 |                 7 |                0 |          31 |               4 | 635d19e3e652972b3267ada0bf9c7b36 |
| SELECT * FROM statement_analysis                                  |          4 |                 0 |               0 |                 4 |                0 |          89 |              22 | 10f918a1a410f4fa0fc2602cff02deb7 |
| SELECT table_schema , SUM ( da ... tables GROUP BY table_schema   |          2 |                 0 |               0 |                 2 |                0 |          24 |              12 | 27fecd44f0bf5c0fc4e46f547083a09d |
| SELECT * FROM statements_with_sorting                             |          2 |                 0 |               0 |                 2 |                0 |           3 |               2 | dc117dd0eb81394322e3d4144a997ffc |
+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+

List all normalized statements that use have done a full table scan ordered by the percentage of times a full scan was done, then by the number of times the statement executed

SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_NO_INDEX_USED AS no_index_used_count,
       SUM_NO_GOOD_INDEX_USED AS no_good_index_used_count,
       ROUND((SUM_NO_INDEX_USED / COUNT_STAR) * 100) no_index_used_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_NO_INDEX_USED &gt; 0
    OR SUM_NO_GOOD_INDEX_USED &gt; 0
ORDER BY no_index_used_pct DESC, exec_count DESC LIMIT 5;

+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+
| query                                                             | exec_count | no_index_used_count | no_good_index_used_count | no_index_used_pct |
+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+
| SELECT `hibadvisor0_` . `sched ... _` . `advisorClassId` IN (?)   |        679 |                 679 |                        0 |               100 |
| SELECT `hibalarm0_` . `alarm_i ... ` . `fixed_time` &lt; ? LIMIT ?   |        365 |                 365 |                        0 |               100 |
| SELECT `id` , `millis_stamp` , ... s` ORDER BY `id` ASC LIMIT ?   |        353 |                 353 |                        0 |               100 |
| SELECT `agent0_` . `hid` AS `h ... ventory` . `Agent` `agent0_`   |        112 |                 112 |                        0 |               100 |
| SELECT COUNT ( * ) AS `col_0_0 ... entry0_` . `fetchedDate` &gt; ?   |         18 |                  18 |                        0 |               100 |
+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+

List all normalized statements that have raised errors or warnings.

SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_ERRORS AS errors,
       (SUM_ERRORS / COUNT_STAR) * 100 as error_pct,
       SUM_WARNINGS AS warnings,
       (SUM_WARNINGS / COUNT_STAR) * 100 as warning_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_ERRORS &gt; 0
    OR SUM_WARNINGS &gt; 0
ORDER BY SUM_ERRORS DESC, SUM_WARNINGS DESC;

+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+
| query                                                             | exec_count | errors | error_pct | warnings | warning_pct | digest                           |
+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+
| CREATE PROCEDURE currently_ena ... w_instruments BOOLEAN DEFAULT  |          2 |      2 |  100.0000 |        0 |      0.0000 | ad6024cfc2db562ae268b25e65ef27c0 |
| CREATE PROCEDURE currently_ena ... ents WHERE enabled = ? ; END   |          2 |      1 |   50.0000 |        0 |      0.0000 | 4aac3ab9521a432ff03313a69cfcc58f |
| CREATE PROCEDURE currently_enabled ( BOOLEAN show_instruments     |          1 |      1 |  100.0000 |        0 |      0.0000 | c6df6711da3d1a26bc136dc8b354f6eb |
| CREATE PROCEDURE disable_backg ... d = ? WHERE TYPE = ? ; END IF  |          1 |      1 |  100.0000 |        0 |      0.0000 | 12e0392402780424c736c9555bcc9703 |
| DROP PROCEDURE IF EXISTS currently_enabled                        |         12 |      0 |    0.0000 |        6 |     50.0000 | 44cc7e655d08f430e0dd8f3110ed816c |
| DROP PROCEDURE IF EXISTS disable_background_threads               |          3 |      0 |    0.0000 |        2 |     66.6667 | 0153b7158dae80672bda6181c73f172c |
| CREATE SCHEMA IF NOT EXISTS ps_helper                             |          2 |      0 |    0.0000 |        1 |     50.0000 | a12cabd32d1507c758c71478075f5290 |
+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+

All of these greatly show the flexibility of having this instrumentation directly within the server, accessible via the server itself. If you haven&#8217;t done it already now is the time to take a little breath, and then shout Hurrah.
Now, it&#8217;s worth noting that all of this is stored in memory. By default, the maximum number of rows in this table in 5.6.5 is 200, which effectively lets you monitor 199 normalized statements &#8211; as there is a row reserved for a NULL digest, which is a catch all for once this number of normalized statements have been generated &#8211; Performance Schema then stores the aggregated statistics for all other statement traffic whose statement digests do not match those already within the table within this NULL row.
In the MEM team we&#8217;ve found in practice that people often have many more than this &#8211; often 500+ different kinds of statements being executed. You can increase the number of digests that are tracked with the performance_schema_digests_size system variable. You can track how much memory this uses with the SHOW ENGINE PERFORMANCE_SCHEMA STATUS command (here I have the number of digests to track set to 400, which takes roughly 490KB of memory):

mysql&gt; show engine performance_schema status;
+--------------------+--------------------------------------------------------------+-----------+
| Type               | Name                                                         | Status    |
+--------------------+--------------------------------------------------------------+-----------+
...
| performance_schema | events_statements_summary_by_digest.row_size                 | 1256      |
| performance_schema | events_statements_summary_by_digest.row_count                | 400       |
| performance_schema | events_statements_summary_by_digest.memory                   | 502400    |
...

This limit has been raised to 1000 within the current development version (5.6.6), along with Performance Schema (and the instrumentation I have talked about here) being enabled by default!
As with other aggregate tables within Performance Schema, you can reset the statistics within the digest table with:

TRUNCATE TABLE performance_schema.events_statements_summary_by_digest;

Any scripts or tools that are used to analyze this table will need to take this in to account. 
We&#8217;re just scratching the surface of possibilities with this table at the moment. I have a number of things that I&#8217;d like to see added (like.. a summary of each of the major classes of lower wait events, if enabled, like &#8220;SUM_FILE_IO_WAIT&#8221;, &#8220;SUM_SOCKET_IO_WAIT&#8221;, &#8220;SUM_MUTEX_WAIT&#8221; etc.), and I&#8217;m sure many of you out there in the community will have your own ideas as well. 
Welcome to the future of statement diagnostics within MySQL, we hope you enjoy it! Tell us what you&#8217;d like to see next!";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 10:23:14 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:18:"performance_schema";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:17:"statement digests";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:29316:"<p><a href="http://dev.mysql.com/doc/refman/5.6/en/news-5-6-5.html">MySQL 5.6.5 was released recently</a>, with <a href="http://d2-systems.blogspot.com/2012/04/global-transaction-identifiers-are-in.html" target="_blank">many</a> a <a href="http://dimitrik.free.fr/blog/archives/04-01-2012_04-30-2012.html#141">great</a> <a href="http://guilhembichot.blogspot.co.uk/2012/04/faster-subqueries-with-materialization.html">blog</a> <a href="http://marcalff.blogspot.co.uk/2012/04/performance-schema-nailing-host-cache.html">written</a> by <a href="http://jorgenloland.blogspot.co.uk/2012/04/on-queries-with-many-values-in-in.html">various</a> <a href="http://blogs.innodb.com/wp/2012/04/new-flushing-algorithm-in-innodb/">developers</a> <a href="http://blogs.innodb.com/wp/2012/04/memcached-with-sasl-support/">involved</a> in making it such an awesome release. The list is large, I&#8217;d recommend reading the change log. The <a href="http://dev.mysql.com/doc/refman/5.6/en/mysql-nutshell.html">MySQL 5.6 What Is New</a> page provides a great overview of the 5.6 release so far as well.</p>
<p>The changelog includes this nugget, which I&#8217;ll expand upon here:<br />
<em><br />
The Performance Schema now maintains statement digest information. This normalizes and groups statements with the same “signature” and permits questions to be answered about the types of statements the server is executing and how often they occur.</p>
<ul>
<li>A statement_digest consumer in the setup_consumers table controls whether the Performance Schema maintains digest information.</li>
<li>The statement event tables (events_statements_current, events_statements_history, and events_statements_history_long) have DIGEST and DIGEST_TEXT columns that contain digest MD5 values and the corresponding normalized statement text strings.</li>
<li>A events_statements_summary_by_digest table provides aggregated statement digest information.</li>
</ul>
<p></em></p>
<p>Many of you should be familiar with this view now:</p>
<p><img alt="" src="http://img.skitch.com/20120430-rnpt12s9mxqfgys5ypaxfitq73.png" title="MEM Query Analysis" class="aligncenter" width="1247" height="597" /></p>
<p>It is the <a href="http://www.mysql.com/products/enterprise/query.html" target="_blank">Query Analysis</a> view from <a href="http://www.mysql.com/products/enterprise/monitor.html" target="_blank">MySQL Enterprise Monitor</a>. This was first released at the end of 2008, and was conceived (actually in 2006, presented and much discussed on a snowy day at a MEM team meeting in Amsterdam) from the need to easily monitor what statements are running within the database instance. </p>
<p>Yes, there were the <em>Slow Query Log</em> and <em>General Query Log</em>, but they had their downfalls when we were evaluating this &#8211; not fine grained enough on the one side, not enough info with too much overhead on the other, and both require access at the OS level to read the log files &#8211; not always a problem, but certainly is in some cases. The only tool that did any kind of analysis and statistics aggregation on queries that MySQL had been executing, at the time, was <a href="http://dev.mysql.com/doc/refman/5.1/en/mysqldumpslow.html">mysqldumpslow</a>. </p>
<p>And so <a href="http://dev.mysql.com/doc/refman/5.5/en/mysql-proxy.html">MySQL Proxy</a> was born, and hooked up with MEM to collect statistics on queries that were funneled through it. It was revolutionary (for the MySQL world, at least). Hot on it&#8217;s heels we saw new GUI tools from the community trying to do the same thing. We&#8217;ve also seen extensions to the old ways of doing things as well with the extended slow query logging, and command line scripts such as <a href="http://www.maatkit.org/doc/mk-query-digest.html">mk-query-digest</a> being born (and <a href="http://www.percona.com/doc/percona-toolkit/2.1/pt-query-digest.html">re-born</a>). We then <a href="http://dev.mysql.com/doc/mysql-monitor/2.3/en/mem-qanal-using-feeding.html">extended Query Analysis in to the Connectors</a> as well, to share the load of statistics gathering across application servers, and not have to funnel everything through MySQL Proxy (among other benefits). </p>
<p>These are all still great solutions today. Yet they all have one major downfall &#8211; they try and skirt the issue of the database instance &#8230; <strong>actually generating this data and having it available with a SQL interface.</strong> That&#8217;s what databases are supposed to be good at &#8230; right?</p>
<p>Now it&#8217;s 2012, and we finally have a solution for this &#8211; <a href="http://dev.mysql.com/doc/refman/5.6/en/performance-schema-statement-digests.html">Performance Schema Statement Digests</a>. So with the picture above in mind, now take a look at:</p>
<p><img alt="" src="http://img.skitch.com/20120430-x6cj9j6c77a98a8gp2nwe3a1hq.png" title="Performance Schema Query Analysis" class="alignnone" width="1869" height="870" /></p>
<p><em>Strikingly similar, no? And it&#8217;s being returned by the database directly.</em> <strong>It gets better.</strong></p>
<p>One of the problems with trying to gather statistics on statement traffic from within the protocol stream (done by Proxy, Connectors, any tcpdump solution, etc.) is that there are certain statistics not exposed within the MySQL protocol &#8211; everything like the number of rows examined, temporary table usage, select type etc. that can be seen in a SHOW SESSION STATUS for example are practically impossible to correlate (we would have to inject that in to each connection to wrap each &#8220;real&#8221; statement).</p>
<p>The <em>Slow Query Log</em> does not have this problem &#8211; and the extensions there in the community have proved that it&#8217;s useful to be able to aggregate more at the statement level too. </p>
<p><strong>Statement Digests</strong> most certainly do not have this problem. Here&#8217;s the current structure of the new <strong>events_statements_summary_by_digest</strong> table:</p>
<pre>
mysql&gt; DESC performance_schema.events_statements_summary_by_digest;
+-----------------------------+---------------------+------+-----+---------------------+-------+
| Field                       | Type                | Null | Key | Default             | Extra |
+-----------------------------+---------------------+------+-----+---------------------+-------+
| DIGEST                      | varchar(32)         | YES  |     | NULL                |       |
| DIGEST_TEXT                 | longtext            | YES  |     | NULL                |       |
| COUNT_STAR                  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| MIN_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| AVG_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| MAX_TIMER_WAIT              | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_LOCK_TIME               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ERRORS                  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_WARNINGS                | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_AFFECTED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_SENT               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_ROWS_EXAMINED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_CREATED_TMP_DISK_TABLES | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_CREATED_TMP_TABLES      | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_FULL_JOIN        | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_FULL_RANGE_JOIN  | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_RANGE            | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_RANGE_CHECK      | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SELECT_SCAN             | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_MERGE_PASSES       | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_RANGE              | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_ROWS               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_SORT_SCAN               | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_NO_INDEX_USED           | bigint(20) unsigned | NO   |     | NULL                |       |
| SUM_NO_GOOD_INDEX_USED      | bigint(20) unsigned | NO   |     | NULL                |       |
| FIRST_SEEN                  | timestamp           | NO   |     | 0000-00-00 00:00:00 |       |
| LAST_SEEN                   | timestamp           | NO   |     | 0000-00-00 00:00:00 |       |
+-----------------------------+---------------------+------+-----+---------------------+-------+
28 rows in set (0.02 sec)
</pre>
<p><strong>Possibilities abound!</strong></p>
<p>Before diving in to some of the use cases, first it&#8217;s worth describing exactly what a <strong>digest</strong> is. </p>
<p>As with Query Analysis in MEM, we track and aggregate statistics by <em>normalizing</em> the statements. Performance Schema does this in slightly different ways to how the MEM components do this, however they both follow roughly the same process, by working on a tokenized representation of the statement.</p>
<p>Performance Schema does this by tying in to the <a href="http://en.wikipedia.org/wiki/Lexical_analysis">lexer</a> within the MySQL server, which spits out the tokenized statement to be parsed, by recording it&#8217;s token output stream and doing some of it&#8217;s own normalization on top. </p>
<p>A normalized statement may have any of the following done to it:</p>
<ul>
<li>Stripping whitespace (done in the lexer layer)</li>
<li>Stripping comments (done in the lexer layer)</li>
<li>Replacing literals (integer and string inputs) with a &#8220;?&#8221; placeholder</li>
<ul>
<li><em>SELECT foo, bar FROM foobar WHERE foo = <strong>100</strong></em>; becomes <em>SELECT foo, bar FROM foobar WHERE foo = <strong>?</strong></em></li>
<li>For a single value INSERT statement, <em>INSERT INTO foobar VALUES (<strong>100</strong>);</em> becomes <em>INSERT INTO foobar VALUES (<strong>?</strong>);</em> </li>
</ul>
<li>Lists of values are folded
<ul>
<li>Folding lists of IN values
<ul>
<li><em>SELECT foo FROM foobar WHERE bar IN (<strong>1, 2, 3</strong>)</em> becomes <em>SELECT foo FROM foobar WHERE bar IN (<strong>&#8230;</strong>)</em></li>
</ul>
<li>Folding multi-row INSERTs with single values
<ul>
<li><em>INSERT INTO t1 VALUES <strong>(1), (2)</strong></em> becomes <em>INSERT INTO t1 VALUES <strong>(?) /* , &#8230; */</strong></em></li>
</ul>
<li>Folding multi-row INSERTs with many values
<ul>
<li><em>INSERT INTO t1 VALUES <strong>(1, 2, 3), (4, 5, 6)</strong></em> becomes <em>INSERT INTO t1 VALUES <strong>(&#8230;) /* , &#8230; */</strong></em></li>
</ul>
<li>Lists of values in the SELECT list</li>
<ul>
<li><em>SELECT <strong>1, 2, 3,</strong> foo, bar FROM foobar</em> becomes <em>SELECT <strong>?, &#8230; ,</strong> foo, bar FROM foobar</em></li>
</ul>
</ul>
</li>
<li>Long normalized statements are truncated, with &#8220;<strong>&#8230;</strong>&#8221; added at the end
<ul>
<li> &#8220;long&#8221; is currently defined as 1024 bytes, set by <em>PSI_MAX_DIGEST_STORAGE_SIZE</em> in psi.h, unfortunately this is not configurable yet, however it should also be noted that this is on the normalized statement &#8211; so large string values etc. are not a concern here</li>
</ul>
</li>
</ul>
<p>MEM does not fold lists of values down, i.e. &#8220;INSERT INTO foobar VALUES (1,2,3,4,5)&#8221; will be normalized to &#8220;INSERT INTO foobar VALUES (?,?,?,?,?)&#8221; within MEM. Performance Schema does this to save memory, by discarding the value tokens along the way and using the above forms for normalization.</p>
<p>With that description out of the way, now let&#8217;s take a look at some the value that you can get from the data!</p>
<p>First, let&#8217;s get the obvious out the way:</p>
<p><strong>A high level overview of the statements like Query Analysis, sorted by those queries with the highest latency</strong></p>
<pre>
SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       IF(SUM_NO_GOOD_INDEX_USED &gt; 0 OR SUM_NO_INDEX_USED &gt; 0, &#039;*&#039;, &#039;&#039;) AS full_scan,
       COUNT_STAR AS exec_count,
       SUM_ERRORS AS err_count,
       SUM_WARNINGS AS warn_count,
       SEC_TO_TIME(SUM_TIMER_WAIT/1000000000000) AS exec_time_total,
       SEC_TO_TIME(MAX_TIMER_WAIT/1000000000000) AS exec_time_max,
       (AVG_TIMER_WAIT/1000000000) AS exec_time_avg_ms,
       SUM_ROWS_SENT AS rows_sent,
       ROUND(SUM_ROWS_SENT / COUNT_STAR) AS rows_sent_avg,
       SUM_ROWS_EXAMINED AS rows_scanned,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
ORDER BY SUM_TIMER_WAIT DESC LIMIT 5;

+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+
| query                                                             | full_scan | exec_count | err_count | warn_count | exec_time_total | exec_time_max | exec_time_avg_ms | rows_total | rows_avg | rows_scanned | digest                           |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+
| SELECT `mysqlserve0_` . `hid`  ... ` . `performanceSchema` AS ... | *         |       7747 |         0 |          0 | 00:00:02.5492   | 00:00:00.0009 |           0.3288 |       5200 |        1 |         5200 | 3176fe79ae8ce631eb328feaaafcf972 |
| SELECT `agent0_` . `hid` AS `h ... ent0_` . `id` = ? FOR UPDATE   |           |       7706 |         0 |          0 | 00:00:02.4067   | 00:00:00.0008 |           0.3122 |       7706 |        1 |         7706 | cbcf9ed706ce0974f1ef91b7c2d690eb |
| SELECT `os0_` . `hid` AS `hid1 ... ion` AS `hasVersion121_` , ... | *         |       5098 |         0 |          0 | 00:00:02.2526   | 00:00:00.0047 |           0.4416 |       5098 |        1 |         5098 | 1677026fd320b4876261b1270d28be38 |
| SELECT `replicatio0_` . `hid`  ... ess` AS `hasMast18_107_` , ... | *         |       2550 |         0 |          0 | 00:00:02.0543   | 00:00:00.0016 |           0.8055 |       2550 |        1 |         2550 | 90b8d082e28985953ff558546e00eb33 |
| SELECT `environmen0_` . `hid`  ... ck130_` , `environmen0_` . ... | *         |       2550 |         0 |          0 | 00:00:01.6082   | 00:00:00.0047 |           0.6304 |       2550 |        1 |         2550 | f0ba68f44b6d9357f1cff9670dc4ff2a |
+-------------------------------------------------------------------+-----------+------------+-----------+------------+-----------------+---------------+------------------+------------+----------+--------------+----------------------------------+
</pre>
<p>My test data is a little bland (this is a MEM schema monitoring a single MySQL instance), but you get the idea. You can instantly get a picture of your top statements, ordered in this case by latency, along with some extra statistics like whether it caused a full scan, their average latency, how many rows were both scanned and returned etc.</p>
<p>But you could slice and dice this data any way you want &#8211; <strong>it&#8217;s just SQL.</strong></p>
<p>Here&#8217;s a few more use cases:</p>
<p><strong>List all normalized statements that use temporary tables ordered by number of on disk temporary tables descending first, then by the number of memory tables.</strong></p>
<pre>
SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_CREATED_TMP_TABLES AS memory_tmp_tables,
       SUM_CREATED_TMP_DISK_TABLES AS disk_tmp_tables,
       ROUND(SUM_CREATED_TMP_TABLES / COUNT_STAR) AS avg_tmp_tables_per_query,
       ROUND((SUM_CREATED_TMP_DISK_TABLES / SUM_CREATED_TMP_TABLES) * 100) AS tmp_tables_to_disk_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_CREATED_TMP_TABLES &gt; 0
ORDER BY SUM_CREATED_TMP_DISK_TABLES DESC, SUM_CREATED_TMP_TABLES DESC LIMIT 5;

+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+
| query                                                             | exec_count | memory_tmp_tables | disk_tmp_tables | avg_tmp_tables_per_query | tmp_tables_to_disk_pct |
+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+
| SELECT DISTINCTROW `hibalarm0_ ... testeval2_` . `alarm_id` = ... |          5 |                15 |               5 |                        3 |                     33 |
| SELECT DISTINCTROW `hibalarm0_ ... testeval2_` . `alarm_id` = ... |          2 |                 6 |               2 |                        3 |                     33 |
| SELECT * FROM latest_file_io                                      |          2 |                 4 |               2 |                        2 |                     50 |
| SELECT * FROM PROCESSLIST                                         |          2 |                 2 |               2 |                        1 |                    100 |
| SELECT SQL_CALC_FOUND_ROWS `st ...  , MIN ( `min_exec_time` ) ... |          1 |                 3 |               1 |                        3 |                     33 |
+-------------------------------------------------------------------+------------+-------------------+-----------------+--------------------------+------------------------+
</pre>
<p><strong>List all normalized statements that have done sorts, ordered by sort_merge_passes, sort_scans and sort_rows, all descending.</strong></p>
<pre>
SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_SORT_MERGE_PASSES AS sort_merge_passes,
       ROUND(SUM_SORT_MERGE_PASSES / COUNT_STAR) AS avg_sort_merges,
       SUM_SORT_SCAN AS sorts_using_scans,
       SUM_SORT_RANGE AS sort_using_range,
       SUM_SORT_ROWS AS rows_sorted,
       ROUND(SUM_SORT_ROWS / COUNT_STAR) AS avg_rows_sorted,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_SORT_ROWS &gt; 0
 ORDER BY SUM_SORT_MERGE_PASSES DESC, SUM_SORT_SCAN DESC, SUM_SORT_ROWS DESC LIMIT 5;

+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+
| query                                                             | exec_count | sort_merge_passes | avg_sort_merges | sorts_using_scans | sort_using_range | rows_sorted | avg_rows_sorted | digest                           |
+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+
| SELECT * FROM ps_helper . statements_with_sorting                 |          7 |                 0 |               0 |                 7 |                0 |          31 |               4 | 635d19e3e652972b3267ada0bf9c7b36 |
| SELECT * FROM statement_analysis                                  |          4 |                 0 |               0 |                 4 |                0 |          89 |              22 | 10f918a1a410f4fa0fc2602cff02deb7 |
| SELECT table_schema , SUM ( da ... tables GROUP BY table_schema   |          2 |                 0 |               0 |                 2 |                0 |          24 |              12 | 27fecd44f0bf5c0fc4e46f547083a09d |
| SELECT * FROM statements_with_sorting                             |          2 |                 0 |               0 |                 2 |                0 |           3 |               2 | dc117dd0eb81394322e3d4144a997ffc |
+-------------------------------------------------------------------+------------+-------------------+-----------------+-------------------+------------------+-------------+-----------------+----------------------------------+
</pre>
<p><strong>List all normalized statements that use have done a full table scan ordered by the percentage of times a full scan was done, then by the number of times the statement executed</strong></p>
<pre>
SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_NO_INDEX_USED AS no_index_used_count,
       SUM_NO_GOOD_INDEX_USED AS no_good_index_used_count,
       ROUND((SUM_NO_INDEX_USED / COUNT_STAR) * 100) no_index_used_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_NO_INDEX_USED &gt; 0
    OR SUM_NO_GOOD_INDEX_USED &gt; 0
ORDER BY no_index_used_pct DESC, exec_count DESC LIMIT 5;

+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+
| query                                                             | exec_count | no_index_used_count | no_good_index_used_count | no_index_used_pct |
+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+
| SELECT `hibadvisor0_` . `sched ... _` . `advisorClassId` IN (?)   |        679 |                 679 |                        0 |               100 |
| SELECT `hibalarm0_` . `alarm_i ... ` . `fixed_time` &lt; ? LIMIT ?   |        365 |                 365 |                        0 |               100 |
| SELECT `id` , `millis_stamp` , ... s` ORDER BY `id` ASC LIMIT ?   |        353 |                 353 |                        0 |               100 |
| SELECT `agent0_` . `hid` AS `h ... ventory` . `Agent` `agent0_`   |        112 |                 112 |                        0 |               100 |
| SELECT COUNT ( * ) AS `col_0_0 ... entry0_` . `fetchedDate` &gt; ?   |         18 |                  18 |                        0 |               100 |
+-------------------------------------------------------------------+------------+---------------------+--------------------------+-------------------+
</pre>
<p><strong>List all normalized statements that have raised errors or warnings.</strong></p>
<pre>
SELECT IF(LENGTH(DIGEST_TEXT) &gt; 64, CONCAT(LEFT(DIGEST_TEXT, 30), &#039; ... &#039;, RIGHT(DIGEST_TEXT, 30)), DIGEST_TEXT) AS query,
       COUNT_STAR AS exec_count,
       SUM_ERRORS AS errors,
       (SUM_ERRORS / COUNT_STAR) * 100 as error_pct,
       SUM_WARNINGS AS warnings,
       (SUM_WARNINGS / COUNT_STAR) * 100 as warning_pct,
       DIGEST AS digest
  FROM performance_schema.events_statements_summary_by_digest
 WHERE SUM_ERRORS &gt; 0
    OR SUM_WARNINGS &gt; 0
ORDER BY SUM_ERRORS DESC, SUM_WARNINGS DESC;

+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+
| query                                                             | exec_count | errors | error_pct | warnings | warning_pct | digest                           |
+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+
| CREATE PROCEDURE currently_ena ... w_instruments BOOLEAN DEFAULT  |          2 |      2 |  100.0000 |        0 |      0.0000 | ad6024cfc2db562ae268b25e65ef27c0 |
| CREATE PROCEDURE currently_ena ... ents WHERE enabled = ? ; END   |          2 |      1 |   50.0000 |        0 |      0.0000 | 4aac3ab9521a432ff03313a69cfcc58f |
| CREATE PROCEDURE currently_enabled ( BOOLEAN show_instruments     |          1 |      1 |  100.0000 |        0 |      0.0000 | c6df6711da3d1a26bc136dc8b354f6eb |
| CREATE PROCEDURE disable_backg ... d = ? WHERE TYPE = ? ; END IF  |          1 |      1 |  100.0000 |        0 |      0.0000 | 12e0392402780424c736c9555bcc9703 |
| DROP PROCEDURE IF EXISTS currently_enabled                        |         12 |      0 |    0.0000 |        6 |     50.0000 | 44cc7e655d08f430e0dd8f3110ed816c |
| DROP PROCEDURE IF EXISTS disable_background_threads               |          3 |      0 |    0.0000 |        2 |     66.6667 | 0153b7158dae80672bda6181c73f172c |
| CREATE SCHEMA IF NOT EXISTS ps_helper                             |          2 |      0 |    0.0000 |        1 |     50.0000 | a12cabd32d1507c758c71478075f5290 |
+-------------------------------------------------------------------+------------+--------+-----------+----------+-------------+----------------------------------+
</pre>
<p>All of these greatly show the flexibility of having this instrumentation directly within the server, accessible via the server itself. If you haven&#8217;t done it already now is the time to take a little breath, and then shout <strong>Hurrah</strong>.</p>
<p>Now, it&#8217;s worth noting that all of this is stored in memory. By default, the maximum number of rows in this table in 5.6.5 is 200, which effectively lets you monitor 199 normalized statements &#8211; as there is a row reserved for a <em>NULL</em> digest, which is a catch all for once this number of normalized statements have been generated &#8211; Performance Schema then stores the aggregated statistics for all other statement traffic whose statement digests do not match those already within the table within this NULL row.</p>
<p>In the MEM team we&#8217;ve found in practice that people often have many more than this &#8211; often 500+ different kinds of statements being executed. You can increase the number of digests that are tracked with the <a href="http://dev.mysql.com/doc/refman/5.6/en/performance-schema-system-variables.html#sysvar_performance_schema_digest_size">performance_schema_digests_size</a> system variable. You can track how much memory this uses with the <strong>SHOW ENGINE PERFORMANCE_SCHEMA STATUS</strong> command (here I have the number of digests to track set to 400, which takes roughly 490KB of memory):</p>
<pre>
mysql&gt; show engine performance_schema status;
+--------------------+--------------------------------------------------------------+-----------+
| Type               | Name                                                         | Status    |
+--------------------+--------------------------------------------------------------+-----------+
...
| performance_schema | events_statements_summary_by_digest.row_size                 | 1256      |
| performance_schema | events_statements_summary_by_digest.row_count                | 400       |
| performance_schema | events_statements_summary_by_digest.memory                   | 502400    |
...
</pre>
<p>This limit has been raised to 1000 within the current development version (5.6.6), <strong>along with Performance Schema (and the instrumentation I have talked about here) being enabled by default!</strong></p>
<p>As with other aggregate tables within Performance Schema, you can reset the statistics within the digest table with:</p>
<pre>
TRUNCATE TABLE performance_schema.events_statements_summary_by_digest;
</pre>
<p>Any scripts or tools that are used to analyze this table will need to take this in to account. </p>
<p>We&#8217;re just scratching the surface of possibilities with this table at the moment. I have a number of things that I&#8217;d like to see added (like.. a summary of each of the major classes of lower wait events, if enabled, like &#8220;SUM_FILE_IO_WAIT&#8221;, &#8220;SUM_SOCKET_IO_WAIT&#8221;, &#8220;SUM_MUTEX_WAIT&#8221; etc.), and I&#8217;m sure many of you out there in the community will have your own ideas as well. </p>
<p><strong>Welcome to the future of statement diagnostics within MySQL, we hope you enjoy it! Tell us what you&#8217;d like to see next!</strong></p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33743&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33743&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:10:"Mark Leith";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:43;a:6:{s:4:"data";s:48:"
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:30:"Notes on row based replication";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:36:"http://code.openark.org/blog/?p=4915";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:65:"http://code.openark.org/blog/mysql/notes-on-row-based-replication";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:2195:"MySQL's Row Based Replication (RBR) succeeds (though not replaces) Statement Based Replication (SBR), as of version 5.1.
Anyone who is familiar with replication data drift -- the unexplained growing data difference between master &amp; slave -- might wish to look into row based replication. On multiple servers I'm handling the change to RBR has eliminated (to the best of my findings) replication data drift.
This is easily explained, as RBR writes particular row IDs into the binary log. You no longer need to hope the statement

DELETE FROM my_table ORDER BY my_column LIMIT 100

acts deterministically on all servers (is my_column UNIQUE?). With row based replication the particular rows deleted on the master are then deleted on the slave.
Here are three notes on RBR:

Binary log size:

With statement based replication the binary log merely logs the statement - typically a short text. But RBR logs the row IDs and changes made to data.
After we finally got used to cheap huge disks, and forgot about the need for cleaning up the binary logs, RBR introduces bloated logs. On some servers I'am again confounded by the fact that 3 days worth of logs will hog my entire disk space.

Forgiveness

RBR is not as forgiving as SBR. With SBR, you could DELETE some rows from the master. If they're already missing on the slave, no problem here. With RBR, such an incident makes for replication failure (RBR: "I'm supposed to DELETE rows 3, 4, 5 but can't find them! I can't to my job like this! Heeelp!")
This is not a bad thing: you get an early alert that something went wrong.

Less slave effort

On the up side, the slave does not need to do much thinking. Given a DELETE command, the slave does not need to look up those rows WHERE some_condition IS TRUE. Instead, the master does all the hard work, the slave just gets the IDs of rows to delete.
I find that this boosts up replication speed in some scenarios, and in particular the scenario of data cleanup: those nightly/weekly purging of old, unused data. If you look hard and all you find are 5 rows to delete, all the slave needs to do is to delete those indicated 5 rows. With single-threaded replication this makes a real difference.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 08:00:24 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:2:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:11:"Replication";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:2830:"<p>MySQL's <a href="http://dev.mysql.com/doc/refman/5.1/en/replication-formats.html">Row Based Replication</a> (RBR) succeeds (though not replaces) Statement Based Replication (SBR), as of version <strong>5.1</strong>.</p>
<p>Anyone who is familiar with replication data drift -- the unexplained growing data difference between master &amp; slave -- might wish to look into row based replication. On multiple servers I'm handling the change to RBR has eliminated (to the best of my findings) replication data drift.</p>
<p>This is easily explained, as RBR writes particular row IDs into the binary log. You no longer need to hope the statement</p>
<blockquote>
<pre>DELETE FROM my_table ORDER BY my_column LIMIT 100</pre>
</blockquote>
<p>acts deterministically on all servers (is <strong>my_column</strong> UNIQUE?). With row based replication the particular rows deleted on the master are then deleted on the slave.</p>
<p>Here are three notes on RBR:<span></span></p>
<ul>
<li><em>Binary log size:</em></li>
</ul>
<p>With statement based replication the binary log merely logs the statement - typically a short text. But RBR logs the row IDs and changes made to data.</p>
<p>After we finally got used to cheap huge disks, and forgot about the need for cleaning up the binary logs, RBR introduces bloated logs. On some servers I'am again confounded by the fact that <strong>3</strong> days worth of logs will hog my entire disk space.</p>
<ul>
<li><em>Forgiveness</em></li>
</ul>
<p>RBR is not as forgiving as SBR. With SBR, you could <strong>DELETE</strong> some rows from the master. If they're already missing on the slave, no problem here. With RBR, such an incident makes for replication failure (RBR: <em>"I'm supposed to DELETE rows 3, 4, 5 but can't find them! I can't to my job like this! Heeelp!"</em>)</p>
<p>This is not a bad thing: you get an early alert that <em>something went wrong</em>.</p>
<ul>
<li><em>Less slave effort</em></li>
</ul>
<p>On the up side, the slave does not need to do much thinking. Given a DELETE command, the slave does not need to look up those rows <strong>WHERE some_condition IS TRUE</strong>. Instead, the master does all the hard work, the slave just gets the IDs of rows to delete.</p>
<p>I find that this boosts up replication speed in some scenarios, and in particular the scenario of data cleanup: those nightly/weekly purging of old, unused data. If you look hard and all you find are <strong>5</strong> rows to delete, all the slave needs to do is to delete those indicated <strong>5</strong> rows. With single-threaded replication this makes a real difference.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33741&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33741&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:12:"Shlomi Noach";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:44;a:6:{s:4:"data";s:53:"
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:53:"Why You Should Attend MySQL Connect, and Register Now";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:64:"https://blogs.oracle.com/MySQL/entry/why_you_should_attend_mysql";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:64:"https://blogs.oracle.com/MySQL/entry/why_you_should_attend_mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:3091:"MySQL Connect is
taking place on September 29 and 30 in San Francisco. The early bird discount
enabling you to save US$
500 is only running for a few more days, until July 13. 
  Are you
still wondering if you should sign up? Here are 10 reasons why you definitely
should:  
    
    
     
    
   
    Learn from
other companies how they tackled similar challenges to the ones you’re facing.
Find out what they learned along the way, and how you can save time, money and
a lot of troubles by avoiding repeating the same mistakes and applying the best
practices they’ve developed. You’ll get the chance to hear from organizations
including PayPal, Verizon, Twitter, Facebook, Ticketmaster, Ning, Mozilla,
CERN, Yahoo! and
more! 
    Don’t miss
this unique opportunity to meet the engineers developing and supporting the MySQL
products in a single location. You’ll be able to ask them all your questions,
which can represent a huge time and money saver. 
    Acquire
detailed knowledge about InnoDB, the MySQL Optimizer, High Availability
strategies, improving performance and scalability, enhancing security and numerous
other topics. You’ll hear it straight &quot;from the horse’s mouth&quot; as well as
from other MySQL experts in the ecosystem.  
    Get a better
understanding about Oracle’s
MySQL strategy and about the MySQL roadmap, so you can better plan where
to use the MySQL database and MySQL Cluster for your next web, cloud-based and
other applications.  
    Get hands-on
experience about improving performance with the MySQL Performance Schema, about
using MySQL Utilities, MySQL Cluster and a lot more with eight different Hands-On
Labs. 
    Express your
ideas, engage into discussions and help influence the MySQL roadmap during Birds-of-a-feather
sessions about replication, backup, query optimizations and other topics.  
    Meet
partners and learn about third party tools that could be useful in your
architecture.  
    Immerse
yourself into the MySQL universe and hang out with MySQL experts for two days.
The discussions as well as the relationships you will create can be priceless and
help you execute on your next projects in a much better and faster way.  
    Register
Now to save US$500 by taking advantage of the Early bird discount running
until July 13. We’ll have parallel tracks so you should consider sending a few
team members to make the most of the event. Are you attending or planning to
attend Oracle OpenWorld or JavaOne? You can add MySQL Connect to your registration
for only US$100! 
    Finally,
it’s always a lot of fun to attend a MySQL conference. The passion and the
energy are contagious…and you’ll likely get plenty of new ideas. 
   
    
  You will find all information about the program in the MySQL
Connect Content Catalog. We look forward to seeing you there!  
    
    
  You can also read interviews with Tomas
Ulin and Ronald
Bradford about MySQL Connect. 
    
    
  Sponsorship and exhibit
opportunities are still available for the conference. You will find more
information here. 
    
    
    
   ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 07:44:09 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:3:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:7:"connect";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:5962:"<p><span><a href="http://www.oracle.com/mysqlconnect/index.html">MySQL Connect</a> is
taking place on September 29 and 30 in San Francisco. The early bird discount
enabling you to <a href="http://www.oracle.com/mysqlconnect/register/packages/index.html">save US$
500</a> is only running for a few more days, until July 13.</span></p> 
  <p><span>Are you
still wondering if you should sign up? Here are 10 reasons why you definitely
should:</span> <br /></p> 
  <p><img style="width: 225px; height: 72px;" src="https://blogs.oracle.com/MySQL/resource/MySQLConnectlogo.png" /> </p> 
  <p> </p> 
  <p>  </p> 
  <p><span> </span></p> 
  <ul> 
    <li><span>Learn from
other companies how they tackled similar challenges to the ones you’re facing.
Find out what they learned along the way, and how you can save time, money and
a lot of troubles by avoiding repeating the same mistakes and applying the best
practices they’ve developed. You’ll get the chance to hear from organizations
including PayPal, Verizon, Twitter, Facebook, Ticketmaster, Ning, Mozilla,
CERN, Yahoo! <a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=&amp;p=&amp;i(10942)=15982&amp;i(11425)=&amp;i(10053)=&amp;i(10048)=&amp;i(11404)=&amp;i(10089)=&amp;i(11488)=">and
more!</a></span></li> 
    <li><span>Don’t miss
this unique opportunity to meet the engineers developing and supporting the MySQL
products in a single location. You’ll be able to ask them all your questions,
which can represent a huge time and money saver.</span></li> 
    <li><span>Acquire
detailed knowledge about InnoDB, the MySQL Optimizer, High Availability
strategies, improving performance and scalability, enhancing security and <a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=&amp;p=&amp;i(10942)=15982&amp;i(11425)=&amp;i(10053)=&amp;i(10048)=&amp;i(11404)=&amp;i(10089)=&amp;i(11488)=">numerous
other topics</a>. You’ll hear it straight &quot;from the horse’s mouth&quot; as well as
from other MySQL experts in the ecosystem. </span></li> 
    <li><span>Get a better
understanding about <a href="http://www.oracle.com/mysqlconnect/learn/keynotes/index.html">Oracle’s
MySQL strategy</a> and about the MySQL roadmap, so you can better plan where
to use the MySQL database and MySQL Cluster for your next web, cloud-based and
other applications. </span></li> 
    <li><span>Get hands-on
experience about improving performance with the MySQL Performance Schema, about
using MySQL Utilities, MySQL Cluster and a lot more with eight different <a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=&amp;p=&amp;i(10942)=15982&amp;i(11425)=18658&amp;i(10053)=&amp;i(10048)=&amp;i(11404)=&amp;i(10089)=&amp;i(11488)=">Hands-On
Labs.</a></span></li> 
    <li><span>Express your
ideas, engage into discussions and help influence the MySQL roadmap during <a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=&amp;p=&amp;i(10942)=15982&amp;i(11425)=18659&amp;i(10053)=&amp;i(10048)=&amp;i(11404)=&amp;i(10089)=&amp;i(11488)=">Birds-of-a-feather
sessions</a> about replication, backup, query optimizations and other topics. </span></li> 
    <li><span>Meet
partners and learn about third party tools that could be useful in your
architecture. </span></li> 
    <li><span>Immerse
yourself into the MySQL universe and hang out with MySQL experts for two days.
The discussions as well as the relationships you will create can be priceless and
help you execute on your next projects in a much better and faster way. </span></li> 
    <li><span><a href="http://www.oracle.com/mysqlconnect/register/packages/index.html">Register
Now to save US$500</a> by taking advantage of the Early bird discount running
until July 13. We’ll have parallel tracks so you should consider sending a few
team members to make the most of the event. Are you attending or planning to
attend Oracle OpenWorld or JavaOne? You can add MySQL Connect to your <a href="http://www.oracle.com/openworld/register/packages/index.html">registration</a>
for only US$100!</span></li> 
    <li><span>Finally,
it’s always a lot of fun to attend a MySQL conference. The passion and the
energy are contagious…and you’ll likely get plenty of new ideas.</span></li> 
  </ul> 
  <p><span> </span></p> 
  <p><span>You will find all information about the program in the </span><a href="https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;searchPhrase=&amp;searchType=session&amp;tc=0&amp;p=&amp;i(10942)=15982&amp;i(10056)=&amp;i(10053)=&amp;i(10048)=&amp;i(10943)=&amp;i(10089)="><span>MySQL
Connect Content Catalog</span></a><span>. We look forward to seeing you there! </span></p> 
  <p> </p> 
  <p><span> </span></p> 
  <p><span>You can also read interviews with <a href="https://blogs.oracle.com/MySQL/entry/mysql_connect_interview_with_tomas">Tomas
Ulin</a> and <a href="https://blogs.oracle.com/MySQL/entry/interview_with_ronald_bradford_about">Ronald
Bradford</a> about MySQL Connect.</span></p> 
  <p> </p> 
  <p><span> </span></p> 
  <p><span>Sponsorship and exhibit
opportunities are still available for the conference. You will find more
information </span><a href="http://www.oracle.com/mysqlconnect/exhibit-sponsor/index.html"><span>here</span></a><span>.</span></p> 
  <p><span> </span></p> 
  <p> </p> 
  <p> </p> 
  <p> </p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33739&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33739&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:15:"MySQL Community";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:45;a:6:{s:4:"data";s:58:"
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:26:"Jenkins Bazaar plugin 1.19";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:40:"http://www.flamingspork.com/blog/?p=3089";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:149:"http://www.flamingspork.com/blog/2012/07/04/jenkins-bazaar-plugin-1-19/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=jenkins-bazaar-plugin-1-19";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:529:"I recently released a new version of the Bazaar plugin for Jenkins. This release was inspired by a problem we noticed at Percona. It is:

run &#8220;bzr revert&#8221; after a pull, as if you have a directory that is removed and re-added while having unknown files in said directory (e.g. build artifacts), you would end up in a very bad place (this is a BZR bug, so we work-around it with a &#8220;bzr revert&#8221;).

The update has already appeared in the Jenkins update centre, so you should already be able to upgrade to it.
";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Wed, 04 Jul 2012 04:53:01 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:4:{i:0;a:5:{s:4:"data";s:4:"code";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:7:"percona";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:6:"bazaar";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:7:"jenkins";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1819:"<!-- Start Shareaholic LikeButtonSetTop Automatic --><!-- End Shareaholic LikeButtonSetTop Automatic --><p>I recently released a new version of the <a href="http://bazaar-vcs.org">Bazaar</a> plugin for <a href="http://jenkins-ci.org">Jenkins</a>. This release was inspired by a problem we noticed at Percona. It is:</p>
<ul>
<li>run &#8220;bzr revert&#8221; after a pull, as if you have a directory that is removed and re-added while having unknown files in said directory (e.g. build artifacts), you would end up in a very bad place (this is a BZR bug, so we work-around it with a &#8220;bzr revert&#8221;).</li>
</ul>
<p>The update has already appeared in the Jenkins update centre, so you should already be able to upgrade to it.</p>
<div></div><!-- Start Shareaholic LikeButtonSetBottom Automatic --><div></div><div><a data-shr_layout='button_count' data-shr_showfaces='false' data-shr_href='http%3A%2F%2Fwww.flamingspork.com%2Fblog%2F2012%2F07%2F04%2Fjenkins-bazaar-plugin-1-19%2F' data-shr_title='Jenkins+Bazaar+plugin+1.19'></a><a data-shr_href='http%3A%2F%2Fwww.flamingspork.com%2Fblog%2F2012%2F07%2F04%2Fjenkins-bazaar-plugin-1-19%2F'></a><a data-shr_size='medium' data-shr_count='true' data-shr_href='http%3A%2F%2Fwww.flamingspork.com%2Fblog%2F2012%2F07%2F04%2Fjenkins-bazaar-plugin-1-19%2F' data-shr_title='Jenkins+Bazaar+plugin+1.19'></a><a data-shr_count='horizontal' data-shr_href='http%3A%2F%2Fwww.flamingspork.com%2Fblog%2F2012%2F07%2F04%2Fjenkins-bazaar-plugin-1-19%2F' data-shr_title='Jenkins+Bazaar+plugin+1.19'></a></div><div></div><!-- End Shareaholic LikeButtonSetBottom Automatic --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33737&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33737&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Stewart Smith";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:46;a:6:{s:4:"data";s:58:"
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:52:"Prepared statement peculiarities (P_S to the rescue)";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:39:"http://mysqlblog.fivefarmers.com/?p=219";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:95:"http://mysqlblog.fivefarmers.com/2012/07/03/prepared-statement-peculiarities-p_s-to-the-rescue/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:10595:"Prepared statements have been with MySQL since version 4.1, including the protocol plumbing that helps support it.  What I didn&#8217;t realize &#8211; until a recent expedition through a general query log &#8211; is that the mysql command-line interface doesn&#8217;t implement the protocol commands that support this explicitly.  I came to this realization after observing a byproduct of this behavior.
The initial observation that triggered this exploration was noting that PREPARE and EXECUTE statements, when issued from the mysql command-line interface, result in two entries per command in the general query log:

6 Query    PREPARE stmt FROM 'SELECT RAND()'
6 Prepare    SELECT RAND()
6 Query    EXECUTE stmt
6 Execute    SELECT RAND()

Contrast this behavior with what is seen when a client sends COM_PREPARE and COM_EXECUTE, such as below with Connector/J (and useServerPrepStmts=true):

14 Prepare    SELECT * FROM t1 WHERE a = ?
14 Execute    SELECT * FROM t1 WHERE a = 2

This was of interest to me because I had a script to take the contents of the general query log, separate them into per-connection files containing the SQL commands, to assist in diagnosing application behavior when developers can&#8217;t effectively isolate or describe problematic behavior.  By stripping out certain commands (connect/disconnect) and transforming others (Init DB to the original USE [db] command), this script could potentially help us build repeatable test cases faster.
Of course, that doesn&#8217;t work if prepared statement commands are duplicated sometimes.
The general query log helpfully records the type of command executed.  From the mysql cli, you can see &#8220;Query&#8221;, which corresponds to the COM_QUERY command in the protocol.  You also see the &#8220;Prepare&#8221; and &#8220;Execute&#8221;, the latter of which has the interpreted values.  Executed from Connector/J, though, you only see the &#8220;Prepare&#8221; and &#8220;Execute&#8221; because the driver is sending COM_STMT_PREPARE and COM_STMT_EXECUTE directly.  The protocol documentation describes these commands well.  That gives me two options, if I want to transform the general query log into a somewhat accurate series of SQL statements:

Throw away all of the Prepare and Execute events, hoping that everybody executing prepared statements is using the cli (or COM_QUERY), and replay only the COM_QUERY.
Parse and discard any COM_QUERY commands that invoke PREPARE or EXECUTE, and just use the generated SQL found in the &#8220;Execute&#8221; entries in the general query log.

I kind of understand why the mysql cli doesn&#8217;t implement the protocol commands for prepared statements, even if it does parse and transform other commands (e.g., &#8220;USE db&#8221;).  The server-side support allows prepared statements to be used at the application level even if the driver doesn&#8217;t support it at the protocol level.  And from a debugging perspective, it&#8217;s very nice to see the generated SQL in the general query log.  Because the general query log includes every command it gets, before execution is started, I also understand why the the original query received has to show there, unaltered.  I do wish, however, that there was a flag in the general query log to indicate that the Prepare or Execute being logged was internally-generated from a COM_QUERY.
The duplication of commands in the general query log did make me wonder about performance.  It seems pretty clear that there&#8217;s some overhead added when COM_QUERY is used to send PREPARE or EXECUTE commands to the server &#8211; at the very minimum, the original statement gets logged to the general query log and some parsing done to redirect execution to code paths handling preparation or execution of prepared statements.  For the fun of it, I thought I would benchmark what the performance difference is on my (admittedly slow) laptop, using Java.  Here&#8217;s what the code looks like:

public static void testPSPerformance() throws Exception {
 Class.forName("com.mysql.jdbc.Driver");
 Properties props = new Properties();
 props.setProperty("user", "root");
 props.setProperty("useServerPrepStmts", "true");
 Connection conn =
  DriverManager.getConnection("jdbc:mysql://localhost:3306/test", props);
 System.out.println("Connected!");
 PreparedStatement ps = conn.prepareStatement("SELECT RAND()");
 long start = System.currentTimeMillis();
 for(int i = 0; i &lt; 1000000; i++){ ps.execute(); }
 long end = System.currentTimeMillis();
 System.out.println("Using COM_PREPARE:  " + (end - start));

 Statement stmt = conn.createStatement();
 stmt.execute("PREPARE stmt FROM 'SELECT RAND()'");
 final String ex = "EXECUTE stmt";
 start = System.currentTimeMillis();
 for(int i = 0; i &lt; 1000000; i++){ stmt.execute(ex); }
 end = System.currentTimeMillis();
 System.out.println("Using COM_QUERY:  " + (end - start));
}

The end results show about 10% performance loss by using COM_QUERY, although the actual difference may be more or less depending on your driver and deployment environment.  For example, the Connector/J Java code path for using Statement objects is different than using server-side PreparedStatements, and overhead may be added or removed there.  Here are the results from my testing:
Using COM_PREPARE:  80516
Using COM_QUERY:  90109
Using COM_PREPARE:  80547
Using COM_QUERY:  87594
Using COM_PREPARE:  81344
Using COM_QUERY:  89781
As you might expect, the overall execution time as well as the performance difference increases when the general query log is enabled:
Using COM_PREPARE:  100031
Using COM_QUERY:  126485
Again, you won&#8217;t want to draw any solid conclusions from the above about whether use of prepared statements in the same way as the mysql cli does represents performance problems for you.  But it might be worth checking.  So, how can that be done?  Unfortunately, the relevant status variables don&#8217;t distinguish between when a statement is prepared using COM_PREPARE or COM_QUERY (UPDATE:  You can evaluate whether COM_QUERY or COM_PREPARE is used by subtracting Com_prepare_sql from Com_stmt_prepare.  The latter is always incremented for PREPARE statements, regardless of whether they are issued as part of a COM_QUERY or COM_PREPARE command, while the former is only incremented when COM_QUERY is used):

mysql&gt; FLUSH STATUS;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE '%prep%';
+---------------------+-------+
| Variable_name       | Value |
+---------------------+-------+
| Com_prepare_sql     | 0     |
| Com_stmt_prepare    | 0     |
| Com_stmt_reprepare  | 0     |
| Com_xa_prepare      | 0     |
| Handler_prepare     | 0     |
| Prepared_stmt_count | 1     |
+---------------------+-------+
6 rows in set (0.00 sec)

mysql&gt; prepare stmt from 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SHOW SESSION STATUS LIKE '%prep%';
+---------------------+-------+
| Variable_name       | Value |
+---------------------+-------+
| Com_prepare_sql     | 1     |
| Com_stmt_prepare    | 1     |
| Com_stmt_reprepare  | 0     |
| Com_xa_prepare      | 0     |
| Handler_prepare     | 0     |
| Prepared_stmt_count | 1     |
+---------------------+-------+
6 rows in set (0.00 sec)

So, how can you determine whether your use of prepared statements uses COM_PREPARE or COM_QUERY?  Using PERFORMANCE_SCHEMA in 5.6, it&#8217;s easy!  Here&#8217;s the query:

SELECT
user,
host,
count_star,
IF(event_name = 'statement/sql/prepare_sql',
'COM_QUERY', 'COM_PREPARE') command
FROM events_statements_summary_by_account_by_event_name
WHERE EVENT_NAME IN
('statement/sql/prepare_sql', 'statement/com/Prepare');

Here it is in action:

mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          4 | COM_QUERY   |
| root | localhost |          3 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)

mysql&gt; prepare stmt from 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          5 | COM_QUERY   |
| root | localhost |          3 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)

mysql&gt; -- Issue prepared statement from Connector/J:
mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          5 | COM_QUERY   |
| root | localhost |          4 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)

The original problem may not be of much concern to you, but the power of PERFORMANCE_SCHEMA to dig deeper into server behavior is something that is applicable regardless of whether prepared statements are a concern for you or not.";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 03 Jul 2012 23:10:31 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:4:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:9:"MySQL 5.6";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:18:"performance_schema";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:19:"prepared statements";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:11416:"<p>Prepared statements have been with MySQL since version 4.1, including the protocol plumbing that helps support it.  What I didn&#8217;t realize &#8211; until a recent expedition through a general query log &#8211; is that the mysql command-line interface doesn&#8217;t implement the protocol commands that support this explicitly.  I came to this realization after observing a byproduct of this behavior.</p>
<p>The initial observation that triggered this exploration was noting that PREPARE and EXECUTE statements, when issued from the mysql command-line interface, result in two entries per command in the general query log:</p>
<pre>
6 Query    PREPARE stmt FROM 'SELECT RAND()'
6 Prepare    SELECT RAND()
6 Query    EXECUTE stmt
6 Execute    SELECT RAND()
</pre>
<p>Contrast this behavior with what is seen when a client sends COM_PREPARE and COM_EXECUTE, such as below with Connector/J (and useServerPrepStmts=true):</p>
<pre>
14 Prepare    SELECT * FROM t1 WHERE a = ?
14 Execute    SELECT * FROM t1 WHERE a = 2
</pre>
<p>This was of interest to me because I had a script to take the contents of the general query log, separate them into per-connection files containing the SQL commands, to assist in diagnosing application behavior when developers can&#8217;t effectively isolate or describe problematic behavior.  By stripping out certain commands (connect/disconnect) and transforming others (Init DB to the original USE [db] command), this script could potentially help us build repeatable test cases faster.</p>
<p>Of course, that doesn&#8217;t work if prepared statement commands are duplicated sometimes.</p>
<p>The general query log helpfully records the type of command executed.  From the mysql cli, you can see &#8220;Query&#8221;, which corresponds to the COM_QUERY command in the protocol.  You also see the &#8220;Prepare&#8221; and &#8220;Execute&#8221;, the latter of which has the interpreted values.  Executed from Connector/J, though, you only see the &#8220;Prepare&#8221; and &#8220;Execute&#8221; because the driver is sending COM_STMT_PREPARE and COM_STMT_EXECUTE directly.  The <a href="http://forge.mysql.com/wiki/MySQL_Internals_ClientServer_Protocol#Command_Packet_.28Overview.29" target="_blank">protocol documentation</a> describes these commands well.  That gives me two options, if I want to transform the general query log into a somewhat accurate series of SQL statements:</p>
<ol>
<li>Throw away all of the Prepare and Execute events, hoping that everybody executing prepared statements is using the cli (or COM_QUERY), and replay only the COM_QUERY.</li>
<li>Parse and discard any COM_QUERY commands that invoke PREPARE or EXECUTE, and just use the generated SQL found in the &#8220;Execute&#8221; entries in the general query log.</li>
</ol>
<p>I kind of understand why the mysql cli doesn&#8217;t implement the protocol commands for prepared statements, even if it does parse and transform other commands (e.g., &#8220;USE db&#8221;).  The server-side support allows prepared statements to be used at the application level even if the driver doesn&#8217;t support it at the protocol level.  And from a debugging perspective, it&#8217;s very nice to see the generated SQL in the general query log.  Because the general query log includes every command it gets, before execution is started, I also understand why the the original query received has to show there, unaltered.  I do wish, however, that there was a <a href="http://bugs.mysql.com/bug.php?id=65800" target="_blank">flag in the general query log</a> to indicate that the Prepare or Execute being logged was internally-generated from a COM_QUERY.</p>
<p>The duplication of commands in the general query log did make me wonder about performance.  It seems pretty clear that there&#8217;s some overhead added when COM_QUERY is used to send PREPARE or EXECUTE commands to the server &#8211; at the very minimum, the original statement gets logged to the general query log and some parsing done to redirect execution to code paths handling preparation or execution of prepared statements.  For the fun of it, I thought I would benchmark what the performance difference is on my (admittedly slow) laptop, using Java.  Here&#8217;s what the code looks like:</p>
<pre>
public static void testPSPerformance() throws Exception {
 Class.forName("com.mysql.jdbc.Driver");
 Properties props = new Properties();
 props.setProperty("user", "root");
 props.setProperty("useServerPrepStmts", "true");
 Connection conn =
  DriverManager.getConnection("jdbc:mysql://localhost:3306/test", props);
 System.out.println("Connected!");
 PreparedStatement ps = conn.prepareStatement("SELECT RAND()");
 long start = System.currentTimeMillis();
 for(int i = 0; i &lt; 1000000; i++){ ps.execute(); }
 long end = System.currentTimeMillis();
 System.out.println("Using COM_PREPARE:  " + (end - start));

 Statement stmt = conn.createStatement();
 stmt.execute("PREPARE stmt FROM 'SELECT RAND()'");
 final String ex = "EXECUTE stmt";
 start = System.currentTimeMillis();
 for(int i = 0; i &lt; 1000000; i++){ stmt.execute(ex); }
 end = System.currentTimeMillis();
 System.out.println("Using COM_QUERY:  " + (end - start));
}
</pre>
<p>The end results show about 10% performance loss by using COM_QUERY, although the actual difference may be more or less depending on your driver and deployment environment.  For example, the Connector/J Java code path for using Statement objects is different than using server-side PreparedStatements, and overhead may be added or removed there.  Here are the results from my testing:</p>
<p>Using COM_PREPARE:  80516<br />
Using COM_QUERY:  90109</p>
<p>Using COM_PREPARE:  80547<br />
Using COM_QUERY:  87594</p>
<p>Using COM_PREPARE:  81344<br />
Using COM_QUERY:  89781</p>
<p>As you might expect, the overall execution time as well as the performance difference increases when the general query log is enabled:</p>
<p>Using COM_PREPARE:  100031<br />
Using COM_QUERY:  126485</p>
<p>Again, you won&#8217;t want to draw any solid conclusions from the above about whether use of prepared statements in the same way as the mysql cli does represents performance problems for you.  But it might be worth checking.  So, how can that be done?  Unfortunately, the relevant status variables don&#8217;t distinguish between when a statement is prepared using COM_PREPARE or COM_QUERY (<strong>UPDATE</strong>:  You <strong>can</strong> evaluate whether COM_QUERY or COM_PREPARE is used by subtracting Com_prepare_sql from Com_stmt_prepare.  The latter is always incremented for PREPARE statements, regardless of whether they are issued as part of a COM_QUERY or COM_PREPARE command, while the former is only incremented when COM_QUERY is used):</p>
<pre>
mysql&gt; FLUSH STATUS;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; SHOW SESSION STATUS LIKE '%prep%';
+---------------------+-------+
| Variable_name       | Value |
+---------------------+-------+
| Com_prepare_sql     | 0     |
| Com_stmt_prepare    | 0     |
| Com_stmt_reprepare  | 0     |
| Com_xa_prepare      | 0     |
| Handler_prepare     | 0     |
| Prepared_stmt_count | 1     |
+---------------------+-------+
6 rows in set (0.00 sec)

mysql&gt; prepare stmt from 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SHOW SESSION STATUS LIKE '%prep%';
+---------------------+-------+
| Variable_name       | Value |
+---------------------+-------+
| Com_prepare_sql     | 1     |
| Com_stmt_prepare    | 1     |
| Com_stmt_reprepare  | 0     |
| Com_xa_prepare      | 0     |
| Handler_prepare     | 0     |
| Prepared_stmt_count | 1     |
+---------------------+-------+
6 rows in set (0.00 sec)
</pre>
<p>So, how can you determine whether your use of prepared statements uses COM_PREPARE or COM_QUERY?  Using PERFORMANCE_SCHEMA in 5.6, it&#8217;s easy!  Here&#8217;s the query:</p>
<pre>
SELECT
user,
host,
count_star,
IF(event_name = 'statement/sql/prepare_sql',
'COM_QUERY', 'COM_PREPARE') command
FROM events_statements_summary_by_account_by_event_name
WHERE EVENT_NAME IN
('statement/sql/prepare_sql', 'statement/com/Prepare');
</pre>
<p>Here it is in action:</p>
<pre>
mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          4 | COM_QUERY   |
| root | localhost |          3 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)

mysql&gt; prepare stmt from 'SELECT 1';
Query OK, 0 rows affected (0.00 sec)
Statement prepared

mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          5 | COM_QUERY   |
| root | localhost |          3 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)

mysql&gt; -- Issue prepared statement from Connector/J:
mysql&gt; SELECT
-&gt;  user,
-&gt;  host,
-&gt;  count_star,
-&gt;  IF(event_name = 'statement/sql/prepare_sql',
-&gt;   'COM_QUERY', 'COM_PREPARE') command
-&gt; FROM events_statements_summary_by_account_by_event_name
-&gt; WHERE EVENT_NAME IN
-&gt; ('statement/sql/prepare_sql', 'statement/com/Prepare');
+------+-----------+------------+-------------+
| user | host      | count_star | command     |
+------+-----------+------------+-------------+
| NULL | NULL      |          0 | COM_QUERY   |
| NULL | NULL      |          0 | COM_PREPARE |
| root | localhost |          5 | COM_QUERY   |
| root | localhost |          4 | COM_PREPARE |
+------+-----------+------------+-------------+
4 rows in set (0.00 sec)
</pre>
<p>The original problem may not be of much concern to you, but the power of PERFORMANCE_SCHEMA to dig deeper into server behavior is something that is applicable regardless of whether prepared statements are a concern for you or not.</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33736&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33736&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:11:"Todd Farmer";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:47;a:6:{s:4:"data";s:78:"
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:24:"Placement over substance";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:42:"http://blog.mclaughlinsoftware.com/?p=6355";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:71:"http://blog.mclaughlinsoftware.com/2012/07/03/placement-over-substance/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:4268:"I was stunned when a SQL query raised an ERROR 1630 (42000) telling me the SUM function didn&#8217;t exist in MySQL 5.5.23. The fix was simple. The opening parenthesis of the SUM function must be on the same line as the SUM keyword without an intervening white space. Alternatively phrased, you can&#8217;t have a line return or white space between the SUM function name and the opening parenthesis of the call parameter list. The same rule doesn&#8217;t apply to the opening parenthesis of the FORMAT function and it seems to me that this parsing inconsistency is problematic. 
Therefore, my surprise, observation, and complaint is that all functions don&#8217;t parse the same way, using the same rules. That is, unless you use specialized SQL_MODE settings.
A simplified version of the code that raises the error follows. As you&#8217;ll notice the opening parenthesis for the FORMAT and SUM function have intervening white space and a line return.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
SELECT   t.transaction_account AS &quot;Transaction&quot;
,        LPAD&#40;FORMAT
           &#40;SUM
             &#40;CASE
                WHEN EXTRACT&#40;MONTH FROM transaction_date&#41; = 1 AND
                     EXTRACT&#40;YEAR FROM transaction_date&#41; = 2011 THEN
                  CASE
                    WHEN t.transaction_type = cl.common_lookup_type THEN
                      t.transaction_amount
                    ELSE
                      t.transaction_amount * -1
                  END
             END&#41;,2&#41;,10,' '&#41; AS &quot;JAN&quot;
FROM     TRANSACTION t CROSS JOIN common_lookup cl
WHERE    cl.common_lookup_table = 'TRANSACTION'
AND      cl.common_lookup_column = 'TRANSACTION_TYPE'
AND      cl.common_lookup_type = 'DEBIT'
GROUP BY t.transaction_account;

Based on the comments, the SQL_MODE is:

mysql&gt; SELECT @@version, @@sql_mode;
+-----------+----------------------------------------------------------------+
| @@version | @@sql_mode                                                     |
+-----------+----------------------------------------------------------------+
| 5.5.23    | STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+-----------+----------------------------------------------------------------+
1 ROW IN SET &#40;0.00 sec&#41;

It raises the following error:

ERROR 1630 &#40;42000&#41;: FUNCTION studentdb.SUM does NOT exist. CHECK the 'Function Name Parsing and Resolution' SECTION IN the Reference Manual

Moving ONLY the opening parenthesis to the end of the SUM keyword (or removing the line return and white space from between the SUM keyword and opening parenthesis) prevents the error but it would be more convenient if it supported both approaches. It seems odd that an intervening line return and white space for the SUM function raises an exception while the same intervening line return and white space doesn&#8217;t raise an exception for the FORMAT function. It strikes me the parser should support both or reject both. Here&#8217;s the fixed code that works without enabling the IGNORE_SPACE SQL Mode option.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
SELECT   t.transaction_account AS &quot;Transaction&quot;
,        LPAD&#40;FORMAT
           &#40;SUM&#40;
              CASE
                WHEN EXTRACT&#40;MONTH FROM transaction_date&#41; = 1 AND
                     EXTRACT&#40;YEAR FROM transaction_date&#41; = 2011 THEN
                  CASE
                    WHEN t.transaction_type = cl.common_lookup_type THEN
                      t.transaction_amount
                    ELSE
                      t.transaction_amount * -1
                  END
             END&#41;,2&#41;,10,' '&#41; AS &quot;JAN&quot;
FROM     TRANSACTION t CROSS JOIN common_lookup cl
WHERE    cl.common_lookup_table = 'TRANSACTION'
AND      cl.common_lookup_column = 'TRANSACTION_TYPE'
AND      cl.common_lookup_type = 'DEBIT'
GROUP BY t.transaction_account;

As noted by the comments, adding the IGNORE_SPACE to the SQL_MODE lets both queries work without moving the open parenthesis. You can do that in a session with the following syntax (which is covered in an older post):

SET SQL_MODE=&#40;SELECT CONCAT&#40;@@sql_mode,',IGNORE_SPACE'&#41;&#41;;

Hope this helps folks&#8230;";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 03 Jul 2012 22:10:06 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:8:{i:0;a:5:{s:4:"data";s:5:"MySQL";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:1;a:5:{s:4:"data";s:3:"PSM";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:2;a:5:{s:4:"data";s:3:"sql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:3;a:5:{s:4:"data";s:7:"SQL/PSM";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:4;a:5:{s:4:"data";s:12:"IGNORE_SPACE";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:5;a:5:{s:4:"data";s:5:"mysql";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:6;a:5:{s:4:"data";s:8:"SQL_MODE";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}i:7;a:5:{s:4:"data";s:12:"SUM function";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:7601:"<p>I was stunned when a SQL query raised an <code>ERROR 1630 (42000)</code> telling me the <code>SUM</code> function didn&#8217;t exist in MySQL 5.5.23. The fix was simple. The opening parenthesis of the <code>SUM</code> function must be on the same line as the <code>SUM</code> keyword without an intervening white space. Alternatively phrased, you can&#8217;t have a line return or white space between the <code>SUM</code> function name and the opening parenthesis of the call parameter list. The same rule doesn&#8217;t apply to the opening parenthesis of the <code>FORMAT</code> function and it seems to me that this parsing inconsistency is problematic. </p>
<p><strong><em>Therefore, my surprise, observation, and complaint is that all functions don&#8217;t parse the same way, using the same rules. That is, unless you use specialized <code>SQL_MODE</code> settings.</em></strong></p>
<p>A simplified version of the code that raises the error follows. As you&#8217;ll notice the opening parenthesis for the <code>FORMAT</code> and <code>SUM</code> function have intervening white space and a line return.</p>

<div><table><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td><pre><span>SELECT</span>   t<span>.</span>transaction_account <span>AS</span> <span>&quot;Transaction&quot;</span>
<span>,</span>        LPAD<span>&#40;</span>FORMAT
           <span>&#40;</span><span>SUM</span>
             <span>&#40;</span><span>CASE</span>
                <span>WHEN</span> <span>EXTRACT</span><span>&#40;</span><span>MONTH</span> <span>FROM</span> transaction_date<span>&#41;</span> <span>=</span> <span>1</span> <span>AND</span>
                     <span>EXTRACT</span><span>&#40;</span><span>YEAR</span> <span>FROM</span> transaction_date<span>&#41;</span> <span>=</span> <span>2011</span> <span>THEN</span>
                  <span>CASE</span>
                    <span>WHEN</span> t<span>.</span>transaction_type <span>=</span> cl<span>.</span>common_lookup_type <span>THEN</span>
                      t<span>.</span>transaction_amount
                    <span>ELSE</span>
                      t<span>.</span>transaction_amount <span>*</span> <span>-</span><span>1</span>
                  <span>END</span>
             <span>END</span><span>&#41;</span><span>,</span><span>2</span><span>&#41;</span><span>,</span><span>10</span><span>,</span><span>' '</span><span>&#41;</span> <span>AS</span> <span>&quot;JAN&quot;</span>
<span>FROM</span>     <span>TRANSACTION</span> t <span>CROSS</span> <span>JOIN</span> common_lookup cl
<span>WHERE</span>    cl<span>.</span>common_lookup_table <span>=</span> <span>'TRANSACTION'</span>
<span>AND</span>      cl<span>.</span>common_lookup_column <span>=</span> <span>'TRANSACTION_TYPE'</span>
<span>AND</span>      cl<span>.</span>common_lookup_type <span>=</span> <span>'DEBIT'</span>
<span>GROUP</span> <span>BY</span> t<span>.</span>transaction_account;</pre></td></tr></table></div>

<p>Based on the comments, the SQL_MODE is:</p>

<div><div><pre>mysql<span>&gt;</span> <span>SELECT</span> @@version<span>,</span> @@sql_mode;
<span>+</span><span>-----------+----------------------------------------------------------------+</span>
<span>|</span> @@version <span>|</span> @@sql_mode                                                     <span>|</span>
<span>+</span><span>-----------+----------------------------------------------------------------+</span>
<span>|</span> 5<span>.</span>5<span>.</span>23    <span>|</span> STRICT_TRANS_TABLES<span>,</span>NO_AUTO_CREATE_USER<span>,</span>NO_ENGINE_SUBSTITUTION <span>|</span>
<span>+</span><span>-----------+----------------------------------------------------------------+</span>
<span>1</span> <span>ROW</span> <span>IN</span> <span>SET</span> <span>&#40;</span><span>0.00</span> sec<span>&#41;</span></pre></div></div>

<p>It raises the following error:</p>

<div><div><pre>ERROR <span>1630</span> <span>&#40;</span><span>42000</span><span>&#41;</span>: <span>FUNCTION</span> studentdb<span>.</span><span>SUM</span> does <span>NOT</span> exist<span>.</span> <span>CHECK</span> the <span>'Function Name Parsing and Resolution'</span> <span>SECTION</span> <span>IN</span> the Reference Manual</pre></div></div>

<p>Moving <strong><em>ONLY</em></strong> the opening parenthesis to the end of the <code>SUM</code> keyword (or removing the line return and white space from between the <code>SUM</code> keyword and opening parenthesis) prevents the error but it would be more convenient if it supported both approaches. It seems odd that an intervening line return and white space for the <code>SUM</code> function raises an exception while the same intervening line return and white space doesn&#8217;t raise an exception for the <code>FORMAT</code> function. It strikes me the parser should support both or reject both. Here&#8217;s the fixed code that works without enabling the <code>IGNORE_SPACE</code> SQL Mode option.</p>

<div><table><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td><pre><span>SELECT</span>   t<span>.</span>transaction_account <span>AS</span> <span>&quot;Transaction&quot;</span>
<span>,</span>        LPAD<span>&#40;</span>FORMAT
           <span>&#40;</span><span>SUM</span><span>&#40;</span>
              <span>CASE</span>
                <span>WHEN</span> <span>EXTRACT</span><span>&#40;</span><span>MONTH</span> <span>FROM</span> transaction_date<span>&#41;</span> <span>=</span> <span>1</span> <span>AND</span>
                     <span>EXTRACT</span><span>&#40;</span><span>YEAR</span> <span>FROM</span> transaction_date<span>&#41;</span> <span>=</span> <span>2011</span> <span>THEN</span>
                  <span>CASE</span>
                    <span>WHEN</span> t<span>.</span>transaction_type <span>=</span> cl<span>.</span>common_lookup_type <span>THEN</span>
                      t<span>.</span>transaction_amount
                    <span>ELSE</span>
                      t<span>.</span>transaction_amount <span>*</span> <span>-</span><span>1</span>
                  <span>END</span>
             <span>END</span><span>&#41;</span><span>,</span><span>2</span><span>&#41;</span><span>,</span><span>10</span><span>,</span><span>' '</span><span>&#41;</span> <span>AS</span> <span>&quot;JAN&quot;</span>
<span>FROM</span>     <span>TRANSACTION</span> t <span>CROSS</span> <span>JOIN</span> common_lookup cl
<span>WHERE</span>    cl<span>.</span>common_lookup_table <span>=</span> <span>'TRANSACTION'</span>
<span>AND</span>      cl<span>.</span>common_lookup_column <span>=</span> <span>'TRANSACTION_TYPE'</span>
<span>AND</span>      cl<span>.</span>common_lookup_type <span>=</span> <span>'DEBIT'</span>
<span>GROUP</span> <span>BY</span> t<span>.</span>transaction_account;</pre></td></tr></table></div>

<p>As noted by the comments, adding the <code>IGNORE_SPACE</code> to the <code>SQL_MODE</code> lets both queries work without moving the open parenthesis. You can do that in a session with the following syntax (which is covered in <a href="http://blog.mclaughlinsoftware.com/2010/03/10/mysql-standard-group-by/">an older post</a>):</p>

<div><div><pre><span>SET</span> SQL_MODE<span>=</span><span>&#40;</span><span>SELECT</span> CONCAT<span>&#40;</span>@@sql_mode<span>,</span><span>',IGNORE_SPACE'</span><span>&#41;</span><span>&#41;</span>;</pre></div></div>

<p>Hope this helps folks&#8230;</p><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33735&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33735&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:18:"Michael McLaughlin";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:48;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:38:"In depth explanation of SQL join types";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:27:"http://mysqljoin.com/?p=625";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:66:"http://mysqljoin.com/joins/in-depth-explanation-of-sql-join-types/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:1215:"
While we at mysqljoin.com want to provide simply and straightforward tutorials which are easy to understand, we love in depth documentation as well. And since relational databases are very complex, a documentation can be much more technical than our tutorials are.
We just came across such a documentation, a very detailed explanation of SQL join types. We felt that we have to share this with our readers. It&#8217;s a very nice follow up reading for everybody who&#8217;s currently learning about MySQL joins. Beside many interesting facts you&#8217;ll find probably the most interesting visualisation of joined tables we&#8217;ve ever seen as well as a visualized classification schemes for SQL joins.
There are different classification schemes and different criteria according to what joins are classified. As a result there is a bit mess in the process of understanding them. I haven&#8217;t found a nice scheme or even textual description how each one of various classification schemes and join types relates to other. In the following Meta model there is one possible variant of that, however theoreticians probably would break lances around other possible classification schemes. - SQL join types
&nbsp;

";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 03 Jul 2012 20:26:40 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:10:"Join Hints";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:1667:"<!-- google_ad_section_start -->
<p>While we at mysqljoin.com want to provide simply and straightforward tutorials which are easy to understand, we love in depth documentation as well. And since relational databases are very complex, a documentation can be much more technical than our tutorials are.</p>
<p>We just came across such a documentation, a very <a href="http://www.gplivna.eu/papers/sql_join_types.htm">detailed explanation of SQL join types</a>. We felt that we have to share this with our readers. It&#8217;s a very nice follow up reading for everybody who&#8217;s currently learning about MySQL joins. Beside many interesting facts you&#8217;ll find probably the most interesting visualisation of joined tables we&#8217;ve ever seen as well as a visualized classification schemes for SQL joins.</p>
<blockquote><p>There are different classification schemes and different criteria according to what joins are classified. As a result there is a bit mess in the process of understanding them. I haven&#8217;t found a nice scheme or even textual description how each one of various classification schemes and join types relates to other. In the following Meta model there is one possible variant of that, however theoreticians probably would break lances around other possible classification schemes. - <a href="http://www.gplivna.eu/papers/sql_join_types.htm">SQL join types</a></p></blockquote>
<p>&nbsp;</p>

<!-- google_ad_section_end --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33734&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33734&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Jan Brinkmann";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}i:49;a:6:{s:4:"data";s:43:"
    
    
    
    
    
    
    
    
  ";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";s:5:"child";a:3:{s:0:"";a:6:{s:5:"title";a:1:{i:0;a:5:{s:4:"data";s:31:"Do you like a question section?";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"guid";a:1:{i:0;a:5:{s:4:"data";s:58:"http://mysqljoin.com/joins/do-you-like-a-question-section/";s:7:"attribs";a:1:{s:0:"";a:1:{s:11:"isPermaLink";s:5:"false";}}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:4:"link";a:1:{i:0;a:5:{s:4:"data";s:58:"http://mysqljoin.com/joins/do-you-like-a-question-section/";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:11:"description";a:1:{i:0;a:5:{s:4:"data";s:171:"
I thought it would be useful if people could simply ask questions about MySQL joins and have a place where they get an answer. Do you think it&#8217;s a useful feature?

";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:7:"pubDate";a:1:{i:0;a:5:{s:4:"data";s:31:"Tue, 03 Jul 2012 19:20:41 +0000";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}s:8:"category";a:1:{i:0;a:5:{s:4:"data";s:9:"Questions";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:40:"http://purl.org/rss/1.0/modules/content/";a:1:{s:7:"encoded";a:1:{i:0;a:5:{s:4:"data";s:453:"<!-- google_ad_section_start -->
<p>I thought it would be useful if people could simply ask questions about MySQL joins and have a place where they get an answer. Do you think it&#8217;s a useful feature?</p>

<!-- google_ad_section_end --><br/>PlanetMySQL Voting:
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33733&vote=1&apivote=1">Vote UP</a> /
	 <a href="http://planet.mysql.com/entry/vote/?entry_id=33733&vote=-1&apivote=1">Vote DOWN</a>";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}s:32:"http://purl.org/dc/elements/1.1/";a:1:{s:7:"creator";a:1:{i:0;a:5:{s:4:"data";s:13:"Jan Brinkmann";s:7:"attribs";a:0:{}s:8:"xml_base";s:0:"";s:17:"xml_base_explicit";b:0;s:8:"xml_lang";s:0:"";}}}}}}}}}}}}}}}}s:4:"type";i:128;s:7:"headers";a:6:{s:4:"date";s:29:"Wed, 11 Jul 2012 21:39:06 GMT";s:6:"server";s:10:"Apache/2.2";s:13:"last-modified";s:29:"Wed, 11 Jul 2012 21:30:53 GMT";s:13:"accept-ranges";s:5:"bytes";s:14:"content-length";s:6:"389885";s:12:"content-type";s:8:"text/xml";}s:5:"build";s:14:"20090627192103";}